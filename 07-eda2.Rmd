# Multivariate Data Exploration {#eda2}

```{r eda2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
library(tidyverse)
library(knitr)
library(bookdown)
library(scales)
library(kableExtra)
```
## Objectives
After this chapter, you should (know / understand / be able to ):

- Conduct a formal exploratory data analysis on multivariate data using geoms from `ggplot`
- Create and interpret a scatterplot between two variables 
- Create and interpret a Q-Q plot
- Create and interpret directional bias in a Tukey mean difference plot
- Create and extract descriptive statistics and qualitative information from Boxplots


## Bivariate Data {#bivariate}
Whereas univariate data analyses are directed at "getting to know" the observations made for a single variable, bivariate (and multivariate) analyses are designed to examine the *relationship* that may exist between two (or more) variables. Like the Chapter on Univariate data, we will focus first on ***data exploration*** - a key step towards "getting to know" your data and one that should always proceed inferential statistics or "making conclusions about your data".

``` {block, type="rmdnote"}
Bivariate means two variables where the observations are paired (i.e., each time an observation is made we sample a value for both variables so that they are linked by place/time/observation).
```

## Scatterplot {#scatt}

Undoubtedly, you have seen scatterplots many times before but we will give them a formal treatment here. The **scatterplot** allows you to assess the strength, direction, and type of relationship between two variables of interest. This can be important for determining factors like:  

* Correlation  
* Linearity  
* Performance (of a measurement) in terms of precision, bias, and dynamic range   

Traditionally, a scatterplot shows paired observations of two variables with the ***dependent variable*** on the y-axis and the ***independent variable*** on the x-axis.  Creating a plot in this way means that, before you begin, you must make a judgement call about which variable *depends* on which.  The roots of this terminology/protocol lie in the practice of *linear regression* and the scientific method, the former of which we will discuss in more detail later.  For the purposes of exploratory data analysis, however, it actually doesn't matter which variable goes on which axis. That said, since we don't wish to break with tradition, let's agree to follow the dependent/independent variable guidelines so as not to invoke the wrath of the statistics gods.

**Statistics:**  
- The independent variable (x-axis) is thought to have some influence/control over the dependent variable (y-axis)

**Scientific Method:**  
The experimenter manipulates the control (independent, x-axis) variable and observes the changes in the response (dependent, y-axis) variable 

**Exploratory Data Analysis:**  
- We throw two variables onto a plot to investigate their relationship.  We make a guess about which one is the independent variable (x-axis) and which one is the dependent variable (y-axis) and we hope that nobody calls us out if we got it wrong.

### Causality
All this talk about **dependent** and **independent** variables is fundamentally rooted in the practice of ***causal inference*** reasoning: the ability to say that "action A" caused "outcome B".  Discovering (and proving) that one thing caused another to happen can be an incredibly powerful event.  It leads to the awarding of Nobel Prizes, the creation of new laws and regulations, guilt or innocence in court, the changing and convincing of human minds and behaviors, and simply put: more understanding.

A full treatment of causal inference reasoning is beyond the scope of this course, but we will, from time to time, delve into this topic.  The art of data science can be a beautiful and compelling way to demonstrate causality....but most of us need to learn to crawl before we can walk, run, or fly.  For now, let's put aside the pursuit of causation and begin with ***correlation***.

### Correlation
The scatterplot is a great way to visualize whether (and how) two variables are correlated.  

``` {block, type="rmdnote"}
Correlation: a mutual relationship or connection between two or more things; the process of establishing a relationship or connection between two or more measures.
```

``` {r corr_plots1, warning=FALSE, echo=FALSE, include=FALSE}

corr_plots1 <- tibble(x = runif(100, 0, 100),
                      y1 = x,
                      y2 = map_dbl(x, ~ (.x*(rnorm(1, mean = 1,  sd = 0.03)) + rnorm(1, mean = 0, sd = 2))),
                      y3 = map_dbl(x, ~ (.x*(rnorm(1, mean = 0.6,  sd = 0.2)) + rnorm(1, mean = 15, sd = 15))),
                      y4 = runif(100, 0, 100)
) %>%
  pivot_longer(y1:y4, names_to = "type") %>%
  mutate(types = as.factor(type))

levels(corr_plots1$types) <- c("Perfect Correlation", "Strong Correlation", "Moderate Correlation", "No Correlation")
  

p1 <- ggplot(data = corr_plots1,
                          aes(x = x, y = value)) +
  geom_point(aes(color = types)) +
  facet_wrap(facets = vars(types),
             ncol = 2) +
  theme_bw() +
  coord_fixed(ratio=1.0) +
  labs(x = "Independent Variable",
       y = "Dependent Variable") +
  theme(legend.position = "none")
 
ggsave("./images/correlation_example_1.png", dpi = 150)
```
Below are four examples of bivariate data with differing degrees of correlation: perfect, strong, moderate, and none.  These are qualitative terms, of course, what is "moderate" to one person may be poor and unacceptable to another.  Later on, in the modeling section \@ref(#model), we will discuss ways to assess the strength of correlation more quantitatively.
``` {r corr-example-1, echo = FALSE,  out.width = "700pt", fig.align = "center", fig.cap = "Scatterplot examples showing bivariate data with varying degrees of correlation."}
knitr::include_graphics("./images/correlation_example_1.png")
```
``` {r corr_plots2, warning=FALSE, echo=FALSE, include=FALSE}

corr_plots2 <- tibble(x = runif(100, 0, 100),
                     y1 = map_dbl(x, ~ (.x*(rnorm(1, mean = .8,  sd = 0.15)) + rnorm(1, mean = 10, sd = 10))),
                     y2 = map_dbl(x, ~ -(.x*(rnorm(1, mean = 0.6,  sd = 0.06)) - rnorm(1, mean = 70, sd = 5))),
                     y3 = map_dbl(x, ~ (.x*(rnorm(1, mean = 1,  sd = 0.02))^2 + rnorm(1, mean = 0, sd = 2))),
                     y4 = map_dbl(x, ~ (100*exp(-.x/20)*(rnorm(1, mean = 1,  sd = 0.03)) + rnorm(1, mean = 4, sd = 2)))
) %>%
  pivot_longer(y1:y4, names_to = "type") %>%
  mutate(types = as.factor(type))

levels(corr_plots2$types) <- c("Positive Correlation", 
                               "Negative Correlation", 
                               "Linear Correlation", 
                               "Non-Linear Correlation")
  

p2 <- ggplot(data = corr_plots2,
                          aes(x = x, y = value)) +
  geom_point(aes(color = types)) +
  facet_wrap(facets = vars(types)) +
  theme_bw() +
  coord_fixed(ratio=1) +
  theme(legend.position = "none") +
  labs(x = "Independent Variable",
       y = "Dependent Variable")
 
ggsave("./images/correlation_example_2.png", dpi = 150)
```
In addition to the strength of the correlation, the sign and form of the correlation can vary, too:  
  - **positive correlation**: the dependent variable *trends in the same direction* as the independent variable   
  - **negative correlation**: the dependent variable *decreases* when the independent variable *increases*  
  - **linear correlation**: the relationship between the two variables can be shown with a straight line  
  - **non-linear correlation**: the relationship between the two variables is curvilinear  

``` {r corr-example-2, echo = FALSE, out.width = "700pt", fig.align = "center", fig.cap = "Scatterplot examples showing bivariate data with varying types of correlation."}
knitr::include_graphics("./images/correlation_example_2.png")
```


## Q-Q Plot (Quantile-Quantile) {#QQ}

## Tukey Mean Difference Plot {#Tukey}
* The Tukey mean-difference plot (also known as a Bland-Altman plot) shows **Difference** on the y-axis vs. **Mean** on the x-axis for two paired measurements.  This plot is most often used as a way to compare two instruments that measure the same thing.  The concept was introduced by the famous statistician John Tukey, and popularize in the field of clinical chemistry by Bland and Altman.    
* The plot helps one examine agreement between two measurement methods as a function of scale.    
* The plot is also useful for evaluating bias and precision (and *accuracy* if one of the instruments happens to be a "gold-standard" reference measure).  

## Boxplot (#box)
A boxplot *can be created* for univariate data, but single boxplots are quite boring by themselves (and the same information presented by a boxplot is just as easy to get from a cumulative distribution plot for univariate data). 

## Exploring Multivariate Data
