# Exploratory Data Analysis I: Univariate {#eda1}



> *<span style="color: blue;"> Exploratory data analysis is graphical detective work - John Tukey </span>*

Exploratory Data Analysis (also known as EDA) is largely a visual technique based on the human trait of *"pattern recognition"*. The purpose of EDA is simple: learn about your data through diagnostic visualization.

**Why is Exploratory Data Analysis (EDA) useful?**
Because getting to know a dataset is a key step towards making sense of it, and EDA is a great way to familiarize oneself with a dataset. As a result, you can use EDA to do many powerful things in a short span of time.

Think of the word **"Exploratory"** in terms of **"Hypothesis Building"**.

- In other words, **modeling** and **statistical inference testing** should come *after* EDA.  
- In other, other words: You can use EDA as a means to jump start ideas on *"what do I do with my data?"*!  

> *<span style="color: blue;"> Perfect data is boring. Flawed data, on the other hand, is interesting and mysterious. Perfect data doesn't get asked out on a second date.  - me </span> *

## Univariate Data

*Univartiate* means "only one variable" being considered (like medical doctors, statisticians enjoy the use fancy words to describe things...).  

*Univariate data analyses* describe ways to discover features about a single variable or quantity. While simple, univariate analyses are a great starting point for EDA because they allow us to isolate a variable for inspection. The variation in diameter of a mass-produced component, or pollutant concentration in the atmosphere, or the rate of a beating heart are all examples of things we might examine in a univariate sense. This is, of course, a potentially risky procedure because, as engineers, we are taught about *mechanisms* and *dependencies* that imply that variable A is inextricably linked to variable B through some physical process. That's OK to admit.  Univariate analyses are still useful. Trust me for now or just skip ahead to multivariate analyses...your choice.

### Location, Dispersion, and Shape
Univariate EDA often begins with an attempy to discover three important properties about an observed variable: its location, dispersion, and shape. Once you define these properties (the "what"), you can begin to probe the underlying causes for these properties (the "how" and "why"). Discovering "how" and "why" of a variable's location, dispersion, and shape might sound simple, and yet, answering such questions often represents the pinnacle of scientific discovery (read: they give out nobel prizes for that stuff).  Let's start with "what" (location, dispersion, and shape) and build from there.

The **location** of univariate data means: where do most of the values/observations fall? Do they tend to be large or small, based on what you know about the variable?

The **dispersion** of the data refers to its variability.  Are the values tightly bound in a small range or do they vary widely from one observation to the next? Note that the phrase "varies widely" is contextual.  The variation in the cost of an ice cream cone from one location to the next might look small to you but could mean the difference between joy and sorrow to a 10-year-old with only a $1.50 in their pocket... 

The **shape** of the distribution is actually a combination of location and dispersion but with some mathematical nuance. Knowing the shape of your data distribution means that you have insight into its *probability density function*. One wishes to know the "shape" of the populaiton distribution they are attempting to observe because once you know a distribution's shape, you can model it.  And if you can model the distribution, you can begin to make inferences about it (read: make extrapolations, predictions, or other types of inference). 

The shape of a distribution of data is often categorized based on whether it looks similar to one of many ***[reference distributions](#dist)***.  You can think of reference distributions like species of living organisms; there are lots out there but once you categorize one you can likely predict its behavior.  In other words, if the shape of your data matches a reference distribution, most of your modeling work is already done! Examples of reference distributions include the normal distribution, the  lognormal distribution, the uniform distribution etc. More on different types of *reference distributions* [here](#dist). 

### Example: Location and Dispersion  

Let's plan a camping trip.  Our trip is purely theotrical...so let's not worry  about costs, logistics, or other important factors. For this exercise we only care about *comfort* while outdoors.  When I think of being comfortable outdoors, the first thing that comes to mind is ***temperature***.  Did I bring the proper clothing?

We will consider going camping in two lovely spots: the first is the forest preserves along the Na Pali coast in Kauai, Hawaii and the second is the sunny hiking/climbing regoin around Jack's Canyon in southwest Colorado. Let's examine the location and dispersion of hourly temperatures in these two regions for the month of July, 2010  We can download such data from the <NOAA Climate Data Center>[https://www.ncdc.noaa.gov/cdo-web/search] that provides  daily average, minimum, and maximum temperature readings for these two regions.  Once we know the **location** and **dispersion** of these data, we can decide what clothes to bring!

We create a simple plot \@ref(fig:NOAA-temps-2) that summarizes the various temperatures measured at these two regions.  



<div class="figure" style="text-align: center">
<img src="./images/NOAA_temps.png" alt="Hourly temperature levels in Colorado and Hawaii for the month of July, 2010." width="700pt" />
<p class="caption">(\#fig:NOAA-temps-2)Hourly temperature levels in Colorado and Hawaii for the month of July, 2010.</p>
</div>

First, if we are picking spots to go camping, both locations have daily averaage temperatures that seem very pleasant (the average is about 78.9 $^\circ$F in Colorado and 78.6 $^\circ$F in Hawaii). When we calculate an average value we are creating an indicator of a variable's **location**.  However, if we look at the **dispersion** of temperature observations (how the readings vary across the month), we can make an important distiction: the range of observed temperature values in Hawaii is fairly narrow, whereas the range in Colorado spans from 64.4 to 
91.8 $^\circ$F!  

The conclusion to be drawn here is that the ***central tendencies*** of hourly temperature are nearly identical between Hawaii and Colorado but the ***dispersion*** of the temperature data suggests that you might want to pack more gear if you want to be comfortable in Colorado (see \@ref(fig:NOAA-temps-annotate). 

You will discover that there are many ways to communicate location (e.g., mode, mean, median) and dispersion (e.g., range, IQR, standard deviation); we will touch upon several such descriptors in this course.

<div class="figure" style="text-align: center">
<img src="images/NOAA_temps_annotate.png" alt="The Hawaii and Colorado temperature data have similar locations (or central tendencies) but differing dispersion (or spread)." width="700pt" />
<p class="caption">(\#fig:NOAA-temps-annotate)The Hawaii and Colorado temperature data have similar locations (or central tendencies) but differing dispersion (or spread).</p>
</div>


## Quantiles

Let's assume you have a sample of univariate data.  A good starting point for exploring these data is to break them into *quantiles* and extract some basic information.  Quantiles allow you to see things like the start, middle, and end rather quickly, because the first step of quantile calculation is to sort your data from smallest to largest value.

<div class="rmdtip">
<p>A Quantile represents a fractional portion of an ordered, univariate distribution.</p>
</div>

In other words, quantiles break a set of observations into similarly sized 'chunks' where each chunk represents an equal fraction of the total distribution from start to finish. If you line your data up from smallest to largest and then slice that list into sections, you have created a set of quantiles. For example, when a distribition is broken into 10 equal chunks, or *deciles*, the first quantile (the 10^th^ % or the 0.1 fraction) represents the value that bounds the lower 10 % of the observed data. The second quantile (0.2 fraction) represents the 20^th^% value for the observed data. The 0.3 quantile represents the 30^th^%, and so on. There are two important aspects to remember about quantiles: (1) Each quantile is defined only by its upper-end value; (2) quantiles are defined after the data have been *rank ordered* from lowest to highest value.  

Quantiles are often used to communicate ***descriptive statistics*** for univariate data:

The two most extreme quantiles define the ***Range***:  

- `Min`: the 0% or lowest value; the zeroth quantile 
- `Max`: the 100^th^ percentile (or 1.0 in fractional  terms); the highest value observed; the n^th^ quantile   

If you break a distribution into quarters, you have created *Quartiles*.  Let's generate a sample of random numbers between 0 and 100 and break the resulting data into quartiles. A random number generator (`runif`) will create a **[uniform distribution](#unif_dist)** across the sample range since all values have equal probability of being chosen.  With that in mind, you can probably guess what the quartiles will look like, given a sufficiently large sample...


```r
set.seed(1)
univar1 <- runif(n = 1000, min = 0, max = 100)
univar1_quartiles <- quantile(univar1, probs = seq(0, 1, 0.25)) %>% round(1)
```

<div class="figure" style="text-align: center">
<img src="./images/quantile_1.png" alt="Quartiles estimated from n=1000 samples of a uniform distribution between 0 and 100" width="700pt" />
<p class="caption">(\#fig:quantiles-2)Quartiles estimated from n=1000 samples of a uniform distribution between 0 and 100</p>
</div>
As expected, the 1^st^, 2^nd^, 3^rd^, and 4^th^ quartiles from a uniform distribnution between 0 and 100 fall into predictable chunks: 25, 50, 75, and 100.

- The first, or lower quartile, contains the lower 25% of the distribution. In quantile terms, we define this quartile by its upper value, which occurs at the 25^th^ percentile (or the 0.25 quantile).   
- The second quartile is defined at the 50^th^ percentile value. Because this quantile ends at 0.5 (the 50^th^ percentile) it also represents the ***median*** of the distriution.  *Note: to define the data that falls into the second quartile, you actually need to know the 1^st^ and 2^nd^ quartile values, since you want the range of values that start at the 25^th^ percentile and end at the 50^th^ percentile. 
- the third quartile contains data for the 0.5 to 0.75 quantiles (50^th^ to the 75^th^%) 
- the fourth quartile contains the upper 25% of the distribution: 0.75 to 1.0 

### Quantiles and Descriptive Statistics

Quantiles allow us to calculate several important **descriptive statistics** for univariate data. For example, the quartile output above allows us to report the following descritptives:



<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>(\#tab:unnamed-chunk-2)Quantiles and descriptive stats 
             for n=1000 samples from a uniform distribution, 0-100</caption>
 <thead>
  <tr>
   <th style="text-align:center;"> Quantile </th>
   <th style="text-align:center;"> Descriptor </th>
   <th style="text-align:center;"> Example Values </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> minimum </td>
   <td style="text-align:center;"> 0.1 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> maximum </td>
   <td style="text-align:center;"> 100.0 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.5 </td>
   <td style="text-align:center;"> median </td>
   <td style="text-align:center;"> 48.3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.25 </td>
   <td style="text-align:center;"> 25^th^% </td>
   <td style="text-align:center;"> 25.8 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.75 </td>
   <td style="text-align:center;"> 75^th^% </td>
   <td style="text-align:center;"> 74.7 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.75 - 0.25 </td>
   <td style="text-align:center;"> IQR </td>
   <td style="text-align:center;"> 48.9 </td>
  </tr>
</tbody>
</table>

A couple notes.  You might wonder why I didn't include *summary statistics* (like the mean and standard deviation) in this list of descriptive statistics.  The reason is not because I don't like using summary statistics, but because their use implies that you know the type/shape of the distribution you are describing (i.e., is it normally distributed, log-normal, bi-modal, etc.).  Use of descriptors like `mean()` and `sd()` are very useful when properly applied, but can be misleading when the distribution is skewed (**more on that later**).  Quantile descirptors, on the other hand, are agnostic to the type of distribution they describe (i.e,. the median is ALWAYS the 50% value, regarless of the shape of the distribution). 

The last descriptor in the list is the *"Inter-Quartile Rage"* or IQR for short; the IQR describes the spread of the data and communicates the range of values needed to go from the 25^th^% to the 75^th^% of the distribution. The IQR is siad to span the bulk, or "middle part" of the distribution.  The IQR is *similar* in concept to a standard deviation but makes no assumptions about the shape/type of the distribution being considered.  

Let's now create quantiles from a *normal distribution* of data (mean = 50, standard deviation = 15).  We'll start by randomly sampling 1000 values (using `rnorm`) and then arranging them with the `quanitle` function (the code chunk also shows how to calculate quantiles manually).

```r
normal1 <- rnorm(1000, mean = 50, sd = 15)
quantile(normal1, probs = seq(0, 1, 0.1)) %>%
  round(0)
```

```
##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% 
##    1   30   36   42   45   49   53   58   63   70  105
```

```r
#manual method for calculating quantiles
normal_data_2 <- tibble(
  sample_data = normal1,  #start with raw sample data
  sorted_data = sort(normal1),   #sort the data
  cum_frac = seq.int(from = 1/length(normal1),   #calculate cumulative fraction for each entry
                     to = 1,
                     by = 1/length(normal1)))


deciles <- seq.int(from = 0, to = 1, by = 0.1)

#take only the quantiles that match the decile values created above
normal_data_deciles <- filter(normal_data_2, cum_frac %in% deciles)
```
Normal distributions are more complicated than uniform distributions, and it's hard to get a lot out of the decile summary when shown in tabular format.  This brings us to my favorite part of exploratory data analysis: ***data visualization***.

## Univariate Data Visualization

Data visualization is a powerful technique for exploring data. Visualizaing univariate data is the **first step** in getting to know your data, once they have been read into R and cleaned.f

Below I introduce formal approaches to visualizing the location, dispersion, and shape of univariate data.  Later on we will explore relationships between variables in [multivariate data analyses](#eda2). The techniques shown below are simple, yet I am surprised how often they are underemployed.

## Cumulative Distribution Plot

A **cumulative distribution plot** lays out the raw quantiles (typically on the y-axis) against the observed data (x-axis). Cumulative distribution plots are nice because they are **enumerative** (a fancy word that means they show ***ALL*** of your data without any modificaitons). Bevause these plots show all the data, each observation gets assigned a unit quantile (i.e., if there are 100 data points in a sample, each datum represents 1% of the total or a quantile of 0.01) . Thus, quatiles are shown on the y-axis and the x-axis shows the observed data, ordered from smallest to largest value.   

<div class="rmdnote">
<p>The cumulative distribution plot shows the rolling fraction (or percent) of data (y-axis) that are “less than or equal to” the (x-axis) values of that data.</p>
</div>

An annotated cumulative distribution plot is shown below for a sample of temperature data that follow a normal distribution:  



<div class="figure" style="text-align: center">
<img src="./images/normal_cum_dist_anno.png" alt="Annotated Cumulative Distribution Plot for Normally-Distributed Temperature Data" width="700pt" />
<p class="caption">(\#fig:norm-cdf-plot)Annotated Cumulative Distribution Plot for Normally-Distributed Temperature Data</p>
</div>

As you can see, there is a lot of information contained in a cumulative distribution plot!  We can see the range (min, max), the median (the 0.5 quantile), and any other percentile values for the distribution.  We can quickly pick out any value to and find out what fraction of the data are below it.  For example, picking out the quantile of 0.9 on the y-axis shows that 90% of the observed temperature values are below a level of 21 $^\circ$C. In fact, most of the basic descriptive statistics can be accessed on a cumulative distribution plot:  

* **median**: x-axis value at quantile 0.5  
* **25^th^ and 75^th^ percentiles** (or quartiles): x-axis values at quantile 0.25 and 0.75, respectively  
* **interquartile range** (IQR): x-axis distance between quantiles 0.25 and 0.75  
* **range**: min, max values at quantile 0 and 1.0, respectively  
  
Cumulative distribution plots also reveal details about the dispersion of the observed data (and, more subtly, about the shape/nature of underlying data distribution). For example, in the temperature example above (Fig. \@ref(fig:norm-cdf-plot)), the curve has geometric symmetry about the median - this symmetry indicates that the dispersion of the data is equal as you move away from the median in either direction. 

### Cumulative Distribution Plot Example

In the example below, we create a cumulative distribition plot of annual salaries reported by individuals with a degree in Mechanical Engineering in the US. These data come from the [NSF Survey of College Gradautes](https://www.nsf.gov/statistics/srvygrads/). I've also taken the liberty of identifying the max, min, and quartile values on the plot.


```r
#load data
raw_salaries <- read_csv("./data/ME_salaries.csv", 
                 col_types = "iif", 
                 col_names = c("salary", "age","gender"),
                 skip = 1)

#create cumulative fractions 
ordered_salaries <- raw_salaries %>%
  select(salary) %>%
  filter(salary < 500000) %>% #to remove numeric identifiers like '989961'
  arrange(salary) %>%
  mutate(cum_pct = seq.int(
    from = 1/length(salary), 
    to = 1, 
    by = 1/length(salary))) 
  
  #alternate method if you don't want to calculate cumulative fractions
  cdf_ME_salaries <- ggplot(data = ordered_salaries, aes(x = salary)) +
  geom_step(stat = "ecdf") + #ecdf = 'empiricial cumulative distribution function'
  theme_bw() +
  labs(x = "Salaries of ME Graduates", y = "Cumulative Fraction") +
  scale_y_continuous(limits=c(-0.05, 1.03), expand = c(0,0),
                     breaks = seq(from = 0,
                                        to = 1,
                                        by = 0.1)) +
  scale_x_continuous(labels = scales::label_dollar(scale = 0.001, 
                                                   prefix = '$', 
                                                   suffix = 'k'),
                     minor_breaks = seq(from = 0,
                                  to = 450000,
                                  by = 10000))+
  geom_segment(data = data.frame(x=quantile(ordered_salaries$salary),
                                 y=rep.int(-.05, 5),
                                 xend=quantile(ordered_salaries$salary),
                                 yend=seq(from = 0, to = 1, by = 0.25)),
               aes(x=x, y=y, xend=xend, yend=yend), 
               color = "red",
               linetype = "dashed")
ggsave("./images/cdf_me_salaries.png", dpi = 150)
```

```
## Saving 7 x 5 in image
```

<div class="figure" style="text-align: center">
<img src="./images/cdf_me_salaries.png" alt="Annual Salaries Reported by US Graduates (all ages) with a Mechanical Engineering Degree" width="700pt" />
<p class="caption">(\#fig:cdf-me-salaries)Annual Salaries Reported by US Graduates (all ages) with a Mechanical Engineering Degree</p>
</div>
The good news is that if you can survive the undergraduate grind in Mechanical Engineering, you have a good chance of making a great living wage.  The median salary is $
96,000 - note this is for graduates of all ages across the country in the year 2017.  Don't expect that paycheck on your first day on the job!

One interesting thing you may notice in Figure \@ref(fig:cdf-me-salaries) is that the 'tails' of the plot (the shape of the curves about the median value) are not symmetrical. One tail looks longer than the other.  This lack of symmetry implies a degree of *skewness* in the data.  Why is that?  We will explore skewness at several points in this course (and discuss some of the mechanisms that cause data to be skewed later on).

## Histogram {#hist}

A **histogram** is a plot of counts (or frequency of observations) as a function of magnitude (or levels). Histograms are useful because they allow you to "see" the spread and shape of your data as distribution. A histogram can tell you several important things about a variable:

* The location or central tendency (what's the most common value?)  
* The approximate range of the data (what are the max, min values?)  
* The dispersion (how spread out are the data?)  
* The nature of the distribution (i.e., do the data appear normally distributed or skewed?)  
* The presence (or absence) of outliers in the data (what are those observations doing way out there?)  

Below is a basic histogram that I've annotated to show some key features.  This histogram was created by sampling a normal distribution `rnorm()` and plotted using `ggplot::geom_histogram`.  Examination of this histogram quickly reveals a lot about the underlying data.  For example, the central tendency is located around a value of ~30 and the majority of the data appear to fall between a range of 20 - 40.  


<div class="figure" style="text-align: center">
<img src="./images/hist_anno_1.png" alt="A histogram is a plot of counts (or frequency of observations) as a function of magnitude (or levels) of univariate data" width="700pt" />
<p class="caption">(\#fig:hist-anno-1)A histogram is a plot of counts (or frequency of observations) as a function of magnitude (or levels) of univariate data</p>
</div>
  
### Histogram: what to look for?
In addition to being able to "see" key data descriptors (e.g., mean, range, spread), a histogram also allows one (a) to get a feel for ***skewness*** (whether the distribution is symmetric about the central tendency), (b) to see ***outliers***, and (c) to visualize other potential interesting features in the dataset.  



<div class="figure" style="text-align: center">
<img src="./images/hist_anno_2.png" alt="A histogram depicting a skewed distribution" width="700pt" />
<p class="caption">(\#fig:skew-hist-out-plot)A histogram depicting a skewed distribution</p>
</div>
Figure \@ref(fig:skew-hist-out-plot) depicts just such a skewed distribution. Examination of Figure \@ref(fig:skew-hist-out-plot) also reveals the presence of potential ***outliers*** in the data: the apparent spike in observations around x = 200. Outliers are interesting features and should neither be ignored nor deleted outright. Outliers exist for some reason, and oftentimes only through detective-work or mechanistic knowledge can you elucidate their source (we will delve into the handling of outliers later on; for now it's enough to know that they exist).  

In Figure \@ref(fig:skew-hist-out-plot) the central tendency is evident, but the spread of the data are not symmetrical about that central tendency.  One outcome of skewed data is that traditional location measures of  central tendency (mode, median, mean) do not agree with each other:  

Figure \@ref(fig:skew-hist-out-plot) central tendency calculations:  

* mode: 14  
* median: 53  
* mean: 87  

<div class="figure" style="text-align: center">
<img src="./images/hist_anno_3.png" alt="Describing the spread of skewed data with a &quot;standard deviation&quot; can lead to confusion and trouble: in this case the existence of physically impossible values." width="700pt" />
<p class="caption">(\#fig:skew-hist-out-plot-2)Describing the spread of skewed data with a "standard deviation" can lead to confusion and trouble: in this case the existence of physically impossible values.</p>
</div>

Another, potentially more troubling, outcome of skewed data is that certain measures of the spread of the data (such as the standard deviation) can lead to misleading conclusions.  For example, if we calculate and extend "one standard deviation" to the mean of the data in Figure \@ref(fig:skew-hist-out-plot-2), we would imply the existence of negative values (87 - 113 = -26), which is often impossible in the case of physical measurements. This is a common mistake in data analysis: the application of "normal" descriptors like `mean` and `sd` to "non-normal" data. 

Skewed distributions can be more challenging to handle because they are *less predictable* than normal distributions (*that said, statisticians have figured out how to model just about any type of distribution, so don't fret*). In any case, we can always rely on quantile calculations for descriptors (median and IQR) since those descriptors do not make assumptions about the shape of the distribution in question.

### What causes skewed histograms?
I want to spend some time thinking about mechanisms that cause variability in an observation. These mechanisms are often responsible for whether a set of observations appears normally distributed or skewed. We will focus on two types of distributions for now: [normal](#normal_dist) and [log-normal](#log_normal_dist) data. Many other types of data distributions exist in the real world.  

A general "rule of thumb" is that *additive* variability tends to produce normally distributed data, whereas mechanisms that cause *multiplicative* variability tend to produce skewed data (log-normal, in the case of Figure \@ref(fig:skew-hist-out-plot)).  By *additive*, I mean that variable x tends to vary in a "plus/minus" sense about its central tendency.  Examples of additive variability include the distribution of heights measured in a sample of 3^rd^ graders or the variability in the mass of a 3D-printed part produced 100 times by the same machine.  

*Multiplicative* (or log-normal) variability arises when the mechanism(s) controlling the variation of "x" are multipliers (or divisors) of x. Many real-world phenomena create multiplicative variability in observed data: the strength of a WiFi signal at different locations within a building, the magnitude of earthquakes measured at a given position on the Earth's surface, the size of rocks found in a section of a riverbed, or the size of particles found in air. All of these phenomena tend to be governed by multiplicative factors. In other words, all of these observations are controlled by mechanisms that suggest x = a*b*c not x = a+b+c. Explicit examples of *multiplicative* variability (and how it leads to log-normal data) are provided [here](#log_normal_dist). 

### Probability Density Function {#pdf}  
The histrogram is really an attempt to visualize the **probability density function** (pdf) for a distribution of univariate data.  The pdf is a mathematical representation of the likelihood of sampling a given value from the distribution, assuming a random draw.  Unlike the histogram (which uses bins to describe counts/frequency for data values), however, the pdf is a continuous function that can be solved at any data value.  A pdf is sometimes expressed empirically (as a numeric approximation) or as an exact analytic equation (in the case normal, lognormal, and other *known* distributional forms). We will discuss the creation and visualization of pdfs in Chapter ... 

## Time-Series  {#time_series}
The **time-series plot** plot (also called a **"run sequence"** plot) is something you have undoubtedly seen before. This plot depicts the value of a variable (y-axis) plotted in the sequence that the observations were made (x-axis, often shown as units of time). A time-series plot allows you to do several things:  

* Identify shifts in "location" of the variable (when and by how much do values change over time?)  
* Identify shifts in "variation" of the variable (does the data get more/less noisy over time?) 
* Identify (potential) outliers and when they occurred 
* Identify the ***timing*** of events!

Let's plot the same temperature data from Hawaii as shown in \@ref(fig:NOAA-temps-2) as a time series.

```r
# plot Temp vs Time for Colorado data
NOAA_HI_time_series<- ggplot(data = filter(NOAA_temps, location == "Hawaii"), 
       aes(x = date, y = temp_hr_f)) +
  geom_line(linejoin = "round",
            color = "purple") +
  geom_point(size = 1,
             shape = 1,
             color = "purple") +
  labs(x = "Date",
       y = "Temperature, °F") +
  theme_bw()
ggsave("./images/NOAA_HI_time_series.png", dpi = 150)
```

<div class="figure" style="text-align: center">
<img src="./images/NOAA_HI_time_series.png" alt="Time series of hourly temperature measurements in Kauai, Hawaii for July 2010." width="700pt" />
<p class="caption">(\#fig:NOAA-HI-time-series)Time series of hourly temperature measurements in Kauai, Hawaii for July 2010.</p>
</div>

**What can we see in these data?**  Here are some quick conclusions:  
  
1. The temperature appears to rise and fall in a predictable pattern each day (duh).
2. Over the course of the month, we can see a gradual increase in daily mean temperature taking place.  

Neither of these conclusions are groundbreaking, but they both allude to an important outcome: there are patterns to be seen in these data!  None of those patterns were evident in the cumulative distribution or histogram plots.  If we zoom in on just the first 5 days of the month, we see the daily temperature patterns in more detail. 
    

```r
# plot Temp vs Time for Colorado data
NOAA_HI_time_series<- ggplot(data = filter(NOAA_temps, location == "Hawaii", date <= "2010-07-05"), 
       aes(x = date, y = temp_hr_f)) +
  geom_line(linejoin = "round",
            color = "purple") +
  geom_point(size = 1,
             shape = 1,
             color = "purple") +
  labs(x = "Date",
       y = "Temperature, °F") +
  theme_bw()
ggsave("./images/NOAA_HI_time_series-2.png", dpi = 150)
```

<div class="figure" style="text-align: center">
<img src="./images/NOAA_HI_time_series-2.png" alt="Time series of hourly temperature measurements in Kauai, Hawaii for July 1-5, 2010." width="700pt" />
<p class="caption">(\#fig:NOAA-HI-time-series-2)Time series of hourly temperature measurements in Kauai, Hawaii for July 1-5, 2010.</p>
</div>



## Autocorrelation {#autocorr}

Autocorrelation, or serial correlation, means *correlated with oneself across time*.  Autocorrelation is a concept that suggests *"whatever is happening in this moment is still likely to be happening in the next moment."* Lots of things are autocorrelated: your mood right now is likely related to your mood 15 minutes from now (or 15 minutes ago).  The weather outside at 08:00 on a given day is a good predictor of the weather outside an hour later at 09:00, or two hours later at 10:00, and so on, but less so in fourteen days hence. 

Almost all physical phenomona have a degree of autocorrelation and that's important because if a datapoint at time 'i' is correlated with time 'i + 1' then those two datapoints are NOT independent. Yet, many statistical tests assume that the underlying data ARE independent from one another!  Thus, autocorrelation can pose a problem when we want to make inferences using data collected over a span of time.  Fortunately, we have ways of detecting the presence (and relative strength) of autocorrelation with a simple exploratory plot. 

The **autocorrelation plot** gives the Pearson correlation coefficient (r) between all measurements (x~i~) and their lags (x~i+n~), where *x* represents an observation and *i* represents a point in time.  

This plot can help answer the following questions:  

* Are the data correlated with each other over time?  
    - Note: if the answer is *yes* then your data are *not necessarily independent measures*  
* Are there patterns (periodicity) to discover?  
    - hourly?  
    - daily?  
    - weekly?  
    - seasonally?  
    
Let's examine the Hawaii temperature data shown in Figures \@ref(fig:NOAA-temps-2) and \@ref(fig:NOAA-HI-time-series) using an autocorrelation plot. 

The `stats` package in R has a function to calculate autocorrelation and plot it through time (as a function of lags). Here, the y-axis represents the Pearson correlation coefficient and the x-axis represents time lags (for the basic unit of time in which the data are arranged). 


```r
#subset the datafile to contain only measures from Colorado
NOAA_temps_HI <- NOAA_temps %>% 
  dplyr::filter(location == "Hawaii")

#call the autocorrelation plot
acf(NOAA_temps_HI$temp_hr_f,       
    main = " " ,
    xlab = "Lag (hours)",
    ylab = "Correlation coefficient") 
```

<img src="05-eda1_files/figure-html/acf-1.png" width="672" style="display: block; margin: auto;" />

The plot suggests a fairly **strong level of autocorrelation in these temperature data**.  We should not be surprised, especially after seeing the time series shown in Figures \@ref(fig:NOAA-HI-time-series) and \@ref(fig:NOAA-HI-time-series-2).  Can you explain why the Pearson correlation coefficient is negative at a 12-hr lag?  Why does the coefficient trend back upwards to nearly perfect correlation at 24-hr lags?

Other notes about autocorrelation plots:
* Bars above the blue horizontal lines indicate that autocorrelation is worth attention.  
* One important implication of autocorrelated data is that the data wihtin a given lag duration are **not independent.**  
* This *lack of independence* violates the assumptions of many modeling approaches (that's potentially bad).  

### Partial Autocorrelation  
The **partial autocorrelation plot** gives the correlation coefficient (r) between all timepoints (x~i~) and their lags (x~i+n~) *after accounting for autocorrelation for all previous lags*.  

The partial autocorrelation plot can help you decide: *how far out in time should I go in order to assume that measurment (x~i~) is independent from (x~i+n~)?*  

```r
pacf(NOAA_temps_HI$temp_hr_f,      #call the partial autocorrelation plot
     main = " ", 
     xlab = "Lag (hours)",
     ylab = "Partial Correlation Coefficient") 
```

<img src="05-eda1_files/figure-html/pacf-1.png" width="672" style="display: block; margin: auto;" />

In this case, after a 2-hr lag, the data loses most autocorrelation, except for some periodicity around 12- and 24-hr lags. 

## In-course Exercise





