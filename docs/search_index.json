[
["index.html", "Engineering Data Analysis in R Chapter 1 Introduction 1.1 How to use this book 1.2 Prerequisites 1.3 Grading 1.4 Course set-up 1.5 Coursebook 1.6 Acknowledgements", " Engineering Data Analysis in R John Volckens and Kathleen E. Wendt 2020-10-03 Chapter 1 Introduction Preface I developed this coursebook to help engineers begin to think about data. Most of my job at the university is to conduct research, yet most of the students who show up to my lab don’t know where to begin when presented with data. The irony is that, while engineering students are continuously drilled on how to solve problems, they are rarely taught how to seek them out. My undergraduate advisor, Dr. David Hemenway, once told me: “The data are always trying to tell you something.” This book is an introduction to data listening because listening comes before understanding. In broad terms, scientific and engineering research is about discovery: finding out something new. In practice, however, research is really about failure. Over the short term, researchers fail in the act of discovery much more often than they succeed. Such failure is to be expected because cutting-edge research ain’t easy. Once you come to terms with accepting failure as a regular occurrence, you put yourself in a position to learn from it. To learn from failure requires that you observe and diagnose “what went wrong” and to do that, you need to listen to what the data are telling you. Let’s begin. 1.1 How to use this book The coursebook is intended to be a sort of self-help guide for students who want to learn R programming and the art of engineering data science. The book is designed to get you started in the art, not master it. I’m not qualified to teach mastery in the art of R or engineering data science, so look elsewhere for that level of tutelage. If you are new to these topics, you probably want to start at the beginning and proceed through each chapter sequentially. Some sections or material might seem boring or too easy. In that case, just skip to the end of the section and see if you can complete the exercises and answer the questions. Nearly all of the graphics and data presented in this book were created or manipulated in R. In many places, however, I have hidden the code in order to streamline the message. If you ever wonder “How did he do that?” you can download any of the R Markdowns on the GitHub repository, where this coursebook is hosted. 1.2 Prerequisites This course is intended for upper-level undergraduates who have completed MECH 231 (Experimental Design). 1.3 Grading 1.3.1 Grading for MECH 481A6 Course grades will be determined by the following five components: Assessment component Percent of grade Exams 60 Quizzes 25 Homework 15 1.3.2 Homework submission There will be a homework assignment per coursebook chapter. Each homework will be due at the start of the class period when lecture coverage of the next chapter commences. For example, Chapter 2 Homework will be due at the start of the first lecture regarding Chapter 3. From RStudio, you will upload (“commit”/“push”) your homework to a private folder on our class GitHub Organization. Your commits will be time-stamped; any commit after the start of class on a given day will be considered late, and the assignment will be graded (or not graded) accordingly. Guidance on installing and connecting R/RStudio and Git/GitHub is provided in the next section. 1.4 Course set-up This might be painful, but bear with me. There will be a lot of software development jargon (e.g., “commit”, “push”, “pull”), but the general idea is: We want to set up and learn how to use a collaborative tracking system. Git is a version control system (i.e., Word’s “track changes” feature on steroids). GitHub is the most common Git-based collaborative cloud (i.e., Dropbox on steroids). Everything in this class will be done within an RStudio Project on your local computer that is mirrored on a private GitHub repository. If you do not use GitHub (properly), you will receive an “incomplete” on the given assignment, so it is imperative that you take your time with these steps and read carefully. If you have any questions at each stage, ask! The following guidance regarding R/RStudio and Git/GitHub draws heavily on Jenny Bryan’s book, Happy Git with R, and her related paper on version control. I sprinkled in some suggestions from others. If you would like to gain more background on basic Git and GitHub, take a look at these slides from Dr. Amelia McNamara. 1.4.1 Install R and RStudio Download and install the pre-compiled binary of the most recent version (4.0+) of R appropriate for your machine’s operating system Download and install the most recent, preview version of RStudio; then, navigate to RStudio &gt; Preferences to NOT “Restore .RData into workspace at setup” and NEVER “save workspace to .RData on exit” (it is okay that you might not know what this means yet) If you’ve installed R and RStudio in the past, please download the latest versions and update the R packages with the following code: # if you&#39;ve previously installed R and RStudio, also update your R packages update.packages(ask = FALSE, checkBuilt = TRUE) If you want to generate PDF output from R Markdown documents, you will also need to install LaTex. I suggest taking the following approach in the RStudio Console, if you have never installed LaTex. More installation guidance can be found here. # install R package install.packages(&quot;tinytex&quot;) # install LaTex &quot;ingredients&quot; tinytex::install_tinytex() 1.4.2 Install Git and create a GitHub account Install Git. See here and here for OS-specific installation instructions. For Mac users, you need to install parts of Xcode for Mac OS and other things). For R 4.0+, this process is easier, as R now uses Apple Xcode 10.1 and GNU Fortran 8.2. Create a GitHub account. Pick a good user name! Introduce yourself to Git in RStudio with the following code in the Console. Provide your given name, not your user name, and the email address you used in creating your GitHub account. These commands return nothing, but you can check that it worked with git config --global --list in the shell. # install `usethis` R package if needed (do this exactly once): ## install.packages(&quot;usethis&quot;) library(usethis) usethis::use_git_config(user.name = &quot;John Doe&quot;, user.email = &quot;john.doe@colostate.edu&quot;) Note: Avoid committing credentials or other sensitive information to GitHub by “vaccinating” with usethis::git_vaccinate(). Optional but recommended for new users: Consider downloading a GUI Git Client to make version control easier and to build intuition. GitHub Desktop is likely sufficient for this course, but the choice is yours. Optional: Sign up for free student perks via GitHub Education. 1.4.3 Connect Git, GitHub, and RStudio Read and follow the instructions in Chapters 9-13 exactly. I hope you won’t need to look at Chapter 14! Move slowly and carefully, and pay attention to the specific needs for your operating system. 1.4.4 Create new project, GitHub first Keep in mind: You should save the local R Project from this step in a top-level directory. We suggest creating an R folder at the top of your Documents folder (or OS-specific equivalent) to contain all of your R Projects. For the duration of this course, you should have a directory pathname like /user/Documents/R/[YourLastName]-MECH481A6. We will discuss directory structures and pathnames more in the next chapter. Read and follow these instructions exactly with the following additions: Give your GitHub username to the TA, so you can be added to our GitHub Organization Work with the TA to create a private repository within the organization, labeled [YourLastName]-MECH481A6 Confirm connection between your R Project and the GitHub repository, make subfolders (data, code, figs) within your [YourLastName]-MECH481A6 folder on your local drive, and push them up to GitHub; this supports a project-oriented workflow 1.4.5 Use RStudio/GitHub system for homework submission For homework submission, you will download the R Markdown templates provided to you and save and edit them within your own R Project, which is connected a private repository on the class GitHub Organization. In order to track your changes and communicate with the Instructor and TA, you will regularly commit changes to your R Project files with meaningful commit messages. We will practice commits, pushes, and pulls to the master branch (main copy of R Project) during class. 1.4.6 Confirm successful set-up At this point, you should be able to commit, push to, and pull from the master branch of your private GitHub repository within the RStudio IDE. In later chapters, we will provide more information on these interfaces, and you will have plenty of opportunities to practice this workflow. For now, “minimally functional” is good enough! Once you have successfully installed and connected R/RStudio and Git/GitHub, open an issue on YOUR private repository within the GitHub Organization. Mention/assign Kathleen (TA) @wendtke to let her know everything is working properly, or to request more help. Then, in the public repository for class-related questions and discussion, open another issue. You can ask a question, share any course-related concerns, or post a brief comment about what you hope to gain from this course. Remember to mention/assign Kathleen @wendtke, so she is alerted to your post. Kathleen will then close both of your issues, and you will be ready to go! 1.4.7 Asking for help (properly) All questions regarding technology and code should be directed to the Teaching Assistant via GitHub Issues on this repository. If the question requires you to include full code, please consider using the R package to generate reproducible examples: reprex. Please watch this tutorial on how to use reprex. Essentially, you are copy-pasting self-contained code in the GitHub Issue, so I can recreate your work and help you more effectively. You won’t do things perfectly as you start, but, hopefully by the end of the semester, you will have an efficient, reproducible workflow and effective solution-seeking toolbox. 1.5 Coursebook This coursebook will serve as the only required textbook for this course. I regularly edit and add to this book, so content may change somewhat over the semester. We typically cover about a chapter of the book every 1-2 weeks of the course. You need to follow along and read this book thoroughly. This coursebook includes: Links to the slides presented in class for each topic In-course exercises, typically including links to the data used in the exercise Homework assignments Appendix of reference distributions A list of vocabulary and concepts that should be mastered for each quiz If you find any typos or bugs, or if you have any suggestions for how the book can be improved, feel free to post it on the book’s GitHub Issues. This book was developed using Yihui Xie’s bookdown framework. The book is built using code that combines R code, data, and text to create a book for which R code and examples can be re-executed every time the book is re-built, which helps identify bugs and broken code examples quickly. The online book is hosted using GitHub’s free GitHub Pages. All material for this book is available and can be explored at the book’s GitHub repository. 1.5.1 Other helpful books (not required) The best book to supplement the coursebook and lectures for this course is R for Data Science by Garrett Grolemund and Hadley Wickham. The entire book is freely available online through the same format of the coursebook. You can also purchase a paper version of the book published by O’Reilly for around $40. This book is an excellent and up-to-date reference by some of the best R programmers in the world. There are a number of other useful books available on general R programming, including: R for Dummies R Cookbook R Graphics Cookbook Roger Peng’s Leanpub books Various books on bookdown.org The R programming language is used extensively within certain fields, including statistics and bioinformatics. If you are using R for a specific type of analysis, you will be able to find many books with advice on using R for both general and specific statistical analysis, including many available in print or online through the CSU library. 1.6 Acknowledgements Most of the introductory material for this book was adapted from Dr. Brooke Anderson’s course on R Programming for Research, to whom I owe thanks not only for the materials but for the many helpful discussions. I would also like to acknowledge John Tukey, one of the pioneers of exploratory data analysis, and the creators of the NIST Engineering Statistics Handbook, from which I have drawn many techniques. "],
["rprog1.html", "Chapter 2 The R Programming Environment 2.1 Ch. 2 Objectives 2.2 R and R Studio 2.3 Communicating with R 2.4 R scripts 2.5 The “package” system 2.6 R’s most basic object types 2.7 Chapter 2 Exercises 2.8 Chapter 2 Homework", " Chapter 2 The R Programming Environment 2.1 Ch. 2 Objectives This chapter is designed around the following learning objectives. Upon completing this chapter, you should be able to: Define free and open source software and list some of its advantages over proprietary software Recognize the difference between R and RStudio Describe the differences between base R code that you initially download and “package” code that you use to expand base R Use RStudio to download and install a package from the Comprehensive R Archive Network (CRAN) to your computer Use RStudio to load a package that you have installed within an R session Demonstrate how to access help documentation including vignettes and helpfiles for a package and its functions Demonstrate how to submit R expressions at the console Define the general syntax for calling a function and for specifying both required and optional arguments for that function Describe what an R object is and how to assign an R object a name to reference it in later code Describe how to create vector objects of numeric and character classes Describe how to explore and extract elements from vector objects Describe how to create dataframe objects Describe how to explore and extract elements from dataframe objects Compare the key differences between running R code from the console versus writing and running R code in an R script 2.2 R and R Studio 2.2.1 What is R? R in an open-source programming language that evolved from the S language. The S language was developed at Bell Labs in the 1970s, which is the same place (and about the same time) that the C programming language was developed. R itself was developed in the 1990s-2000s at the University of Auckland. It is open-source software, freely and openly distributed under the GNU General Public License (GPL). The base version of R that you download when you install R on your computer includes the critical code for running R, but you can also install and run “packages” that people all over the world have developed to extend R. With new developments, R is becoming more and more useful for a variety of programming tasks. It really shines in working with data and doing statistical analysis. R is currently popular in a number of fields, including statistics, machine learning, and data analysis. R is an interpreted language. That means that you can communicate with it interactively from a command line. Other common interpreted languages include Python and Perl. Figure 2.1: Broad types of software programs. R is an interpreted language. ‘Point-and-click’ programs, like Excel and Word, are often easiest for a new user to get started with, but are slower for the computer and are restricted in the functionality they offer. By contrast, compiled languages (like C and Java), assembly languages, and machine code are faster for the computer and allow you to create a wider range of things, but can take longer to code and take longer for a new user to learn to work with. Compared to Python, R has some of the same strengths (e.g., quick and easy to code, interfaces well with other languages, easy to work interactively) and weaknesses (e.g., slower than compiled languages). For data-related tasks, R and Python are fairly neck-and-neck, with Julia an up-and-coming option. Nonetheless, R is still the first choice of statisticians in most fields, so I would argue that R has a an advantage, if you want to have access to cutting-edge statistical methods. “The best thing about R is that it was developed by statisticians. The worst thing about R is that…it was developed by statisticians.” – Bo Cowgill, Google, at the Bay Area R Users Group 2.2.2 Free and open-source software “Life is too short to run proprietary software.” – Bdale Garbee R is free and open-source software. Conversely, many other popular statistical programming languages such as SAS and SPSS are proprietary. It’s useful to know what it means for software to be “open-source”, both conceptually and in terms of how you will be able to use and add to R in your own work. R is free, and it’s tempting to think of open-source software just as “free software”. It is a little more subtle than that. It helps to consider some different meanings of the word “free”. “Free” can mean: Gratis: Free as in free beer Libre: Free as in free speech Figure 2.2: An overview of how software can be each type of free (beer and speech). For software programs developed using a compiled programming language, the final product that you open on your computer is run by machine-readable binary code. A developer can give you this code for free (as in beer) without sharing any of the original source code with you. This means you can’t dig in to figure out how the software works and how you can extend it. By contrast, open-source software (free as in speech) is software for which you have access to the human-readable code that was used as in input in creating the software binaries. With open-source code, you can figure out exactly how the program is coded. Open-source software is the libre type of free (Figure 2.2). This means that, with software that is open-source, you can: Access all of the code that makes up the software Change the code as you’d like for your own applications Build on the code with your own extensions Share the software and its code, as well as your extensions, with others Often, open-source software is also free, making it “free and open-source software”, or “FOSS”. Popular open source licenses for R and R packages include the GPL and MIT licenses. “Making Linux GPL’d was definitely the best thing I ever did.” – Linus Torvalds In practice, this means that, once you are familiar with the software, you can dig deeply into the code to figure out exactly how it’s performing certain tasks. This can be useful for finding and eliminating bugs and can help researchers figure out if there are any limitations in how the code works for their specific research. It also means that you can build your own software on top of existing R software and its extensions. I explain a bit more about R packages a bit later, but this open-source nature of R has created a large community of people worldwide who develop and share extensions to R. As a result, you can pull in packages that let you do all kinds of things in R, like visualizing Tweets, cleaning up accelerometer data, analyzing complex surveys, fitting machine learning models, and a wealth of other cool things. “Despite its name, open-source software is less vulnerable to hacking than the secret, black box systems like those being used in polling places now. That’s because anyone can see how open-source systems operate. Bugs can be spotted and remedied, deterring those who would attempt attacks. This makes them much more secure than closed-source models like Microsoft’s, which only Microsoft employees can get into to fix.” – Woolsey and Fox. To Protect Voting, Use Open-Source Software. New York Times. August 3, 2017. You can download the latest version of R from CRAN. Be sure to select the distribution for your type of computer system. R is updated occasionally; you should plan to re-install R at least once a year to make sure you’re working with one of the newer versions. Check your current R version (e.g., by running sessionInfo() at the R console) to make sure you’re not using an outdated version of R. “The R engine …is pretty well uniformly excellent code but you have to take my word for that. Actually, you don’t. The whole engine is open source so, if you wish, you can check every line of it. If people were out to push dodgy software, this is not the way they’d go about it.” – Bill Venables, R-help (January 2004) “Talk is cheap. Show me the code.” – Linus Torvalds 2.2.3 What is RStudio? To get the R software, you’ll download R from the R Project for Statistical Computing. This is enough for you to use R on your own computer. But, for a more user-friendly experience, you should also download RStudio, an integrated development environment (IDE) for R. It provides you an interface for using R, with a lot of nice extras like R Projects that will make your life easier. All of the code chunks shown in this book were produced using RStudio. As Chapter 1 outlined, you should download R first, then the RStudio IDE. RStudio, PBC is a leader in the R community. Currently, the company: Develops and freely provides the RStudio IDE Provides excellent resources for learning and using R (e.g., cheat sheets, free online books) Is producing some of the popular R packages Employs some of the top people in R development Is a key member of The R Consortium in addition to others such as Microsoft, IBM, and Google R has been advancing by leaps and bounds in terms of what it can do and the elegance with which it does it, in large part because of the enormous contributions of people involved with RStudio. 2.3 Communicating with R Because R is an interpreted language, you can communicate with it interactively. You do this using the following general steps: Open an R session At the prompt in the console, enter an R expression Read R’s “response” (i.e., output) Repeat 2 and 3 Close the R session 2.3.1 R sessions, console, and command prompt An R session is an “instance” of you using R. To open an R session, double-click on the icon for the RStudio IDE on you computer. When RStudio opens, you will be in a “fresh” R session, unless you restore a saved session, which is not best practice. To avoid saving work sessions, you should change the defaults in RStudio’s Preferences menu, such that RStudio never saves the workspace to .RData on exit. A “fresh” R session means that, once you open RStudio, you will need to “set up” your session, including loading packages and importing data (discussed later). In RStudio, the screen is divided into several “panes”. We’ll start with the pane called “Console”. The console lets you “talk” to R. This is where you can “talk” to R by typing an expression at the prompt (the caret symbol, “&gt;”). You press the “Return” key to send this message to R. Figure 2.3: Finding the ‘Console’ pane and the command prompt in RStudio. Once you press “Return”, R will respond in one of three ways: R does whatever you asked it to do with the expression and prints the output, if any, of doing that, as well as a new prompt so you can ask it something new. R doesn’t think you’ve finished asking for something, and instead of giving you a new prompt (“&gt;”) it gives you a “+”. This means that R is still listening, waiting for you to finish asking it something. R tries to do what you asked it to, but it can’t. It gives you an error message, as well as a new prompt so you can try again or ask it something new. 2.3.2 R expressions, function calls, and objects To “talk” with R, you need to know how to give it a complete expression. Most expressions you’ll want to give R will be some combination of two elements: Function calls Object assignments We’ll go through both these pieces and also look at how you can combine them together for some expressions. According to John Chambers, one of the creators of the S language (precursor to R): Everything that exists in R is an object Everything that happens in R is a call to a function In general, function calls in R take the following structure: # generic code (this won&#39;t run) function_name(formal_argument_1 = named_argument_1, formal_argument_2 = named_argument_2, [etc.]) Sometimes, we’ll show “generic” code in a code block, that doesn’t actually work if you put it in R, but instead shows the generic structure of an R call. We’ll try to always include a comment with any generic code, so you’ll know not to try to run it in R. A function call forms a complete R expression, and the output will be the result of running print() or show() on the object that is output by the function call. Here is an example of this structure: print(x = &quot;Hello, world!&quot;) ## [1] &quot;Hello, world!&quot; Figure 2.4 shows an example of the typical elements of a function call. In this example, we’re calling a function with the name print. It has one argument, with a formal argument of x, which in this call we’ve provided the named argument: “Hello, world!”. Figure 2.4: Main parts of a function call. This example is calling a function with the name ‘print’. The function call has one argument, with a formal argument of ‘x’, which in this call is provided the named argument ‘Hello world’. The arguments are how you customize the call to an R function. For example, you can use change the named argument value to print different messages with the print() function. Note that the formal argument never changes. print(x = &quot;Hello, world!&quot;) ## [1] &quot;Hello, world!&quot; print(x = &quot;Hi, Fort Collins!&quot;) ## [1] &quot;Hi, Fort Collins!&quot; Some functions do not require any arguments. For example, the getRversion() function will print out the version of R you are using. getRversion() ## [1] &#39;4.0.2&#39; Some functions will accept multiple arguments. For example, the print() function allows you to specify whether the output should include quotation marks, using the quote formal argument: print(x = &quot;Hello world&quot;, quote = TRUE) ## [1] &quot;Hello world&quot; print(x = &quot;Hello world&quot;, quote = FALSE) ## [1] Hello world Arguments can be required or optional. For a required argument, if you don’t provide a value for the argument when you call the function, R will respond with an error. For example, x is a required argument for the print() function, so if you try to call the function without it, you’ll get an error: print() Error in print.default() : argument &quot;x&quot; is missing, with no default For an optional argument on the other hand, R knows a default value for that argument, so if you don’t give it a value for that argument, it will just use the default value provided by the R developer who wrote the function. For example, for the print() function, the quote argument has the default value TRUE. So if you don’t specify a value for that argument, R will assume it should use quote = TRUE. That’s why the following two calls give the same result: print(x = &quot;Hello, world!&quot;, quote = TRUE) ## [1] &quot;Hello, world!&quot; print(x = &quot;Hello, world!&quot;) ## [1] &quot;Hello, world!&quot; Often, you’ll want to find out more about a function, including: Examples of how to use the function Which arguments you can include for the function Which arguments are required versus optional What the default values are for optional arguments You can find out all this information in the function’s helpfile, which you can access using the function ?. For example, the mean() function will let you calculate the mean (average) of a group of numbers. To find out more about this function, at the console type: ?mean This will open a helpfile in the “Help” pane in RStudio. Figure 2.5 shows some of the key elements of an example helpfile, the helpfile for the mean() function. In particular, the “Usage” section helps you figure out which arguments are required and which are optional in the Usage section of the helpfile. Figure 2.5: Navigating a helpfile. This example shows some key parts of the helpfile for the ‘mean’ function. There’s one class of functions that looks a bit different from others. These are the infix operator functions. Instead using parentheses after the function name, they usually go between two arguments. One common example is the + operator: 2 + 3 ## [1] 5 There are operators for several mathematical functions: +, -, *, /. There are also other operators, including logical operators and assignment operators, which we’ll cover later. In R, a variety of different types and structures of data can be saved in objects. For right now, you can just think of an R object as a discrete container of data in R. Function calls will produce an object. If you just call a function, as we’ve been doing, then R will respond by printing out that object. But, we often want to use that object more. For example, we might want to use it as an argument later in our “conversation” with R, when we call another function later. If you want to re-use the results of a function call later, you can assign that object to an object name. This kind of expression is called an assignment expression. Once you do this, you can use that object name to refer to the object. This means that you don’t need to re-create the object each time you need it—instead, you can create it once, and then just reference it by name each time you need it after that. For example, you can read in data from an external file as a dataframe object and assign it an object name. Then, when you need that data later, you won’t need to read it in again from the external file. The “gets arrow” (&lt;-) is R’s assignment operator. It takes whatever you’ve created on the right hand side of the &lt;- and saves it as an object with the name you put on the left hand side of the &lt;-: # generic code-- this will not work [object name] &lt;- [object] For example, if I just type \"Hello, world!\", R will print it back to me, but it won’t save it anywhere for me to use later: &quot;Hello, world!&quot; ## [1] &quot;Hello, world!&quot; If I assign it to an object, I can “refer” to that object in a later expression. For example, the code below assigns the object \"Hello, world!\" the object name message. Later, I can just refer to this object using the name message, for example in a function call to the print() function: message &lt;- &quot;Hello, world!&quot; print(x = message) ## [1] &quot;Hello, world!&quot; When you enter an assignment expression like this at the R console, if everything goes right, then R will “respond” by giving you a new prompt, without any kind of message. There are three ways you can check to make sure that the object was successfully assigned to the object name: Enter the object’s name at the prompt and press return. The default if you do this is for R to “respond” by calling the print() function with that object as the x argument. Call the ls() function, which doesn’t require any arguments. This will list all the object names that have been assigned in the current R session. Look in the “Environment” pane in RStudio. This also lists all the object names that have been assigned in the current R session. Here are examples of these strategies: Enter the object’s name at the prompt and press return: message ## [1] &quot;Hello, world!&quot; Call the ls() function: ls() ## [1] &quot;message&quot; Look in the “Environment” pane in RStudio (see Figure 2.6). Figure 2.6: ‘Environment’ pane in RStudio. This shows the names and first few values of all objects that have been assigned to object names in the global environment. You can make assignments in R using either the “gets arrow” (&lt;-) or =. When you read other people’s code, you’ll see both. R gurus advise using &lt;- rather than = when coding in R, because as you move to doing more complex things, some subtle problems might crop up if you use =. You can tell the age of a programmer by whether he or she uses the “gets arrow” or =, with = more common among the young and hip. For this course, however, I am asking you to code according to Hadley Wickham’s R style guide, which specifies using the “gets arrow” for object assignment. While the “gets arrow” takes two key strokes, you can somewhat get around this limitation by using RStudio’s keyboard shortcut for the “gets arrow”. This shortcut is Alt + - on Windows and Option + - on Macs. To see a full list of RStudio keyboard shortcuts, go to the “Help” tab in RStudio and select “Keyboard Shortcuts”. There are some absolute rules for the names you can use for an object name: Use only letters, numbers, and underscores Don’t start with anything but a letter If you try to assign an object to a name that doesn’t follow the “hard” rules, you’ll get an error. For example, all of these expressions will give you an error: 1message &lt;- &quot;Hello world&quot; _message &lt;- &quot;Hello world&quot; message! &lt;- &quot;Hello world&quot; In addition to these fixed rules, there are also some guidelines for naming objects that you should adopt now, since they will make your life easier as you advance to writing more complex code in R. The following three guidelines for naming objects are from Hadley Wickham’s R style guide: Use lower case for variable names (message, not Message) Use an underscore as a separator (message_one, not messageOne) Avoid using names that are already defined in R (e.g., don’t name an object mean, because a mean() function exists) “Don’t call your matrix ‘matrix’. Would you call your dog ‘dog’? Anyway, it might clash with the function ‘matrix’.” – Barry Rowlingson, R-help (October 2004) Another good practice is to name objects after nouns (e.g., message) and later, when you start writing functions, name those after verbs (e.g., print_message). You’ll want your object names to be short enough that they don’t take forever to type as you’re coding, but not so short that you can’t remember to what they refer. Sometimes, you’ll want to create an object that you won’t want to keep for very long. For example, you might want to create a small object to test some code, but you plan to not need the object again once you’ve done that. You may want to come up with some short, generic object names that you use for these kinds of objects, so that you’ll know that you can delete them without problems when you want to clean up your R session. There are all kinds of traditions for these placeholder variable names in computer science. foo and bar are two popular choices, as are, evidently, xyzzy, spam, ham, and norf. There are different placeholder names in different languages: for example, toto, truc, and azerty (French); and pippo, pluto, paperino (Disney character names in Italian). See the Wikipedia page on metasyntactic variables to find out more. What if you want to “compose” a call from more than one function call? One way to do it is to assign the output from the first function call to a name and then use that name for the next call. For example: message &lt;- paste(&quot;Hello&quot;, &quot;world&quot;) print(x = message) ## [1] &quot;Hello world&quot; If you give two objects the same name, the most recent definition will be used; objects can be overwritten by assigning new content to the same object name. For example: a &lt;- 1:10 b &lt;- LETTERS [1:3] a ## [1] 1 2 3 4 5 6 7 8 9 10 b ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; a &lt;- b a ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; To create an R expression you can “nest” one function call inside another function call. For example: print(x = paste(&quot;Hello&quot;, &quot;world&quot;)) ## [1] &quot;Hello world&quot; Just like with math, the order that the functions are evaluated moves from the inner set of parentheses to the outer one (Figure 2.7). There’s one more way we’ll look at later called “piping”. Figure 2.7: ‘Environment’ pane in RStudio. This shows the names and first few values of all objects that have been assigned to object names in the global environment. 2.4 R scripts This is a good point in learning R for you to start putting your code in R scripts, rather than entering commands at the console. An R script is a plain text file where you can save a series of R commands. You can save the script and open it up later to see or re-do what you did earlier, just like you could with something like a Word document when you’re writing a paper. To open a new R script in RStudio, go to the menu bar and select “File” -&gt; “New File” -&gt; “R Script”. Alternatively, you can use the keyboard shortcut Command-Shift-N. Figure 2.8 gives an example of an R script file opened in RStudio and points out some interesting elements. Figure 2.8: Example of an R script in RStudio. To save a script you’re working on, you can click on the “Save” button, which looks like a floppy disk, at the top of your R script window in RStudio or use the keyboard shortcut Command-S. You should save R scripts using a “.R” file extension. Within the R script, you’ll usually want to type your code so there’s one command per line. If your command runs long, you can write a single call over multiple lines. It’s unusual to put more than one command on a single line of a script file, but you can if you separate the commands with semicolons (;). These rules all correspond to how you can enter commands at the console. Running R code from a script file is very easy in RStudio. You can use either the “Run” button or Command-Return, and any code that is selected (i.e., that you’ve highlighted with your cursor) will run at the console. You can use this functionality to run a single line of code, multiple lines of code, or even just part of a specific line of code. If no code is highlighted, then R will instead run all the code on the line with the cursor and then move the cursor down to the next line in the script. You can also run all of the code in a script. To do this, use the “Source” button at the top of the script window. You can also run the entire script either from the console or from within another script by using the source() function, with the filename of the script you want to run as the argument. For example, to run all of the code in a file named “MyFile.R” that is saved in your current working directory, run: source(&quot;MyFile.R&quot;) While it’s generally best to write your R code in a script and run it from there rather than entering it interactively at the R console, there are some exceptions. A main example is when you’re initially checking out a dataset to make sure you’ve imported it correctly. It often makes more sense to run commands for this task, like str(), head(), tail(), and summary(), at the console. These are all examples of commands where you’re trying to look at something about your data right now, rather than code that builds toward your analysis, or helps you import or wrangle your data. 2.4.1 Commenting code Sometimes, you’ll want to include notes in your code. You can do this in all programming languages by using a comment character to start the line with your comment. In R, the comment character is the hash symbol, #. You can add comments into an R script to let others know (and remind yourself) what you’re doing and why. Any line on a script line that starts with # will not be read by R. You can also take advantage of commenting to comment out certain parts of code that you don’t want to run at the moment. But, make sure to finalize your R scripts with only functional code and useful comments. R will skip any line that starts with # in a script. For example, if you run the following code: # Don't print this. \"But print this\" R will only print the second, uncommented line. You can also use a comment in the middle of a line, to add a note on what you’re doing in that line of the code. R will skip any part of the code from the hash symbol on. For example: &quot;Print this&quot; # But not this, it&#39;s a comment. ## [1] &quot;Print this&quot; There’s usually no reason to use code comments when running commands at the R console; however, it’s very important to get in the practice of including meaningful comments in R scripts. This helps you remember what you did when you revisit your code later. “You know you’re brilliant, but maybe you’d like to understand what you did 2 weeks from now.” – Linus Torvalds 2.5 The “package” system 2.5.1 R packages “Any doubts about R’s big-league status should be put to rest, now that we have a Sudoku Puzzle Solver. Take that, SAS!” – David Brahm (announcing the sudoku package), R-packages (January 2006) Your original download of R is only a starting point. You can expand functionality of R with what are called packages, or extensions with new code and functionality that add to the basic “base R” environment. To me, this is a bit like this toy train set. You first buy a very basic set that looks something like Figure 2.9. Figure 2.9: The toy version of base R. To take full advantage of R, you’ll want to add on packages. In the case of the train set, at this point, a doting grandparent adds on extensively through birthday presents, so you end up with something that looks like Figure 2.10. Figure 2.10: The toy version of what your R set-up will look like once you find cool packages to use for your research. Each package is basically a bundle of extra R functions. They may also include help documentation, datasets, and some other objects, but typically the heart of an R package is the new functions it provides. You can get these “add-on” packages in a number of ways. The main source for installing packages for R remains the Comprehensive R Archive Network, or CRAN. However, GitHub is growing in popularity, especially for packages that are still in active development. You can also create and share packages among your collaborators or co-workers, without ever posting them publicly. 2.5.2 Installing from CRAN Figure 2.11: Celebrating CRAN’s 10,000th package, which was developed by Dr. Brooke Anderson. The most popular place from which to download packages is currently CRAN, which has over 10,000 R packages available (Figure 2.11). You can install packages from CRAN using R code, with the install.packages() function. For example, telephone keypads include letters for each number (Figure 2.12), which allow companies to have “named” phone numbers that are easier for people to remember, like 1-800-GO-FEDEX and 1-800-FLOWERS. Figure 2.12: Telephone keypad with letters corresponding to each number. The phonenumber package is a cool little package that will convert between numbers and letters based on the telephone keypad. Since this package is on CRAN, you can install the package to your computer using the install.packages() function: install.packages(pkgs = &quot;phonenumber&quot;) This downloads the package from CRAN and saves it in a special location on your computer where R can load it when you’re ready to use it. Once you’ve installed a package to your computer this way, you don’t need to re-run this install.packages() for the package ever again, unless the package maintainer posts an updated version. Just like R itself, packages often evolve and are updated by their maintainers. You should update your packages as new versions come out. Typically, you have to reinstall packages when you update your version of R, so this is a good chance to get the most up-to-date version of the packages you use. 2.5.3 Loading an installed package Once you have installed a package, it will be saved to your computer. But, you won’t be able to access its functions within an R session until you load it in that R session. Loading a package essentially makes all of the package’s functions available to you. You can load a package in an R session using the library() function, with the package name inside the parentheses. library(package = &quot;phonenumber&quot;) Figure 2.13 provides a conceptual picture of the different steps of installing and loading a package. Figure 2.13: Install a package (with install.packages()) to get it onto your computer. Load it (with library()) to get it into your R session. Once a package is loaded, you can use all its exported (i.e., public) functions by calling them directly. For example, the phonenumber package has a function called letterToNumber() that converts a character string to a number. If you have not loaded the phonenumber package in your current R session and try to use this function, you will get an error. Once you’ve loaded phonenumber using the library() function, you can use this function in your R session: fedex_number &lt;- &quot;GoFedEx&quot; letterToNumber(value = fedex_number) ## [1] &quot;4633339&quot; R vectors can have several different classes. One common class is the character class, which is the class of the character string we’re using here (“GoFedEx”). You’ll always put character strings in quotation marks. Another key class is numeric (numbers). Later in the course, we’ll introduce other classes that vectors can have, including factors and dates. For the simplest vector classes, these classes are determined by the type of data that the vector stores. When you open RStudio, unless you reload the history of a previous R session (which I strongly do not recommend), you will start your work in a “fresh” R session. This means that, once you open RStudio, you will need to run the code to load any packages, define any objects, and read in any data that you will need for analysis in that session. If you are using a package in academic research, you should cite it, especially if it implements a nonstandard algorithm or method. You can use the citation() function to get the information you need about how to cite a package: citation(package = &quot;phonenumber&quot;) ## ## To cite package &#39;phonenumber&#39; in publications use: ## ## Steve Myles (2015). phonenumber: Convert Letters to Numbers and Back ## as on a Telephone Keypad. R package version 0.2.2. ## https://CRAN.R-project.org/package=phonenumber ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {phonenumber: Convert Letters to Numbers and Back as on a Telephone Keypad}, ## author = {Steve Myles}, ## year = {2015}, ## note = {R package version 0.2.2}, ## url = {https://CRAN.R-project.org/package=phonenumber}, ## } We’ve talked here about loading packages using the library() function to access their functions. This is not the only way to access the package’s functions. The syntax [package name]::[function name] will allow you to use a function from a package you have installed on your computer, even if its package has not been loaded in the current R session. Typically, this syntax is not used much in data analysis scripts, in part because it makes the code much longer. You will occasionally see it in learning contexts to build familiarity with the package::function connection and in which package a function exists. It is also used to distinguish between two functions from different packages that have the same name, as this format makes the desired function unambiguous. One example where this syntax often is needed is when both plyr and dplyr packages are loaded in an R session, since these share functions with the same name. Packages typically include some documentation to help users. These include: Package vignettes: Longer, tutorial-style documents that walk the user through the basics of how to use the package and often give some helpful example cases of the package in use. Function helpfiles: Files for each user-facing function within the package, following an established structure. These include information about what inputs are required and optional for the function, what output will be created, and what options can be selected by the user. In many cases, these also include examples of using the function. To determine which vignettes are available for a package, you can use the vignette() function, with the package’s name specified for the package option: vignette(package = &quot;phonenumber&quot;) From the output of this, you can call any of the package’s vignettes directly. For example, the previous call tells you that this package only has one vignette, and that vignette has the same name as the package (“phonenumber”). Once you know the name of the vignette you would like to open, you can also use vignette() to open it: vignette(topic = &quot;phonenumber&quot;) To access the helpfile for any function within a package you’ve loaded, you can use ? followed by the function’s name, but note the lack of (): ?letterToNumber 2.6 R’s most basic object types An R object stores some type of data that you want to use later in your R code, without fully recreating it. The content of R objects can vary from very simple (e.g., \"GoFedEx\" string in the example code above) to very complex objects with lots of elements (e.g., machine learning model). Objects can be structured in different ways, in terms of how they “hold” data. These difference structures are called object classes. One class of objects can be a subtype of a more general object class. There are a variety of different object types in R, shaped to fit different types of objects, from the simple to complex. In this section, we’ll start by describing two object types that you will use most often in basic data analysis: vectors (one-dimensional objects) and dataframes (two-dimensional objects). For these two object classes (vectors and dataframes), we’ll look at: How that class is structured How to make a new object with that class How to extract values from objects with that class 2.6.1 Vectors To get an initial grasp of the vector object type in R, think of it as a one-dimensional object, or a string of values. Figure 2.14 provides an example of the structure for a very simple vector, one that holds the names of the three main characters in the Harry Potter book series. Figure 2.14: An example of the structure of an R object with the vector class. This object class contains data as a string of values, all with the same data type. All values in a vector must be of the same data type (i.e., all numbers, all characters, or all dates). If you try to create a vector with elements from different types (e.g., vector of “FedEx”, which is a character, and 3, a number), R will coerce all of the elements to the most generic class of the included elements (e.g., “FedEx” and “3” will both become characters, since “3” can be changed to a character, but “FedEx” can’t be changed to a number). Figure 2.15 gives some examples of different classes of vectors. Figure 2.15: Examples of vectors of different classes. All the values in a vector must be of the same type (e.g., all numbers or all characters). There are different classes of vectors depending on the type of data they store. To create a vector from different elements, you’ll use the concatenate function, c() to join them together, with commas between the elements; concatenate is a fancy word that means “to link together”. For example, to create the vector shown in Figure 2.14, you can run: c(&quot;Harry&quot;, &quot;Ron&quot;, &quot;Hermione&quot;) ## [1] &quot;Harry&quot; &quot;Ron&quot; &quot;Hermione&quot; If you want to use that object later, you can assign it an object name in the expression: main_characters &lt;- c(&quot;Harry&quot;, &quot;Ron&quot;, &quot;Hermione&quot;) print(x = main_characters) ## [1] &quot;Harry&quot; &quot;Ron&quot; &quot;Hermione&quot; This assignment expression, for assigning a vector an object name, follows the structure we covered earlier for function calls and assignment expressions (Figure 2.16). Figure 2.16: Elements of the assignment expression for creating a vector and assigning it an object name. If you create a numeric vector, you should not put the values in quotation marks: n_kids &lt;- c(1, 7, 1) If you mix classes when you create the vector, R will coerce all the elements to most generic class of the included elements: mixed_classes &lt;- c(1, 3, &quot;five&quot;) mixed_classes ## [1] &quot;1&quot; &quot;3&quot; &quot;five&quot; Notice that the two integers, 1 and 3, are now in quotation marks because they were put in a vector with a value with the character data type. You can use the class() function to determine the class of an object: class(x = mixed_classes) ## [1] &quot;character&quot; A vector’s length is the number of elements in the vector. You can use the length() function to determine a vector’s length: length(x = mixed_classes) ## [1] 3 Once you create an object, you will often want to reference the whole object in future code. Nonetheless, there will be some times when you’ll want to reference only certain elements of the object. You can pull out certain values from a vector by using indexing with square brackets ([...]) to identify the locations of the element you want to extract. For example, to extract the second element of the main_characters vector, you can run: main_characters[2] # Get the second value ## [1] &quot;Ron&quot; You can use this same method to extract more than one value. You just need to create a numeric vector with the position of each element you want to extract and pass that in the square brackets. For example, to extract the first and third elements of the main_characters vector, you can run: main_characters[c(1, 3)] # Get first and third values ## [1] &quot;Harry&quot; &quot;Hermione&quot; The : operator can be very helpful with extracting values from a vector. This operator creates a sequence of values from the value before the : to the value after :, going by units of 1. For example, if you want to create a list of the numbers between 1 and 10, you can run: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 If you want to extract the first two values from the main_characters vector, you can use the : operator: main_characters[1:2] # Get the first two values ## [1] &quot;Harry&quot; &quot;Ron&quot; You can also use logic to pull out some values of a vector. For example, you might only want to pull out even values from the fibonacci vector. One thing that people often find confusing when they start using R is knowing when to use and not use quotation marks. The general rule is that you use quotation marks when you want to refer to a character string literally, but no quotation marks when you want to refer to the value in a previously-defined object. For example, if you saved the string “Volckens” as the object my_name (my_name &lt;- “Volckens”), then in later code, if you type my_name (no quotation marks), you’ll get “Volckens”, while if you type out “my_name” (with quotation marks), you’ll get “my_name” (what you typed). One thing that makes this rule confusing is that there are a few cases in R where you really should, following this rule, use quotation marks, but the function is coded to let you be lazy and get away without them. One example is the library() function. In the code earlier in this section to load the “phonenumber” package, you want to load the package “phonenumber” (with quotation marks), rather than load whatever character string is saved in the object named phonenumber. But, library() is one of the functions where you can be lazy and skip the quotation marks, and it will still load the package. Therefore, this function works if you do or do not use quotation marks around the package name. 2.6.2 Dataframes A dataframe is a two-dimensional object made of one or more vectors of the same length stuck together side-by-side. It is the closest R has to an Excel spreadsheet-type structure. Figure 2.17 gives a conceptual example of a dataframe created from several of the vector examples in Figure 2.15. Figure 2.17: An example dataframe created from several vectors of the same length and with observations aligned across vector positions. For example, the first value in each vector provides a value for Harry, the second for Ron. Here’s how the dataframe in Figure 2.17 will look in R: ## # A tibble: 3 x 4 ## first_name last_name n_kids survived ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Harry Potter 1 TRUE ## 2 Ron Weasley 7 TRUE ## 3 Hermione Granger 1 TRUE This dataframe is arranged in rows and columns, with names for each column (Figure 2.18). Note that each row of this dataframe gives a different observation. In this case, our unit of observation is a Harry Potter character. Each column gives a different type of information, including first name, last name, birth year, and whether they’re still alive for each of the observations (i.e., book characters). Notice that the number of elements in each of the columns must be the same in this dataframe, but that the different columns can have different classes of data (e.g., character vectors for first_name and last_name; logical value of TRUE or FALSE for alive). Figure 2.18: The elements of a dataframe: columns, rows, and column names. We will be working with a specific class of dataframe called a tibble. You can create tibble dataframes using the tibble() function from the tibble package. Most often you will create a dataframe by reading in data from a file, using something like read_csv() from the readr package. There are base R functions for both of these tasks (i.e., data.frame() and read.csv(), respectively), eliminating the need to load additional packages with a library() call. The series of packages that make up what’s called the “tidyverse” have brought a huge improvement in the ease and speed of working with data in R. We will be teaching these tools in this course, and that’s why we’re going directly to tibble() and read_csv() from the start, rather than base R equivalents. Later in the course, we’ll talk more about this “tidyverse” and what makes it so great. To create a dataframe, you can use the tibble() function from the tibble package. The general format for using tibble() is: ## generic code; will not run [name of object] &lt;- tibble([1st column name] = [1st column content], [2nd column name] = [2nd column content]) with an equals sign between the column name and column content for each column, and commas between each of the columns. Here is an example of the code used to create the Harry Potter tibble dataframe shown above: library(package = &quot;tibble&quot;) hp_data &lt;- tibble(first_name = c(&quot;Harry&quot;, &quot;Ron&quot;, &quot;Hermione&quot;), last_name = c(&quot;Potter&quot;, &quot;Weasley&quot;, &quot;Granger&quot;), n_kids = c(1, 7, 1), survived = c(TRUE, TRUE, TRUE)) hp_data ## # A tibble: 3 x 4 ## first_name last_name n_kids survived ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Harry Potter 1 TRUE ## 2 Ron Weasley 7 TRUE ## 3 Hermione Granger 1 TRUE You can also create a dataframe by sticking together vectors you already have saved as R objects. For example: hp_data &lt;- tibble(first_name = main_characters, last_name = c(&quot;Potter&quot;, &quot;Weasley&quot;, &quot;Granger&quot;), n_kids = n_kids, survived = c(TRUE, TRUE, TRUE)) hp_data ## # A tibble: 3 x 4 ## first_name last_name n_kids survived ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Harry Potter 1 TRUE ## 2 Ron Weasley 7 TRUE ## 3 Hermione Granger 1 TRUE Note that this call requires the main_characters and n_kids vectors to be the same length. They don’t have to be, and, in this case, are not, the same class of objects. Specifically, main_characters is a character class, and n_kids is numeric. You can put more than one function call in a single line of R code, as in this example. The c() creates a vector, while the tibble() creates a dataframe, using the vectors created by the calls to c(). When you use multiple functions within a single R call, R will evaluate starting from the inner-most parentheses outward, much like the order of mathematical operations. So far, we’ve only seen how to create dataframes from scratch within an R session. Usually, however, you’ll create R dataframes by reading in data from an outside file using the read_csv() from the readr package or other related functions. For example, you might want to analyze data on all the guests that came on the Daily Show, circa Jon Stewart. If you have this data in a comma-separated (csv) file on your computer called “daily_show_guests.csv”, you can read it into your R session with the following code: library(package = &quot;readr&quot;) daily_show &lt;- read_csv(file = &quot;daily_show_guests.csv&quot;, skip = 4) In this code, the read_csv() function is reading in the data from the file “daily_show_guests.csv”, while the “gets arrow” (&lt;-) assigns that data to the object daily_show, which you can then reference in later code to explore and plot the data. You can use the functions dim(), nrow(), and ncol() to figure out the dimensions (i.e., number of rows and columns) of a dataframe: dim(x = daily_show) ## [1] 2693 5 nrow(x = daily_show) ## [1] 2693 ncol(x = daily_show) ## [1] 5 Base R also has some useful functions for quickly exploring dataframes: str(): Show the structure of an R object, including a dataframe summary(): Give summaries of each column of a dataframe. For example, you can explore the data we just pulled in on the Daily Show with: str(object = daily_show) ## tibble [2,693 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ YEAR : num [1:2693] 1999 1999 1999 1999 1999 ... ## $ GoogleKnowlege_Occupation: chr [1:2693] &quot;actor&quot; &quot;Comedian&quot; &quot;television actress&quot; &quot;film actress&quot; ... ## $ Show : chr [1:2693] &quot;1/11/99&quot; &quot;1/12/99&quot; &quot;1/13/99&quot; &quot;1/14/99&quot; ... ## $ Group : chr [1:2693] &quot;Acting&quot; &quot;Comedy&quot; &quot;Acting&quot; &quot;Acting&quot; ... ## $ Raw_Guest_List : chr [1:2693] &quot;Michael J. Fox&quot; &quot;Sandra Bernhard&quot; &quot;Tracey Ullman&quot; &quot;Gillian Anderson&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. YEAR = col_double(), ## .. GoogleKnowlege_Occupation = col_character(), ## .. Show = col_character(), ## .. Group = col_character(), ## .. Raw_Guest_List = col_character() ## .. ) summary(object = daily_show) ## YEAR GoogleKnowlege_Occupation Show Group ## Min. :1999 Length:2693 Length:2693 Length:2693 ## 1st Qu.:2003 Class :character Class :character Class :character ## Median :2007 Mode :character Mode :character Mode :character ## Mean :2007 ## 3rd Qu.:2011 ## Max. :2015 ## Raw_Guest_List ## Length:2693 ## Class :character ## Mode :character ## ## ## To extract data from a dataframe, you can use some functions from the dplyr package, including select() and slice(). The select() function will pull out columns, while the slice() function will extract rows. In this chapter, we’ll talk about how to extract certain rows or columns of a dataframe by their position (i.e., based on row or column number). For example, if you wanted to get the first two rows of the hp_data dataframe, you could run: library(package = &quot;dplyr&quot;) slice(.data = hp_data, c(1:2)) ## # A tibble: 2 x 4 ## first_name last_name n_kids survived ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Harry Potter 1 TRUE ## 2 Ron Weasley 7 TRUE If you wanted to get the first and fourth columns, you could run: select(.data = hp_data, c(1, 4)) ## # A tibble: 3 x 2 ## first_name survived ## &lt;chr&gt; &lt;lgl&gt; ## 1 Harry TRUE ## 2 Ron TRUE ## 3 Hermione TRUE You can compose calls from both functions. For example, you could extract the values in the first and fourth columns of the first two rows with: select(.data = slice(.data = hp_data, c(1:2)), c(1, 4)) ## # A tibble: 2 x 2 ## first_name survived ## &lt;chr&gt; &lt;lgl&gt; ## 1 Harry TRUE ## 2 Ron TRUE You can use square-bracket indexing ([..., ...]) for dataframes, too, but you will need to manage two dimensions: rows and columns. Put the rows you want before the comma and the columns after; if you want all rows or all columns, leave the corresponding spot blank. Here are two examples of using square-bracket indexing to pull a subset of the hp_data dataframe: hp_data[1:2, 2] # First two rows, second column ## # A tibble: 2 x 1 ## last_name ## &lt;chr&gt; ## 1 Potter ## 2 Weasley hp_data[3, ] # Last row, all columns ## # A tibble: 1 x 4 ## first_name last_name n_kids survived ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Hermione Granger 1 TRUE If you forget to put the comma in the indexing for a dataframe (e.g., fibonacci_seq[1:2]), you will index out the columns that fall at that position or positions. To avoid confusion, I suggest that you always use indexing with a comma when working with dataframes. 2.7 Chapter 2 Exercises 2.7.1 Set 1: Session, helpfiles, scripts, and objects Within your R project for this course, open a “fresh” R session (Session &gt; Restart R, if RStudio is already open). Using getwd() in the console, confirm the working directory is your R project. Type sessionInfo() into the console. What R version are you using? What base R packages were loaded automatically in your R session? Still in the console and using ?, open and examine the helpfile for one of the base R packages named above. Use the function listed in the “Details” section of the helpfile to call the full package documentation, including a list of functions. Call and examine the helpfile for one of these functions. Go to File &gt; New File &gt; R Script to open a new R script. Add your name to the top of the R script as a comment. Call mtcars (dataset about cars in base R) in the console. Then, using the gets arrow, save mtcars as an object named mtcars_data in your R script. Saving the relevant commands in the R script, examine the structure of mtcars_data and determine its dimensions and variable class types (e.g., numeric, logical). Save your R script with an informative file name (e.g., “class-activity-DATE”) in the /code folder of your R project and commit your changes to GitHub with a meaningful commit message and push the changes. 2.7.1.1 Example Code Working directory # confirm working directory is in high-level folder of R project base::getwd() Session information # identify R version and base R packages utils::sessionInfo() Helpfiles # call main helpfile for whole base R package ?stats # use function from Details section to call list of functions base::library(help = &quot;stats&quot;) # call helpfile for one function ?lm Object assignment # load and view mtcars data mtcars # assign data as object in environment mtcars_data &lt;- mtcars # view structure of mtcars_data str(mtcars_data) # a tidyverse alternative to `str()` ## extra: https://stackoverflow.com/questions/23660094/whats-the-difference-between-integer-class-and-numeric-class-in-r tibble::glimpse(mtcars_data) # assess dimensions of mtcars_data dim(mtcars_data) # determine variable class types (by individual variable) class(mtcars_data$mpg) There are 32 observations (vehicle models) and 11 variables, all of which are numeric. 2.7.2 Set 2: Loading and using packages Reopen the R script from the above set of exercises. Install dplyr, a popular R package for data wrangling, from your console. In your R script, load dplyr. Using dplyr::filter(), determine the number of cars in mtcars_data with an average miles per gallon (mpg) above 25. Save the R script and commit the changes to GitHub with a meaningful commit message; push the changes. Navigate to the “Tutorial” tab in your environment panel. Complete the “Data Basics” tutorial via learnr. 2.7.2.1 Example Code Data wrangling # install.packages(&quot;dplyr&quot;) if needed in the console # load `dplyr`, saving the command in your R script library(dplyr) # filter to vehicles with mpg above 25 dplyr::filter(mtcars_data, mpg &gt; 25.0) There are six vehicles in mtcars_data with MPG above 25. 2.8 Chapter 2 Homework During the next few class periods and for homework, you will complete ten lessons in swirl, an R package for learning R in R, written by Roger Peng, Brooke Anderson, and Sean Kross. Each lesson might take 10-15 minutes. In a text file, record the lesson names and a very brief description of what you learned from each. Save this file in your local R Project in the appropriate directory (e.g., /homework) and commit/push the file with regular updates to your private GitHub repository. Follow the steps here to install, load, and start swirl. When you are prompted to install a course, you can load “R Programming,” which covers material related to the recent class lectures. If you are already familiar with this content, feel free to select a different course such as “Exploratory Data Analysis.” Please complete the following swirl lessons: Module 1: Basic Building Blocks Module 2: Workspace and Files Module 3: Sequences of Numbers Module 4: Vectors Module 5: Missing Values Module 6: Subsetting Vectors Module 7: Matrices and Data Frames Module 8: Logic Module 9: Functions Module 12: Looking at Data swirl lessons have a mix of base R and tidyverse approaches, so don’t be alarmed or discouraged if you see some unfamiliar techniques or concepts. If you are interested in learning more about something from swirl, Google is a great place to start. We will cover a lot of the material later in the class. 2.8.1 Special swirl commands In the swirl environment, knowing about the following commands will be helpful: The prompt ... indicates you should press enter to continue in the lesson. skip(): skip current question play(): temporarily exit swirl nxt(): return to swirl after play()ing around in the console main(): return to swirl’s main menu bye() or “escape” key: exit swirl "],
["rprog2.html", "Chapter 3 Getting and Cleaning Data 3.1 Ch. 3 Objectives 3.2 Overview 3.3 Reading data into R 3.4 Directories and pathnames 3.5 Data cleaning 3.6 Piping 3.7 Markdowns 3.8 Chapter 3 Exercises 3.9 Chapter 3 Homework", " Chapter 3 Getting and Cleaning Data 3.1 Ch. 3 Objectives This chapter is designed around the following learning objectives. Upon completing this chapter, you should be able to: Recognize what a flat file is and how it differs from data stored in a binary file format Distinguish between delimited and fixed width formats for flat files Identify the delimiter in a delimited file Describe a working directory Demonstrate how to read in different types of flat files Demonstrate how to read in a few types of binary files (e.g., Matlab, Excel) Recognize the difference between relative and absolute file pathnames Describe the basics of your computer’s directory structure Reference files in different locations in your directory structure using relative and absolute pathnames Apply the basic dplyr functions (e.g., rename(), select(), mutate(), slice(), filter(), and arrange()) to work with data in a dataframe object Define a logical operator and know the R syntax for common logical operators Apply logical operators in conjunction with dplyr’s filter() function to create subsets of a dataframe based on logical conditions Apply a sequence of dplyr functions to a dataframe using piping (%&gt;%) Create R Markdown documents and describe their basic content and function 3.2 Overview There are four basic steps you will often repeat as you prepare to analyze data in R: Identify the location of the data. If it’s on your computer, which directory? If it’s online, what’s the link? Read data into R (e.g., using a function like read_delim() or read_csv() from the readr package) using the file path you figured out in step 1 Check to make sure the data came in correctly using functions like dim(), head(), tail(), str(), and/or glimpse(). Clean the data up by removing missing (or nonsense) values, renaming or reclassifying variables, performing units conversions, or other actions that support a streamlined data analysis. In this chapter, I’ll go over the basics for each of these steps and dive a bit deeper into some related topics you should learn now to make your life easier as you get started using R for data analysis. 3.3 Reading data into R Data comes in files of all shapes and sizes. R has the capability to import data from many files types and locations, even proprietary files for other software. Here are some of the types of data files that R can read and work with: Flat files (more about these soon) Files from other software packages such as MATLAB or Excel Tables on webpages (e.g., the table on Ebola outbreaks near the end of this Wikipedia page) Data in a database (e.g., MySQL, Oracle) Data in JSON and XML formats Really crazy data formats used in other disciplines (e.g., netCDF files from climate research, MRI data stored in Analyze, NIfTI, and DICOM formats) Geographic shapefiles Data through Application Programming Interfaces (APIs; most websites use APIs to ask you for input and then use that input to direct new information back to you) Often, it is possible to import and wrangle extremely messy data by using functions like scan() and readLines() to read the data in a line at a time, and then using regular expressions to clean up the data as it gets imported. For this course, however, we will begin with less challenging file formats (and degrees of messiness). 3.3.1 Reading local flat files Much of the data that you will want to read in will be in flat files that are stored locally (i.e., on your computer’s hard drive). A flat file is basically a file that you can open using a text editor. The most common type you’ll work with are probably comma-separated files, often with a .csv or .txt file extension. Most flat files come in two general categories: Fixed width files, and Delimited files, which include: “.csv”: Comma-separated values “.tab”, “.tsv”: Tab-separated values Other possible delimiters: colon, semicolon, pipe (“|”) Fixed-width files are files where a column always has the same width, for all the rows in the column. These tend to look very neat and easy-to-read when you open them in a text editor. For example, the first few rows of a fixed-width file might look like this: Course Number Day Time Thermodynamics 337 M/W/F 9:00-9:50 Aerosol Physics and Technology 577 M/W/F 10:00-10:50 Fixed-width files used to be very popular, and they make it easier to look at data when you open the file in a text editor. Now, it’s rare to just use a text editor to open a file and check out the data. Also, these files can be a bit of a pain to read into R and other programs because you sometimes have to specify the length of each column. You may come across a fixed-width file every now and then, particularly when working with older data, so it’s useful to be able to recognize one and to know how to import it. Delimited files use some delimiter such as a comma or tab to separate each column value within a row. The first few rows of a delimited file might look like this: Course, Number, Day, Time &quot;Thermodynamics&quot;, 337, &quot;M/W/F&quot;, &quot;9:00-9:50&quot; &quot;Aerosol Physics and Technology&quot;, 577, &quot;M/W/F&quot;, &quot;10:00-10:50&quot; Delimited files are very easy to read into R. You just need to be able to figure out what character is used as a delimiter and specify that to R in the function call to read in the data. These flat files can have a number of different file extensions. The most generic is .txt, but they will also have ones more specific to their format, like .csv for a comma-delimited file (.csv stands for “comma-separated values”), or .fwf for a fixed-width file. R can read in data from both fixed-width and delimited flat files. The only catch is that you need to tell R a bit more about the format of the flat file, including whether it is fixed-width or delimited. If the file is fixed-width, you will usually have to provide R with information about each column (see read_fwf() for details). If the file is delimited, you’ll need to tell R which delimiter, such as comma or tab, is being used. The read_delim() family of functions are used to read delimited flat files into R - these functions come from the readr package, which we will use extensively in ths course. All members of the read_delim() family do the same basic thing: import flat files into a tibble. The major difference is what defaults each function has for the delimiter (delim). Members of the read_delim() family include: Function Delimiter read_csv() comma read_csv2() semi-colon read_table2() whitespace read_tsv() tab You can use read_delim() to read in any delimited file, regardless of the delimiter; however, you will need to specify the type of delimiter using the delim argument. If you remember the more specialized function call (e.g., read_csv() for a comma-delimited file), you can save yourself some typing. For example, to read in the Ebola data data file, which is comma-delimited, you could either use read_table() with a delim argument specified or use read_csv(), in which case you don’t have to specify delim: library(package = &quot;readr&quot;) # The following two calls do the same thing ebola &lt;- readr::read_delim(file = &quot;data/country_timeseries.csv&quot;, delim = &quot;,&quot;) ebola &lt;- readr::read_csv(file = &quot;data/country_timeseries.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_character(), ## Day = col_double(), ## Cases_Guinea = col_double(), ## Cases_Liberia = col_double(), ## Cases_SierraLeone = col_double(), ## Cases_Nigeria = col_double(), ## Cases_Senegal = col_double(), ## Cases_UnitedStates = col_double(), ## Cases_Spain = col_double(), ## Cases_Mali = col_double(), ## Deaths_Guinea = col_double(), ## Deaths_Liberia = col_double(), ## Deaths_SierraLeone = col_double(), ## Deaths_Nigeria = col_double(), ## Deaths_Senegal = col_double(), ## Deaths_UnitedStates = col_double(), ## Deaths_Spain = col_double(), ## Deaths_Mali = col_double() ## ) The message that R prints after this call (“Parsed with column specification: …”) lets you know what classes were used for each column. This function tries to guess the appropriate class and typically gets it right. You can suppress the message using the cols_types = cols() argument, or by adjusting the code chunk options in an R Markdown. If readr doesn’t correctly assign some of the columns classes, you can use the type_convert() function for R to guess again after you’ve tweaked the formats of the rogue columns. This family of functions has a few other helpful options you can specify. For example, if you want to skip the first few lines of a file before you start reading in the data, you can use skip() to set the number of lines to skip. If you only want to read in a few lines of the data, you can use the n_max() option. For example, if you have a really large file, and you want to save time by only reading in the first ten lines, as you figure out what other optional arguments to use in read_delim() for that file, you could include the option n_max = 10. Here is a table of some of the most useful options common to the read_delim() family of functions: Option Description skip() How many lines of the start of the file should you skip? col_names() Use the column names provided or define your own names? col_types() What are the column types (e.g., chr, num, int, logi etc.])? n_max() How many rows do you want to read in? na() How are missing values coded? Remember that you can always find out more about a function by looking at its help file. For example, check out ?read_delim and ?read_fwf (note the lack of parentheses). You can also use the help files to determine the default values of arguments for each function. So far, we’ve only looked at functions from the readr package for reading in data files. There is a similar family of functions available in base R, the read.table() family of functions. The readr family of functions is very similar to the base R read.table() functions, but have some more sensible defaults. Compared to the read.table() function family, the readr functions are: Faster; show progress bar of data import Work better with large datasets Have more sensible defaults (e.g., characters default to characters, not factors) I recommend that you always use the readr functions rather than their base R alternatives, given these advantages; however, you are likely to come across code with these base R functions, so it is helpful to be aware of them. Functions in the read.table family include: read.csv() read.delim() read.table() read.fwf() Note: these base R functions use periods (read.) whereas the readr functions use underscores (read_). The readr package is a member of the tidyverse suite of R packages. The tidyverse describes an evolving collection of R packages with a common philosophy and approach, and they are unquestionably changing the way people code in R. Many of these R packages were developed in part or full by Hadley Wickham and others at RStudio. Many of these packages are less than ten years old but have been rapidly adapted by the R community. As a result, newer examples of R code will often look very different from the code in older R scripts, including examples in books that are more than a few years old. In this course, I’ll focus on tidyverse functions when possible, but I do put in details about base R equivalent functions or processes at some points. This will help you interpret older code. You can download all the tidyverse packages at the same time with install.packages(“tidyverse”) and make all the tidyverse functions available for use withlibrary(“tidyverse”). 3.3.2 Reading in other file types Later in the course, we’ll talk about how to open a variety of other file types in R. You might find it immediately useful to be able to read in files from other statistical programs. There are two “tidyverse” packages, readxl and haven, that help with this. They allow you to read in files from the following formats: File type Function Package Excel read_excel() readxl SAS read_sas() haven SPSS read_spss() haven Stata read_stata() haven 3.4 Directories and pathnames 3.4.1 Directory structure So far, we’ve only looked at reading in files that are located in your current working directory. For example, if you’re working in an R Project, by default the project will open with that directory as the working directory, so you can read files that are saved in that project’s main directory using only the file name as a reference. You will often want to read in files that are located somewhere else on your computer, or even files that are saved on another computer or posted online. Doing this is very similar to reading in a file that is in your current working directory; the only difference is that you need to give R some directions so it can find the file. The most common case will be reading in files in a subdirectory of your current working directory. For example, you may have created a “data” subdirectory in one of your R Projects directories to keep all the project’s data files in the same place while keeping the structure of the main directory fairly clean. In this case, you’ll need to direct R into that subdirectory when you want to read one of those files. To understand how to give R these directions, you need to have some understanding of the directory structure of your computer. It seems a bit of a pain and a bit complex to have to think about computer directory structure in the “basics” part of this class, but this structure is not terribly complex once you get the idea of it. There are a couple of very good reasons why it’s worth learning now. First, many of the most frustrating errors you get when you start using R trace back to understanding directories and filepaths. For example, when you try to read a file into R using only the filename, and that file is not in your current working directory, you will get an error like: Error in file(file, &quot;rt&quot;) : cannot open the connection In addition: Warning message: In file(file, &quot;rt&quot;) : cannot open file &#39;Ex.csv&#39;: No such file or directory This error is especially frustrating when you’re new to R because it happens at the very beginning of your analysis—you can’t even import the data. Also, if you don’t understand a bit about working directories and how R looks for the file you’re asking it to find, you’d have no idea where to start to fix this error. Second, once you understand how to use pathnames, especially relative pathnames, to tell R how to find a file that is in a directory other than your working directory, you will be able to organize all of your files for a project in a much cleaner way. For example, you can create a directory for your project, then create one subdirectory to store all of your R scripts, and another to store all of your data, and so on. This can help you keep very complex projects more structured and easier to navigate. Your computer organizes files through a collection of directories. Chances are, you are fairly used to working with these in your daily life already, although you may call them “folders” rather than “directories”. For example, you’ve probably created new directories to store data files and Word documents for a specific project. Figure 3.1 gives an example file directory structure for a hypothetical computer. Directories are shown in blue, and files in green. Figure 3.1: An example of file directory structure. Notice a few interesting things from Figure 3.1. First, you might notice the structure includes a few of the directories that you use a lot on your own computer, like Desktop, Documents, and Downloads. Next, the directory at the very top is the computer’s root directory, /. For a PC, the root directory might something like C:. For Unix and Macs, it’s usually /. Finally, if you look closely, you’ll notice that it’s possible to have different files in different locations of the directory structure with the same file name. For example, in the figure, there are files names heat_mort.csv in both the CourseText directory and in the example_data directory. These are two different files with different contents, but they can have the same name as long as they’re in different directories. This fact—that you can have files with the same name in different places—should help you appreciate how useful it is that R requires you to give very clear directions to describe exactly which file you want R to read in, if you aren’t importing something in your current working directory. You will have a home directory somewhere near the top of your structure, although it’s likely not your root directory. In the hypothetical computer in Figure 3.1, the home directory is /Users/brookeanderson. I’ll describe just a bit later how you can figure out what your own home directory is on your own computer. 3.4.2 Working directory When you run R, it’s always running from within some working directory, which will be one of the directories somewhere in your computer’s directory structure. At any time, you can figure out which directory R is working in by running the command getwd() (short for “get working directory”). For example, my R session is currently running in the following directory: getwd() ## [1] &quot;/Users/wendtke/Documents/R/edar_coursebook&quot; This means that, for my current R session, R is working in the edar_coursebook subdirectory of my johnvolckens directory (home directory). There are a few general rules for which working directory R selects when you open an R session. These are not absolute rules, but they’re generally true. If you have R closed, and you open it by double-clicking on an R script, then R will generally open with, as its working directory, the directory in which that script is stored. This is often a very convenient convention, because often any of the data you’ll import for that script is somewhere near where the script file is saved in the directory structure. If you open R by double-clicking on the R icon in “Applications” (or the start menu on a PC), R will start in its default working directory. You can find out what this is, or change it, in RStudio’s “Preferences”. Finally, if you open an R Project, R will start in that project’s working directory—where the .Rproj file for the project is stored. This is one of the reasons why we always create a new R Project when starting a data analysis - RStudio projects remember where to look! 3.4.3 File and directory pathnames Once you get a picture of how your directories and files are organized, you can use pathnames, either absolute or relative, to read in files from different directories outside your current working directory. Pathnames are the directions for getting to a directory or file stored on your computer. When you want to reference a directory or file, you can use one of two types of pathnames: Relative pathname: How to get to the file or directory from your current working directory Absolute pathname: How to get to the file or directory from anywhere on the computer Absolute pathnames are a bit more straightforward conceptually because they don’t depend on your current working directory; however, they’re also a lot longer to write and very inconvenient if you’ll be sharing some of your code with other people who might try to run it on their own computers. I’ll explain this second point a bit more later in this section. I strongly advise against the use of absolute pathnames because of the aforementioned collaborative issue, but I will include some details here nonetheless. Absolute pathnames give the full directions to a directory or file, starting all the way at the root directory. For example, the daily_show_guests.csv file in the data directory has the absolute pathname: &quot;/Users/johnvolckens/Documents/Teaching/DataSci/edar_coursebook/data/daily_show_guests.csv&quot; You can use this absolute pathname to read this file in using any of the readr functions to read in data. This absolute pathname will always work, regardless of your current working directory, because it gives directions from the root. In other words, it will always be clear to R exactly what file you’re talking about. Here’s the code to use to read that file in using the read.csv() function with the file’s absolute pathname: daily_show &lt;- readr::read_csv(file = &quot;/Users/johnvolckens/Documents/Teaching/DataSci/edar_coursebook/data/daily_show_guests.csv&quot;, skip = 4) The relative pathname, on the other hand, gives R the directions for how to get to a directory or file from the current working directory. If the file or directory you’re looking for is pretty close to your current working directory in your directory structure, then a relative pathname can be a much shorter way to tell R how to get to the file than an absolute pathname. But, the relative pathname depends on your current working directory—the relative pathname that works perfectly when you’re working in one directory will not work at all once you move into a different working directory. As an example of a relative pathname, say you’re working directory is edar_coursebook and you want to read in the daily_show_guests.csv file in the data directory (one of the edar_coursebook subdirectories). To get from edar_coursebook to that file, you’d need to look in the subdirectory data, where you could find daily_show_guests.csv. Therefore, the relative pathname from your working directory would be: &quot;data/daily_show_guests.csv&quot; You can use this relative pathname to tell R where to find and read in the file: daily_show &lt;- readr::read_csv(&quot;data/daily_show_guests.csv&quot;) While this pathname is much shorter than the absolute pathname, it is important to remember that if you are working in a different working directory, this relative pathname would no longer work. There are a few abbreviations that can be really useful for pathnames: Shorthand Meaning ~ Home directory . Current working directory .. One directory up from current working directory ../.. Two directories up from current working directory These can help you keep pathnames shorter and also help you move “up-and-over” to get to a file or directory that’s not on the direct path below your current working directory. For example, my home directory is /Users/johnvolckens. You can use the list.files() function to list all the files in a directory. If I wanted to list all the files in my Downloads directory, which is a direct sub-directory of my home directory, I could use: list.files(&quot;~/Downloads&quot;) As a second example, say I was working in the working directory CourseText, (see Figure 3.1 but I wanted to read in the heat_mort.csv file that’s in the example_data directory, rather than the one in the CourseText directory. I can use the .. abbreviation to tell R to look up one directory from the current working directory, and then down within a subdirectory of that. The relative pathname in this case is: &quot;../Week2_Aug31/example_data/heat_mort.csv&quot; The ../ tells R to look one directory up from the working directory (the directory that is one level above the current directory is also known as the parent directory), which in this case is to RCourseFall2015, and then down within that directory to Week2_Aug31, then to example_data, and then to look wihtin that directory for the file heat_mort.csv. The relative pathname to read this file while R is working in the CourseTest directory would be: heat_mort &lt;- read_csv(&quot;../Week2_Aug31/example_data/heat_mort.csv&quot;) Relative pathnames “break” as soon as you try them from a different working directory—this fact might make it seem like you would never want to use relative pathnames, and would always want to use absolute ones instead, even if they’re longer. If that were the only consideration (length of the pathname), then perhaps that would be true. However, as you do more and more in R, there will likely be many occasions when you want to use relative pathnames instead. They are particularly useful if you ever want to share a whole directory, with all subdirectories, with a collaborator. In that case, if you’ve used relative pathnames, all the code should work fine for the person you share with, even though they’re running it on their own computer. Conversely, if you’d used absolute pathnames, none of them would work on another computer, because the “top” of the directory structure (i.e., for me, /Users/johnvolckens/) will definitely be different for your collaborator’s computer than it is for yours. If you’re getting errors reading in files, and you think it’s related to the relative pathname you’re using, it’s often helpful to use list.files() to make sure the file you’re trying to load is in the directory guided by the relative pathname. The list.files() function is very useful because it returnsa character vector of filenames (and paths, if desired). Once you have a vector of filenames you can do things like ask logical questions (does this file exist?), or count the number of files, or pass a relative path to a new function… 3.4.4 Tangent: paste This is a good opportunity to explain how to use some functions that can be very helpful when you’re using relative or absolute pathnames: paste() and paste0(). It’s important that you understand that you can save a pathname (absolute or relative) as an R object and then use that R object in calls to later functions like list.files() and read_csv(). For example, to use the absolute pathname to read the heat_mort.csv file in the CourseText directory, you could run: my_file &lt;- &quot;/Users/brookeanderson/Desktop/RCourseFall2015/CourseText/heat_mort.csv&quot; heat_mort &lt;- read_csv(file = my_file) You’ll notice from this code that the pathname to get to a directory or file can sometimes become ungainly and long. To keep your code cleaner, you can address this by using the paste or paste0 functions. These functions come in handy in a lot of other applications, too, but this is a good place to introduce them. The paste() function is very straightforward. It takes, as inputs, a series of different character strings you want to join together, and it pastes them together in a single character string. (As a note, this means that your resulting vector will only be one element long for basic uses of paste(), while the inputs will be several different character stings.) You separate all the different things you want to paste together using with commas in the function call. For example: paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;) ## [1] &quot;Sunday Monday Tuesday&quot; length(x = c(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;)) ## [1] 3 length(x = paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;)) ## [1] 1 The paste() function has an option called sep =. This tells R what you want to use to separate the values you’re pasting together in the output. The default is for R to use a space, as shown in the example above. To change the separator, you can change this option, and you can put in just about anything you want. For example, if you wanted to paste all the values together without spaces, you could use sep = \"\": paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, sep = &quot;&quot;) ## [1] &quot;SundayMondayTuesday&quot; As a shortcut, instead of using the sep = \"\" option, you could achieve the same thing using the paste0 function. This function is almost exactly like paste, but it defaults to \"\" (i.e., no space) as the separator between values by default: paste0(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;) ## [1] &quot;SundayMondayTuesday&quot; With pathnames, you will usually not want spaces. Therefore, you could think about using paste0() to write an object with the pathname you want to ultimately use in commands like list.files() and setwd(). This will allow you to keep your code cleaner, since you can now divide long pathnames over multiple lines: my_file &lt;- paste0(&quot;/Users/brookeanderson/Desktop/&quot;, &quot;RCourseFall2015/CourseText/heat_mort.csv&quot;) heat_mort &lt;- read_csv(file = my_file) You will end up using paste() and paste0() for many other applications, but this is a good example of how you can start using these functions to start to get a feel for them. 3.4.5 Reading online flat files So far, I’ve only shown you how to import data from files that are saved to your computer. R can also read in data directly from the web. If a flat file is posted online, you can read it into R in almost exactly the same way that you would read in a local file. The only difference is that you will use the file’s URL instead of a local file path for the file argument. With the read_* family of functions, you can do this both for flat files from a non-secure webpage (i.e., one that starts with http) and for files from a secure webpage (i.e., one that starts with https), including GitHub and Dropbox. For example, to read in data from this GitHub repository of Ebola data, you can run: url &lt;- paste0(&quot;https://raw.githubusercontent.com/cmrivers/&quot;, &quot;ebola/master/country_timeseries.csv&quot;) ebola &lt;- readr::read_csv(file = url) slice(.data = (dplyr::select(.data = ebola, 1:3)), 1:3) ## # A tibble: 3 x 3 ## Date Day Cases_Guinea ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1/5/2015 289 2776 ## 2 1/4/2015 288 2775 ## 3 1/3/2015 287 2769 3.5 Data cleaning Once you have loaded data into R, you’ll likely need to clean it up a little before you’re ready to analyze it. Here, I’ll go over the first steps of how to do that with functions from dplyr, another package in the tidyverse. Here are some of the most common data-cleaning tasks, along with the corresponding dplyr function for each: Task dplyr function Renaming columns rename() Filtering to certain rows filter() Selecting certain columns select() Adding or changing columns mutate() In this section, I describe how to do each of these four tasks. For the examples in this section, I use example data listing guests to the Daily Show. To follow along with these examples, you’ll want to load that data, as well as load the dplyr package. Install it using install.packages() if you have not done so already. library(&quot;dplyr&quot;) daily_show &lt;- readr::read_csv(file = &quot;data/daily_show_guests.csv&quot;, skip = 4) I’ve used this data in previous examples, but as a reminder, here’s what it looks like: head(x = daily_show) ## # A tibble: 6 x 5 ## YEAR GoogleKnowlege_Occupation Show Group Raw_Guest_List ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman ## 4 1999 film actress 1/14/99 Acting Gillian Anderson ## 5 1999 actor 1/18/99 Acting David Alan Grier ## 6 1999 actor 1/19/99 Acting William Baldwin 3.5.1 Renaming columns A first step is often renaming the columns of the dataframe. It can be hard to work with a column name that: is long includes spaces or other special characters includes uppercase letters You can check out the column names for a dataframe using the colnames() function, with the dataframe object as the argument. Several of the column names in daily_show have some of these issues: colnames(x = daily_show) ## [1] &quot;YEAR&quot; &quot;GoogleKnowlege_Occupation&quot; ## [3] &quot;Show&quot; &quot;Group&quot; ## [5] &quot;Raw_Guest_List&quot; To rename these columns, use rename(). The basic syntax is: ## generic code; will not run dplyr::rename(.data = dataframe, new_column_name_1 = old_column_name_1, new_column_name_2 = old_column_name_2) The first argument is the dataframe for which you’d like to rename columns. Then you list each pair of new and old column names (in that order) for each of the columns you want to rename. To rename columns in the daily_show data using rename(), for example, you would run: daily_show &lt;- dplyr::rename(.data = daily_show, year = YEAR, job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) head(x = daily_show, 3) ## # A tibble: 3 x 5 ## year job date category guest_name ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman Many of the functions in tidyverse packages, including those in dplyr, provide exceptions to the general rule about quotation marks. Unfortunately, this may make it a bit hard to learn when to use quotation marks. One way to think about this, which is a bit of an oversimplification but can help as you’re learning, is to assume that anytime you’re using a dplyr function, every column in the dataframe you’re working with has been loaded to your R session as its own object, which means you don’t need to use parentheses—most of the time. If you have been paying close attention to the code snippets, you may have noticed the last bit of code included both the package name and the function call separated by two colons, as in dplyr::rename(). This syntax of package.name::package.function is used for the sake of being explicit, because (as you may have guessed) some R packages use the same for functions that do entirely different things! For example, both the base R stats package and the dplyr package have a function called filter() - the former is used to pick our rows from a data frame and the latter is used to manipulate time-series objects. When two packages are loaded containing functions with the same name, R will default to using the function for the most recently loaded package (and send you a message stating as much). This can be tricky business when your R session has many packages running, which is why it never hurts to be explicit in your function calls. 3.5.2 Selecting columns Next, you may want to select only some columns of the dataframe. You can use the select() function from dplyr to subset the dataframe to certain columns. The basic structure of this command is: ## generic code; will not run dplyr::select(.data = dataframe, column_name_1, column_name_2, ...) In this call, you first specify the dataframe to use and then list all of the column names to include in the output dataframe, with commas between each column name. For example, to select all columns in daily_show except year (since that information is already included in date), run: dplyr::select(.data = daily_show, job, date, category, guest_name) ## # A tibble: 2,693 x 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman ## 4 film actress 1/14/99 Acting Gillian Anderson ## 5 actor 1/18/99 Acting David Alan Grier ## 6 actor 1/19/99 Acting William Baldwin ## 7 Singer-lyricist 1/20/99 Musician Michael Stipe ## 8 model 1/21/99 Media Carmen Electra ## 9 actor 1/25/99 Acting Matthew Lillard ## 10 stand-up comedian 1/26/99 Comedy David Cross ## # … with 2,683 more rows Don’t forget that, if you want to change column names in the saved object, you must reassign the object to be the output of rename(). If you run one of these cleaning functions without reassigning the object, R will print out the result, but the object itself won’t change. You can take advantage of this, as I’ve done in this example, to look at the result of applying a function to a dataframe without changing the original dataframe. This can be helpful as you’re figuring out how to write your code. The select() function also provides some time-saving tools. In the last example, we wanted all the columns except one. Instead of writing out all the columns we want, we can use - with only the columns we don’t want to save time (notice the object reassignment/override): daily_show &lt;- dplyr::select(.data = daily_show, -year) head(x = daily_show, n = 3) ## # A tibble: 3 x 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman 3.5.3 Add or change columns You can change a column or add a new column using the mutate() function from the dplyr package. That function has the syntax: # generic code; will not run dplyr::mutate(.data = dataframe, changed_column = function(changed_column), new_column = function(other arguments)) For example, the job column in daily_show sometimes uses upper case and sometimes does not. This call uses the unique() function to list only unique values in this column: head(x = unique(x = daily_show$job), n = 10) ## [1] &quot;actor&quot; &quot;Comedian&quot; &quot;television actress&quot; ## [4] &quot;film actress&quot; &quot;Singer-lyricist&quot; &quot;model&quot; ## [7] &quot;stand-up comedian&quot; &quot;actress&quot; &quot;comedian&quot; ## [10] &quot;Singer-songwriter&quot; To make all the observations in the job column lowercase, use the str_to_lower() function from the stringr package within a mutate() function: library(package = &quot;stringr&quot;) mutate(.data = daily_show, job = str_to_lower(string = job)) ## # A tibble: 2,693 x 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman ## 4 film actress 1/14/99 Acting Gillian Anderson ## 5 actor 1/18/99 Acting David Alan Grier ## 6 actor 1/19/99 Acting William Baldwin ## 7 singer-lyricist 1/20/99 Musician Michael Stipe ## 8 model 1/21/99 Media Carmen Electra ## 9 actor 1/25/99 Acting Matthew Lillard ## 10 stand-up comedian 1/26/99 Comedy David Cross ## # … with 2,683 more rows We will take a deeper dive into strings and the stringr package later on. 3.5.4 Base R equivalents to dplyr functions Just so you know, all of these dplyr functions have alternatives, either functions or processes, in base R: dplyr Base R equivalent rename() Reassign colnames() select() Square bracket indexing filter() subset() mutate() Use $ to change or create columns slice() subset() with logical expression You will see these alternatives used in older code examples. Some of these functions have variants specific to particular data wrangling needs. For example, under slice(), there are others such as slice_max() and slice_min(), which extract the top and bottom values, respectively, from a dataset based on user input in the required arguments, including n and order_by. 3.5.5 Filtering to certain rows Next, you might want to filter the dataset to certain rows. For example, you might want to get a dataset with only the guests from 2015, or only guests who are scientists. You can use the filter() function from dplyr to filter a dataframe down to a subset of rows. The syntax is: ## generic code; will not run filter(.data = dataframe, logical expression) The logical expression in this call gives the condition that a row must meet to be included in the output data frame. For example, if you want to create a data frame that only includes guests who were scientists, you can run: scientists &lt;- filter(.data = daily_show, category == &quot;Science&quot;) head(x = scientists) ## # A tibble: 6 x 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 neurosurgeon 4/28/03 Science Dr Sanjay Gupta ## 2 scientist 1/13/04 Science Catherine Weitz ## 3 physician 6/15/04 Science Hassan Ibrahim ## 4 doctor 9/6/05 Science Dr. Marc Siegel ## 5 astronaut 2/13/06 Science Astronaut Mike Mullane ## 6 Astrophysicist 1/30/07 Science Neil deGrasse Tyson To build a logical expression to use in filter, you’ll need to know some of R’s logical operators. Some commonly used ones are: Operator Meaning Example == equals category == \"Acting\" != does not equal category != \"Comedy %in% match; contains the following category %in% c(\"Academic\", \"Science\") is.na() is missing is.na(job) !is.na() is not missing !is.na(job) &amp; and year == 2015 &amp; category == \"Academic\" | or year == 2015 | category == \"Academic\" We’ll use these logical operators and expressions a lot more as the course continues, so they’re worth memorizing. Two common mistakes with logical operators are: (1) Using = instead of == to check if two values are equal; and (2) Using == NA instead of is.na to check for missing observations. 3.6 Piping So far, I’ve shown how to use these dplyr functions one at a time to clean up the data, reassigning the dataframe object at each step; however, there’s a trick called “piping” (with %&gt;%) that will let you complete multiple data wrangling steps at once. If you look at the format of these dplyr functions, you’ll notice that they all take a dataframe as their first argument: # generic code; will not run rename(.data = dataframe, new_column_name_1 = old_column_name_1, new_column_name_2 = old_column_name_2) select(.data = dataframe, column_name_1, column_name_2) filter(.data = dataframe, logical expression) mutate(.data = dataframe, changed_column = function(changed_column), new_column = function(other arguments)) Without piping, you have to reassign the dataframe object at each step of this cleaning if you want the changes saved in the object: daily_show &lt;-read_csv(file = &quot;data/daily_show_guests.csv&quot;, skip = 4) daily_show &lt;- rename(.data = daily_show, job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) daily_show &lt;- select(.data = daily_show, -YEAR) daily_show &lt;- mutate(.data = daily_show, job = str_to_lower(job)) daily_show &lt;- filter(.data = daily_show, category == &quot;Science&quot;) Piping lets you streamline this process. It can be used with any function that inputs a dataframe (or vector) as its first argument. The %&gt;% operator pipes the object on the left-hand-side of the pipe (%&gt;%) into the function on the right-hand-side (immediately after the pipe). With piping, therefore, all of the data cleaning steps shown avove would look like: daily_show &lt;- readr::read_csv(file = &quot;data/daily_show_guests.csv&quot;, skip = 4) %&gt;% dplyr::rename(job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) %&gt;% dplyr::select(-YEAR) %&gt;% dplyr::mutate(job = str_to_lower(job)) %&gt;% dplyr::filter(category == &quot;Science&quot;) Notice that, when piping a data frame, the first argument (name of the data frame) is excluded from all function calls that follow a pipe. This is because piping sends the dataframe from the last step into each of the following functions as the dataframe argument. Remember: Order matters in a data wrangling pipeline. For example, if you removea a column in an early line of code in the pipeline but then reference that column name later, R will throw an error. You can use selective highlighting to run one line at a time to see how the dataframe changes in real-time as you move through successive pipes. Piping with %&gt;% should only be used when you want to perform succesive data wrangling steps on a single object. Each pipe operation should be followed by a new line, as shown above. Creating a new line after each pipe step aids readability of the pipe, since each new action occurs on a new line of code. Also, if a single pipe function contains multiple arguments, consider putting each argument on a separate line, too (also shown in the code snippet above). 3.7 Markdowns A markdown is a file format designed for the internet. Markdown files allow you to enter plain text into a file, format that text, and embed code/images/data into the file (everything you are reading in this coursebook was written and created with markdown files). Markdown files are versatile because: Markdowns can be rendered into html, pdf, and doc files easily. Thus, markdown files can be turned into websites, email messages, reports, blogs, textbooks, and other forms of media without worry; Markdowns are independent of the operating system (Mac, PC, Linux, Android, and iOS devices can read them); Markdowns can be opened by almost any application (the file format is non-proprietary), so you don’t need to worry about having special software to read them. A markdown document provides an excellent template for reproducible research - allowing you to communicate what you did, how and why you did it, what you found, and any conclusions (or follow-on questions) you can draw from the work. 3.7.1 R Markdowns The R Studio IDE can create “R Markdowns” (file extension .Rmd) specifically for the R programming environment. An R markdown file allows you do lots of things; we will use them to create assignments and homework reports that display R code, the outputs of that code, and plain text. To use the R markdown format, you need to install the rmarkdown package: install.packages(\"rmarkdown\"). Going forward, all of your homework and coding assignments will be created and submitted in the R Markdown format using either html or pdf outputs. This may seem uncomfortable at first but you will get accustomed to this format quickly. Each R markdown file contains three basic elements: header, text, and code chunks. I will explain each of these elements below, but I recommend a visit to the R Markdown section on the RStudio website. A detailed guide on many of the R markdown output styles (beyond just html and pdf files) is provided in R Markdown: The Definitive Guide. 3.7.2 Header The R Markdown “header” section is where you specify details about the file being created. A markdown header contains YAML metadata, which stands for “YAML Ain’t Markup Language”. The YAML (pronounced like “camel”) header is essentially a list of directives (referred to as “key:value” pairs) that help application software interpret the file. A YAML header can act simultaneously as a “configuration file”, a “log file”, and “translator file” - allowing one software program to read the output of another program. An example header with YAML metadata is shown below. Figure 3.2: Example of a YAML header to render an R Markdown into an html file. The header is delineated at the top of the file by a section that begins and ends with three dashes, “—”. Within the header are YAML metadata representing key:value pairs. What are “key:value” pairs? The “key:” is a directive that you want to give to the file and the “value” represents the level of detail or information that you want to associate with that directive. Key:value pairs provide instructions on how the file should be read, interpreted, and output. In Figure 3.2, the keys are “title:”, “author:”, “date:”, and “output:” and the corresponding values are “Markdowns”, “JV”, “7/23/2020”, and “html_document”. You can learn more about key:value pairs in the R Markdown Style Guide for html and pdf. The YAML header is optional in an R Markdown and default key:value pairs will be implemented if none are supplied. That said, I would encourage you to specify key directives like “author:”, “date:” and “output:” in your YAML headers. Sometimes, you will want to provide nested formatting directives in your markdown header. For example, you can specify the addition of a “table of contents” to your html output file that “floats” alongside the text. In that case, your YAML metadata would look like this: Figure 3.3: Example R Markdown header with nested YAML directives to render an html file with a floating table of contents. An important detail to remember with nested YAML metadata is that each nested command must be indented by 2 spaces to be interpreted properly. 3.7.3 Text The default space within an R markdown is a plain text editor, similar to a normal word processing file. Formatting text is more tedious in markdown files (a small price to pay given their versatility). Some basic formatting operations are shown below. Format Desired Typeset in Markdown Example Output Italics *one star on each side* one star on each side Bold **two stars on each side** two stars on each side Superscript superscript^2^ superscript2 Subscript subscript~2~ subscript2 To start a new paragraph in a markdown text section, end a line with two spaces (followed by a return). To see a more complete set of formatting options see the R Markdown CheatSheet provided by RStudio. 3.7.4 Code Chunks Code chunks are the places where you write and execute R code. A code chunk is initiated with 3 back ticks ```, followed by a set of braces { }, or curly brackets, wihtin which you can name the chunk and specify chunk options. The chunk options tell the knitr package (the package that renders an R markdown into an output style) how you want that chunk to run and what to do with the output. A list of chunk options can be found here. An example markdown is shown below: Figure 3.4: Example R Markdown showing header, text, and code chunks. Once your markdown is complete, you can render it into an output file (e.g., html, pdf, doc, rtf) using the knitr package, which interprets your YAML header and “knits” the markdown sections into the desired format. Here is the same markdown rendered into an html document using the knit button. Figure 3.5: Example R Markdown when “rendered” into an html document. 3.8 Chapter 3 Exercises The following sections includes exercises for each lecture regarding Chapter 3. Rhetorical or group discussion questions are also included to help you think about why we are making certain workflow recommendations and to get you in the habit of doing these things regularly. Throughout, I included challenge questions will require you to look outside of the lecture and coursebook. These are optional and do not include answers within the coursebook, but give them a shot. They cover concepts that are important to you as an independent R programmer. 3.8.1 Set 1: Pathnames and data import Open your class R Project locally. Restart session, if your R Project is already open. Create an R script (e.g., ch-3-exercises.R) and save it in the /code folder. In the RStudio Console, determine your current working directory. Make sure the working directory is pointed to the location of ch-3-exercises.R within your R Project. If it is not, navigate to Session &gt; Set Working Directory &gt; To Source File Location. Again, check your current working directory to confirm. Write “metadata” at the top of the R script using #, including your name, today’s date, class, and chapter. Add a heading called “Pathnames and Data Import”. Download the Fort Collins ozone dataset locally and save the .csv file in the /data folder in your R Project. What would be a better, more descriptive name for the data file that also follows file-naming conventions (e.g., no spaces, human- and machine-readable)? Change the file name locally. Commit the additions to GitHub (e.g., “add ozone data file”, “create ch3 r script”) but leave a queue of commits to push at the end of the class. What is the absolute pathname of the ozone data file on your local computer? Using paste0(), save your absolute pathname as an object called ozone_abs_path. What is the relative pathname of the ozone data file on your local computer? Using the appropriate function call from the readr package, within the package::function() coding style, import the ozone data using the relative pathname and assign it to a dataframe/tibble object called ozone_data. Now, try importing the data using the ozone_abs_path object. Commit this work. What kind of problems would you encounter if you regularly used absolute pathnames in your R scripts in setting the working directory or importing/exporting files? Rerun the line of code for data import with the relative pathname. What kind of message did R return in the console when you imported the data file? What information can you glean from this? In your R script, execute some of the suggested function calls (from previous lectures and chapters) that provide similar information to “see” and explore the dataframe/tibble object. Examine the output. Make a commit. Push all queued commits to GitHub. PSA: Regularly follow this workflow of small, regular commits and intermittent pushes in everything you do in R. # 1. working directory getwd() # 4. relative pathname ozone_data &lt;- readr::read_csv(file = &quot;data/ftc_o3.csv&quot;) # 4. absolute pathname - for illustration - not recommended usually ozone_abs_path &lt;- base::paste0(&quot;/Users/johnvolckens/Documents/Teaching/DataSci&quot;, &quot;/edar_coursebook/data/ftc_o3.csv&quot;) ozone_abs_ex &lt;- readr::read_csv(file = ozone_abs_path) # 5. possible functions for initial data view and check tibble::glimpse(ozone_data) head(ozone_data) tail(ozone_data) str(ozone_data) summary(ozone_data) dim(ozone_data) nrow(ozone_data) ncol(ozone_data) length(ozone_data) colnames(ozone_data) class(ozone_data$[selected_colname]) # example structure; will not run 3.8.2 Set 2: Data wrangling and piping Note: In the real world, you would not receive such a “tidy” data file as you will use for the following exercises. You would normally need to clean up the file structure, variables, and column names before anything else, typically within a “pipe” of “verbs” from dplyr. Before you write any code, however, you need to think about what kind of wrangling needs to be done for your data. I recommend you literally sketch our your desired dataframe result. But, in this case, don’t worry because we did that leg work for you; the ozone data file is clean upon import. In the future, keep the handy function janitor::clean_names() in mind to clean all variable names at once. Also, peruse Hadley Wickham’s paper on “tidy data” to understand what it is and how to make messy data tidy. Open your class R Project locally. Restart session, if R Project is already open. Open R script you used for the first set of exercises. Rerun the code to populate your environment, i.e., load and view the data. Using dplyr::select() and the pipe (%&gt;%), select the sample_measurement, units_of_measure, and datetime and assign the resulting dataframe/tibble to an object called ozone_hourly. Remember to load (and install, if needed) the related R packages! What are some of the R packages that contain the pipe? There are many, and you have been introduced to at least one. Examine the ozone_hourly dataframe. Do you notice anything strange about the sample_measurement values? If not, try to find the range, minimum, or maximum of those values. You can’t—there are NA values. Add a line to your ozone_hourly pipe in the previous question that removes the missing observations, using drop_na() from the tidyr package. Note: There are base R alternatives for removing missing observations, but, for the sake of continuity within your pipe, try to use drop_na(). Remember, not all functions require formal arguments. How many observations do you lose due to missingness? Obtain a six-number summary of the sample_measurement vector. What is the minimum value? Maximum? How do these compare to the median and mean? Using another dplyr verb but no pipe, determine how many times the ozone measurement was above 0.07. Now, re-write the code on a new line using a pipe. Save the output of one of these approaches to a new df, following naming conventions. Overwriting the ozone_hourly dataframe and using a different dplyr verb within a pipe, create a new column (ozone_ppb) by multiplying the “parts-per-million” values from sample_measurement by 1000. Challenge: Generate a histogram of sample_measurement. # 0. set-up ## rerun code from the first set of exercises # 1. select variables and create new df library(magrittr) # original R package for pipe %&gt;% library(dplyr) # for verbs; also contains pipe ozone_hourly &lt;- ozone_data %&gt;% dplyr::select(sample_measurement, datetime) %&gt;% tidyr::drop_na() # 2. check df ## see example code from first set for possible initial viewing functions range(ozone_hourly$sample_measurement) min(ozone_hourly$sample_measurement) max(ozone_hourly$sample_measurement) # manage missingness ozone_hourly &lt;- ozone_data %&gt;% dplyr::select(sample_measurement, datetime) %&gt;% tidyr::drop_na() # 3. missingness and summary ## 606 observations were lost due to missingness summary(ozone_hourly$sample_measurement) # 4. filter ozone level ## without pipe dplyr::filter(.data = ozone_data, sample_measurement &gt; 0.070) ## with pipe ozone_hourly %&gt;% dplyr::filter(sample_measurement &gt; 0.070) ## save output from one of the above approaches high_ozone &lt;- ozone_hourly %&gt;% dplyr::filter(sample_measurement &gt; 0.070) # create new column ozone_hourly &lt;- ozone_hourly %&gt;% dplyr::mutate(ozone_ppb = sample_measurement*1000) 3.8.3 Set 3: R Markdown Open your class R Project locally. Restart session, if R Project is already open. Create a new R Markdown file (.Rmd) in RStudio IDE (File &gt; New File &gt; R Markdown) and pick a title (of document, not file) such as “Chapter 3 Exercises”. Name the file (“ch-3-exercises.Rmd”) and save it to your local R Project in /code. In the RStudio Console, determine your current working directory. Make sure the working directory is pointed to the location of .Rmd within your R Project. If it is not, navigate to Session &gt; Set Working Directory &gt; To Source File Location. Again, check your current working directory to confirm. Knit the sample R Markdown to HTML and/or PDF (if you installed LaTex). Commit and push the sample R Markdown and its knitted counterpart to your repository on GitHub with a short, informative commit message such as “test knit rmd template”. Delete the sample content from the R Markdown file (line 8 and beyond) and recommit the change (e.g., “delete rmd template contents”). Below the YAML ( where does the YAML end, and what tells you that? ), copy your code from the R script you used for the prior sets of exercises into the R Markdown file. Organize this code into relevant headings and within thematically appropriate code chunks. For example, the code for data import would be in its own code chunk. Add first- , second-, and third-order headings (e.g., “Data Import”, “Using Relative Pathname”, \"Using Absolute Pathname), using the appropriate Markdown symbol. What does this symbol also do (to a completely different effect) within R scripts or inside R Markdown code chunks? Knit this document to HTML and/or PDF and examine how R Markdown returns output. 3.8.3.1 Challenge Questions Change the date to a YYYY-MM-DD format within the R Markdown YAML such that it changes every time you re-knit the document on a different day. Implement a change to the code chunk itself that suppresses the message returned upon data import. Create a global chunk option below the YAML, telling the R Markdown file (a) where to automatically save figures within your R Project directory using a short, relative pathname and (b) to suppress warnings and messages when knitting. What are some other things you can do in a global options code chunk? Compare these to what can be done in an individual code chunk. 3.9 Chapter 3 Homework You will continue to explore and manipulate data on ozone measurements in Fort Collins, CO during January 2019. On Canvas, download the ozone data (.csv) and R Markdown template (.Rmd), which includes a description of the data, the homework questions, and the general framework of code-figure-text integration, including the framework for a code appendix. Save the data and template files in your local R Project in the /data and /homework folders, respectively. Remember to check and set your working directory (e.g., Session &gt; Set Working Directory &gt; To Source File Location) to point from the R Markdown file and detect the data file in /data and also points to /figs as the place to save figures and images, which is determined in the R Markdown global option chunk I include in the R Markdown template. This homework assignment is due at the start of the class when we begin Chapter 4. We will look for the R Markdown file and the corresponding knitted PDF or HTML document within your /homework file. Remember, make regular, memorable commits, so you never lose your work. Your work will be considered late if the latest knit occurs after the deadline. "],
["dataviz.html", "Chapter 4 Data Visualization with ggplot2 4.1 Chapter 4 Objectives 4.2 Install and load ggplot2 4.3 Steps to create a ggplot2 object 4.4 Initializing a ggplot2 object 4.5 Plot aesthetics 4.6 Adding geoms 4.7 Shapes and colors 4.8 Scales: useful plot edits 4.9 ggplot2 example 1 4.10 ggplot2 example 2 4.11 Store and save ggplot2 objects 4.12 Getting help with ggplot2 4.13 Chapter 4 Homework", " Chapter 4 Data Visualization with ggplot2 4.1 Chapter 4 Objectives This chapter is designed around the following learning objectives for basic data visualization in R. Upon completing this chapter, you should be able to: Install, load, and use ggplot2 functions to visualize dataframe elements Differentiate between data, aesthetics, and layers in a ggplot2 object Customize element properties such as color, size, and shape in a ggplot2 layer Create, store, and save a ggplot2 object in an R script 4.2 Install and load ggplot2 “The best design gets out of the way between the viewer’s brain and the content.” - Edward Tufte In this chapter, you will learn how to make basic plots using the ggplot2 package in R, which is another package in tidyverse, like dplyr and readr. This section will focus on making useful, rather than attractive graphs because, at this stage, we are focusing on exploring data rather than presenting results to others. Later on, you will learn about how to customize ggplot2 objects. Customization often helps to make plots that “get out of the way” between the content you wish to present and the viewer’s brain, wherein you hope understanding takes root. If you don’t already have ggplot2 installed, you’ll need to install it. You then need to load the package in your current session of R: # install ggplot2 package (done once per R/RStudio installation) install.packages(&quot;ggplot2&quot;) # load ggplot2 in current R session library(ggplot2) Alternatively, if you are planning on using other tidyverse R packages in the same R session, you can simply install and load the tidyverse “R package suite of R packages” with library(tidyverse). 4.3 Steps to create a ggplot2 object The process of creating a plot using ggplot2 follows conventions that are a bit different than most of the code you’ve seen so far in R, although it is somewhat similar to the idea of piping I introduced in the last chapter. The basic steps behind creating a plot with ggplot2 are: Create an object of the ggplot2 class, typically specifying the data and some or all of the aesthetics Add a layer or geom to the plot, along with other specific elements, using + Aesthetics or aes() in R represent the things that we are plotting: the x and y data. Geoms like geom_point() represent the way in which we layer the aesthetics onto the plot. The geom is the type of plot that we are calling. You can layer on one or many geoms and other elements to create plots that range from very simple to very customized. We will start by focusing on simple geoms and added elements; later on, we will explore more options for customization. 4.4 Initializing a ggplot2 object The first step in creating a plot using ggplot() is to create a ggplot object. This object will not, by itself, create a plot with anything in it. Instead, this first step typically specifies the data frame you want to use and which aesthetics will be mapped to certain columns of that data frame. Aesthetics are explained more in the next subsection. Outside of a pipeline, you can use the following conventions to initialize a ggplot2 object: ## generic code; will not run object &lt;- ggplot(data = my_dataframe, aes(x = data_column_1, y = data_column_2)) The dataframe is the first parameter in a ggplot() function and, if you like, you can use the parameter definition with that call (e.g., data = dataframe). Aesthetics are defined within an aes() function call that is typically defined within the ggplot() function. While the ggplot() call is the place where you will most often see an aes() call, you can also make calls to aes() within the calls to specific geoms. This can be particularly useful if you want to map aesthetics differently for different geoms in your plot. We’ll see some examples of this use of aes() more in later sections, when we talk about customizing plots. The data = argument can be used in specific geom calls to use different dataframes (from the one defined when creating the original ggplot object), although this is less common. 4.5 Plot aesthetics Aesthetics are properties of the plot that can show certain elements of the data. For example, in Figure 4.1, we call an x-axis aesthetic (x = class) from the mpg dataset. We then plot counts of cars within different vehicle classes using geom_bar(). The mpg dataframe is included in the ggplot2 package; you can learn more about it by typing ?mpg (no parentheses) into the console after loading ggplot2 with library(ggplot2). You can also learn more by typing str(mpg) and head(mpg). As seen in Chapters 2 and 3, one should always look at the data upon import and examine the dataframe structure and variable classes. According to ?mpg: “This dataset contains a subset of the fuel economy data that the EPA makes available on http://fueleconomy.gov. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car.” # use ggplot() to map the data and a single aesthetic (variable = class) ggplot(data = mpg, aes(x = class)) + geom_bar() # call to a specific geom to plot the mapped data Figure 4.1: Example of a simple call to ggplot showing counts of vehicle classes from the mpg dataframe. Let’s call this plot again with a second aesthetic, the fill color, which will be mapped to drv, a variable in the mpg data frame that specifies vehicle drive type (i.e., 4-wheel, font-wheel, or rear-wheel). The x-position will continue to show vehicle class (class), but we will fill each bar with colors pertaining to drv (i.e., to show the counts within each vehicle class colored colored by drv). # call to ggplot to map the data and a single aesthetic ggplot(data = mpg, aes(x = class, fill = drv)) + geom_bar() # call to a specific geom to plot the mapped data Figure 4.2: Example of a call to ggplot showing counts of vehicle classes from the mpg dataframe and colored by the fill aesthetic mapped to drive type (drv). What new information can we learn from the Figure 4.2? For starters, we can see that compact and mid-size cars tend to be front-wheel drive, whereas pickups and SUVs (who tend to share the same chassis) tend to be 4-wheel drive. This result is not surprising to anyone who studies cars and trucks, but it’s nice to confirm one’s knowledge with quantitative data! ggplot() will choose colors and add legends to plots when an aesthetic mapping creates such opportunities. You will learn ways to customize colors, legends, and other plot elements later. Which aesthetics are required for a plot depend on which geoms (more on those in a second) you’re adding to the plot. You can find out the aesthetics you can use for a geom in the “Aesthetics” section of the geom’s helpfile (e.g., ?geom_bar). Required aesthetics are often shown in bold in this section of the helpfile. You can also view a concise summary of aesthetic specification by typing vignette(\"ggplot2-specs\") into the R console. Common plot aesthetics you might want to specify include: Table 4.1: Common Plot Aesthetics Code Description x Variable to plot on x-axis y Variable to plot on y-axis shape Shape of the element being plotted color Color of border of elements fill Color of inside of elements size Size of the element alpha Transparency (1: opaque; 0: transparent) linetype Type of line (e.g., solid, dashed) 4.6 Adding geoms When creating plots, you’ll often want to add more than one geom to the plot. You can add these with + after the ggplot() statement to initialize the ggplot2 object. Some of the most common geoms are: Table 4.2: Common Plot Aesthetics Plot type ggplot2 function Histogram (1 numeric variable) geom_histogram() Scatterplot (2 numeric variables) geom_point() Boxplot (1 numeric variable, possibly 1 factor variable) geom_boxplot() Line graph (2 numeric variables) geom_line() A common error when writing ggplot2 code is to put the + to add a geom or element at the beginning of a line rather than the end of a previous line. In this case, R will try to execute the call too soon. If R gets to the end of a line and there is no indication to continue the call (e.g., %&gt;% for piping or + for ggplot2 plots), R interprets that as a message to run the call without reading in further code. Thus, to avoid errors, be sure to end each line in ggplot2 calls with +, except for the final line when the call is actually done. Don’t start lines with +. 4.6.1 Aesthetic override: a warning The ggplot2 package, like many tidyverse packages, is both flexible and forgiving, designed to accommodate the user by “filling in the blanks” when no information is provided. For example, in the ggplot() call that created Figure 4.2, we didn’t specify the colors to be used or the contents of the legend; instead, ggplot2 figured those out for us. The ggplot2 package is also somewhat flexible in how calls and aesthetic mappings can be structured. For example, the following four calls all produce the same (identical) plot as shown in Figure 4.2. Try it for yourself. # call to ggplot() with aes() specified in main call ggplot(data = mpg, aes(x = class, fill = drv)) + geom_bar() # call to ggplot() with aes() specified in geom ggplot(data = mpg) + geom_bar(aes(x = class, fill = drv)) # call to ggplot() with a mix of aes() mappings ggplot(data = mpg, aes(x = class)) + geom_bar(aes(fill = drv)) # call to ggplot() with all mappings in the geom ggplot() + geom_bar(data = mpg, aes(x = class, fill = drv)) For most plots that you make, the first example is best, where the aesthetics are called out as arguments within the main call to ggplot(), such as: ggplot(data = mpg, aes(x = class, fill = drv)) + geom_bar() In this case, the geom_bar() function inherits the aesthetics that were called above in the main ggplot call. Specifying aesthetics in the main call to ggplot() makes it easier to keep track of what you are trying to do! The ggplot flexibility also comes with occasional confusion, as you can often override one mapping with another one later on in the same call. For example, see what happens when two different fill mappings are specified at different points in the call: # call to ggplot where one `fill` overrides another ggplot(data = mpg, aes(x = class, fill = drv)) + geom_bar(fill = &quot;darkgreen&quot;) Figure 4.3: Example of a call to ggplot showing counts of vehicle classes from the mpg dataframe and colored by the fill aesthetic mapped to drive type (drv). In this case, the aesthetic mapping of aes(fill = drv) was overridden by the specification in geom_bar(), where we wrote fill = \"darkgreen\". This second specification essentially wiped away the stacked bar colors and the legend, as shown in Figure 4.2. As your ggplot2 objects become more customized this sort of issue can arise; it comes with the territory of having flexible code. 4.7 Shapes and colors In R, you can specify the shape of points with a number. Figure 4.4 shows the shapes that correspond to the numbers 1 to 25 in the shape aesthetic. This figure also provides an example of the difference between color (black for all these example points) and fill (red for these examples). You can see that some point shapes include a fill (21 for example), while some are either empty (1) or solid (19). Figure 4.4: Examples of the shapes corresponding to different numeric choices for the shape aesthetic. For all examples, color is set to black and fill to red. If you want to set color to be a constant value, you can do that in R using character strings for different colors. Figure 4.5 gives an example of some of the different blues available in R. To find links to listings of different R colors, look up “R colors” and search by “Images”. Note that colors are specified as character strings and define using quotes \" \". See the code chunk for Figure 4.3 where color is defined by fill = \"darkgreen\". Figure 4.5: Example of available shades of blue in R. 4.8 Scales: useful plot edits The ggplot2 package uses scales as a way to make all sorts of tweaks and changes to how the plot is presented. According to the ggplot2 documentation: “Scales control the details of how data values are translated to visual properties. Override the default scales to tweak details like the axis labels or legend keys, or to use a completely different translation from data to aesthetic.” There are many scale elements that you can add onto a ggplot2 object using +. A few that are used very frequently are: Table 4.3: Common Scale Elements Element Description ggtitle() Plot title xlab(), ylab() Labels for x- and y-axis xlim(), ylim() Limits of x- and y-axis scale_x_log10() Log scale of x-axis Note: There is also a separate R package called scales, which has various function options to automatically detect and show breaks and labels for axes and legends. These additional functions can be called on a ggplot object. 4.9 ggplot2 example 1 For the example plots, we will continue to use the mpg dataset from the ggplot2 package. We will use functions from the dplyr package, too, so both need to be loaded. Fortunately, the ggplot2 package is loaded in addition to regular tidyverse packages when you call library(tidyverse). The first example is actually of two similar scatterplots: one using geom_point() and one using geom_jitter(). These plots will examine the agreement (i.e., the correlation) between a vehicle’s highway (hwy) and city (cty) fuel economies for model year 2008. Because the mpg dataset contains data from model year 1999 and 2008, we will apply a dplyr::filter() command within our call to ggplot() to limit the data to year == 2008. We will also color the points on the plot according to the class of vehicle. If you are wondering what types of vehicle classes are included with mpg, you could type unique(mpg$class) or, if you want to see a quantitative summary, you could pipe together the following: # quantitative summary in pipe mpg %&gt;% dplyr::filter(year == 2008) %&gt;% dplyr::group_by(class) %&gt;% dplyr::tally() %&gt;% dplyr::ungroup() ## # A tibble: 7 x 2 ## class n ## &lt;chr&gt; &lt;int&gt; ## 1 2seater 3 ## 2 compact 22 ## 3 midsize 21 ## 4 minivan 5 ## 5 pickup 17 ## 6 subcompact 16 ## 7 suv 33 # alternative mpg %&gt;% dplyr::filter(year == 2008) %&gt;% dplyr::count(class) # load required R packages library(dplyr) # for data wrangling and manipulation library(ggplot2) # for data visualization ## alternatively, use `library(tidyverse)`, if you will need multiple packages ggplot2::ggplot() + geom_point(data = dplyr::filter(mpg, year == 2008), # filter data aes(x = hwy, y = cty, color = class)) # assign x, y, and color Figure 4.6: Scatterplot (geom_point) of highway vs. city fuel economy for model year 2008, colored by vehicle type A few things to note about this plot. First, there is a clear relationship between a vehicle’s highway and city fuel economy, but the correlation is not necessarily one-to-one. Second, we can see that compact and midsize cars tend to have better fuel efficiency that pickups and SUVs (…duh). Third, in my opinion, this plot has a few drawbacks: If you were paying close attention, the sum of the tally() function above reported over 100 different entries (117 to be precise), but the plot above shows only about 50 data points… Why? (Hint: if you look at the mpg data, the fuel economies are rounded to the nearest mile per gallon.) We will address this issue with geom_jitter() below. The limits of the x- and y-axes are not equal, which distorts the relationship a bit. We will address this issue with coord_fixed() below. The relationship between cty and hwy might be easier to distinguish if we drew a “one-to-one” line on the plot (i.e., y = x). We will accomplish this need with geom_abline below. Personally, I don’t like the grey background and I think that the x and y axis labels are a little vague. I prefer my axis labels to be more descriptive and to communicate the units being plotted. We will use a theme_ call to clean up the background and will specify axis labels with xlab() and ylab() elements. Below is the same data from Figure 4.6 plotted using geom_jitter(). This geom is just like geom_point() except that it allows the plotted points to contain some “jitter” (a small amount of wobble as to where the point actually shows up in x and y space) so that overlapping data points can be distinguished from one another. The degree of jitter is set using height = and/or width = arguments. Note that adding jitter to data is the same as making the plotted data less precise (in a random way) so be careful not to add too much jitter to a plot—aim for just enough jitter so that the points are visible without impacting the overall conclusion to be drawn from the plot. We can also add a degree of transparency to the plotted data by using alpha = 0.6 within the geom_jitter() layer. The alpha = argument is available in most geoms within the ggplot2 package and allows you to set the degree of transparency between 0 (transparent) and 1 (completely solid). We add a one-to-one line (y = x) that communicates what perfect agreement between variables would look like. This is accomplished using geom_abline() (the name comes from drawing a line between points a and b on a plot). A geom_abline() call requires us to specify a slope and an intercept, which we will set to 1 and 0, respectively. Finally, we clean up the plot by: fixing the scale of the x and y axes (i.e., ensuring that 10 units of x distance are equal to 10 units of y distance) by specifying a fixed coordinate system with coord_fixed(); adding x and y axis labels using strings as arguments to xlab() and ylab() elements; and setting a theme for the plot that removes the grey background using theme_minimal(). # call to ggplot, note that data and aesthetics are called in first geom layer ggplot2::ggplot() + # first geom layer (jitter) geom_jitter(data = dplyr::filter(mpg, year == 2008), aes(x = hwy, y = cty, color = class), width = 0.4, alpha = 0.6, size = 2) + # second geom layer (line) geom_abline(intercept = 0, slope = 1) + # call fixed coordinate system coord_fixed() + # set axis limits xlim(c(0,40)) + ylim(c(0,40)) + # add axis labels xlab(&quot;Highway Fuel Economy, mi/gal&quot;) + ylab(&quot;City Fuel Economy, mi/gal&quot;) + # adopt theme without grey background theme_minimal() Figure 4.7: Scatterplot (geom_jitter) of highway vs. city fuel economy for model year 2008, colored by vehicle type 4.10 ggplot2 example 2 In the second example, we will look at highway fuel efficiency for SUVs in 2008, ordered by manufacturer and colored by the engine displacement size in liters. We create subsets of the mpg dataframe in two ways: We create a summary dataframe (mpg_subset) by applying two filter() calls on the mpg object. We then group_by() the manufacturer so that average values for highway fuel economy (hwy_mean) and engine displacement (displ_mean) can be calculated through a call to summarize(). We subset the mpg dataframe again, this time directly within the data = call for ggplot(). The first layer (geom_jitter()) is a point plot that adds a slight amount of “wobble” or “jitter” to the data points so that they don’t overlap on the plot. Here, we have called geom_jitter() to display the individual values for 2008 SUV fuel economy on the highway as a function of manufacturer. The second layer (geom_errorbar()) is a horizontal line plot showing the mean values for SUV models within each manufacturer. The geom_errorbar() function is often used to show precision (or uncertainty) about data; here we are using it to identify a single value (the mean) for each SUV manufacturer. We also add custom labels and a color scale to investigate whether engine displacement has an effect on fuel efficiency. Note the additional aesthetic calls for color = in each layer. The final part of the call in theme_classic() tells ggplot() to remove the gray background and the grid lines, which are neither necessary nor visually appealing. # use dplyr to create a summary subset from the `mpg` dataframe mpg_subset &lt;- mpg %&gt;% dplyr::filter(class == &quot;suv&quot;, year == 2008) %&gt;% dplyr::group_by(manufacturer) %&gt;% dplyr::summarize(hwy_mean = mean(hwy), displ_mean = mean(displ)) # call to ggplot, note that data and aesthetics are called in each geom layer ggplot() + # first layer - note the main dataframe was called geom_jitter(data = filter(mpg, class == &quot;suv&quot; &amp; year == 2008), aes(x = manufacturer, y = hwy, color = displ), width = 0.1, size = 2) + # second layer - note the subset dataframe was called geom_errorbar(data = mpg_subset, aes(x = manufacturer, ymin = hwy_mean, ymax = hwy_mean, color = displ_mean), alpha = 0.5, size = 1) + # customize plot labels labs(title = &quot;Fuel Economy for 2008 SUVs by Manufacturer and Engine Displacement&quot;, color = &quot;Disp (L)&quot;) + ylab(&quot;highway fuel economy (miles/gal)&quot;) + # add a fancy color scale scale_color_viridis_c(option = &quot;D&quot;, direction = -1) + # adopt a theme without gray background theme_classic() Figure 4.8: A two-layer (two geom) plot with customization What conclusions can you draw from examining Figure 4.8? In general, model year 2008 SUVs did not have great fuel economy, evidenced by both the means and the individual data points. 4.11 Store and save ggplot2 objects Sometimes, you will want to store a ggplot2 plot as an object in your global environment, so that it can be called or manipulated later. This is done in the same way as you would create and assign a name to any other object in R. But remember to use descriptive plot names following the naming advice from Chapter 3 (i.e., meaningful words; lowercase; underscore as separator). In the following example, plot1 does not follow proper naming conventions! # create a ggplot object called &quot;plot1&quot; plot1 &lt;- ggplot(data = mpg, aes(x = class)) + geom_bar() When you create and store a ggplot() object, the plot itself will be created and stored but not returned as output.If you want to “see” the plot, just enter its name into the console or script, and it will appear in the Viewer pane. You can also save ggplot2 plots as image files to a local directory using the ggsave() function. This function requires a file name but also allows you to specify parameters including image resolution (dpi = 300), image type (device = png()), and image height, width and units of measurement. # create a ggplot object called &quot;plot1&quot; plot1 &lt;- ggplot(data = mpg, aes(x = class)) + geom_bar() ggplot2::ggsave(&quot;images/mpg-bar.png&quot;, plot = plot1, dpi = 150, device = png(), width = 20, units = &quot;cm&quot;) 4.12 Getting help with ggplot2 The ggplot2 package has become so popular that most of my “how do I do this?” questions have already been asked, answered, and archived on sites like Stack Overflow. Another great source is the ggplot2 reference section on the tidyverse site. This page contains a nice, concise summary of how to call and customize plot objects. I recommend starting there because (1) it is created and maintained by the ggplot2 developers (and, thus, is authoritative) and (2) the reference page contains all the function calls in an organized list, for which you can conduct a ‘control/command F’ search. You can also print this RStudio ggplot2 cheat sheet to reference while coding. If you would like some hands-on training in ggplot2, look for tutorials or webinars like this one from Dr. Samantha Tyner, the creator and maintainer of geomnet, a ggplot2 extension. Speaking of which, the R community has created a large number of ggplot2 extensions for different data visualization needs. If you are thinking about a custom ggplot style, it probably already exists! Before building your own (which is sometimes necessary and/or fun), take a look at this compilation of ggplot2 extensions. 4.13 Chapter 4 Homework You will continue to work with the ozone measurement data introduced in the previous chapter. On Canvas, download the R Markdown template (.Rmd), which includes a description of the data, the homework questions, and the general framework of code-figure-text integration, including the framework for a code appendix. Save the data and template files in your local R Project in the /data and /homework folders, respectively. You should already have the ozone data (.csv) saved in your /data folder. Remember to check and set your working directory (e.g., Session &gt; Set Working Directory &gt; To Source File Location) to point from the R Markdown file and detect the data file in /data and also points to /figs as the place to save figures and images, which is determined in the R Markdown global option chunk I include in the R Markdown template. This homework assignment is due at the start of the class when we begin Chapter 5. We will look for the R Markdown file and the corresponding knitted PDF or HTML document within your /homework file. Remember, make regular, memorable commits, so you never lose your work. Your work will be considered late if the latest knit occurs after the deadline. "],
["eda1.html", "Chapter 5 Univariate Data Exploration 5.1 Ch. 5 Objectives 5.2 Univariate Data 5.3 Quantiles 5.4 Univariate Data Visualization 5.5 Exploring Data 5.6 Chapter 5 Homework", " Chapter 5 Univariate Data Exploration “Exploratory data analysis is graphical detective work.” - John Tukey Exploratory Data Analysis (also known as EDA) is largely a visual technique based on the human trait of pattern recognition. The purpose of EDA is simple: learn about your data by visualizing it. Why is Exploratory Data Analysis (EDA) useful? Because getting to know a dataset is a key step towards making sense of it, and EDA is a great way to familiarize oneself with a data set. EDA is fast, efficient, and intuitive. Think of the word “Exploratory” in terms of “Hypothesis-Building”. In other words, modeling and statistical inference testing should come after EDA. You can use EDA to jump-start ideas on “what do I do with my data?”! “Perfect data are boring. Flawed data, on the other hand, are interesting and mysterious. Perfect data don’t get asked out on a second date.” - John Volckens 5.1 Ch. 5 Objectives This chapter is designed around the following learning objectives. Upon completing this Chapter, you should be able to: Define and interpret the location, dispersion, and shape of univariate data distributions Calculate quantiles and basic descriptive statistics for univariate data Create a cumulative distribution plot and extract quantile information from it Create and interpret data from a histogram Create and interpret data from a boxplot Describe the differences between and the strengths/weaknesess of summary and enumerative plots Define skewness and its effect on measures of spread and central tendency in qualitative terms Create a time-series plot and identify shifts in location and dispersion Define autocorrelation, quantitatively and qualitatively Identify time lags where autocorrelation is significant, using autocorrelation and partial autocorrelation plots 5.2 Univariate Data Univariate means that “only one variable” is being considered. Just like medical doctors, statisticians enjoy the use fancy words to describe basic things. Univariate data analyses describe ways to discover features about a single variable or quantity. While simple, univariate analyses are a great starting point for EDA because they allow us to isolate a variable for inspection. The variation in diameter of a mass-produced component, pollutant concentration in the atmosphere, or the rate of a beating heart are all examples of things we might examine in a univariate sense. This is, of course, a potentially risky procedure because, as engineers, we are taught about mechanisms and dependencies that imply that variable A is inextricably linked to variable B through some physical process. That’s OK to admit. Univariate analyses are still useful. Trust me for now, or skip ahead to multivariate analyses or modeling—your choice. 5.2.1 Location, Dispersion, and Shape Univariate EDA often begins with an attempt to discover three important properties about an observed variable: its location, dispersion, and shape. Once you define these properties (the “what”), you can begin to probe the underlying causes for these properties (the “how” and “why”). Discovering “how” and “why” of a variable’s location, dispersion, and shape might sound simple, and yet, answering such questions often represents the pinnacle of scientific discovery; they give out Nobel Prizes for that stuff! Let’s start with “what” (location, dispersion, and shape) and build from there. 5.2.1.1 Location The location of univariate data means: where do the values or observations fall? Do the values tend to be large or small, based on what you know about the variable? Do the observations have a central tendency - a region where values are more likely to show up? 5.2.1.2 Dispersion The dispersion of the data refers to its variability. Are the values tightly bound in a small range, or do they vary widely from one observation to the next? Note that the phrase “varies widely” is contextual. The variation in the cost of an ice cream cone from one location to the next might look small to you, but could mean the difference between joy and sorrow for a 10-year-old with only $1.50 in their pocket. 5.2.1.3 Shape The shape of the distribution is actually a combination of location and dispersion but with some mathematical nuance. Knowing the shape of your data distribution means that you have insight into its probability density function. Once you know a distribution’s shape, you can model it. And if you can model the distribution, you can begin to make inferences about it (e.g., extrapolations, predictions). More on this later in Modeling. The shape of a distribution of data is often categorized based on whether it follows that of a reference distribution, of which there are many types. You can think of reference distributions like species of living organisms; there are lots out there, but once you categorize one, you can likely predict its behavior. In other words, if the shape of your data matches a reference distribution, most of your modeling work is already done! Examples of reference distributions include the uniform distribution, normal distribution, and the lognormal distribution. More on different types of reference distributions here. 5.2.1.3.1 Example: Location and Dispersion Let’s plan a camping trip. Our trip is purely hypothetical, so let’s not worry about costs, logistics, or other important factors. For this exercise, we only care about comfort while outdoors. When I think of being comfortable outdoors, the first thing that comes to mind is temperature: Did I bring the proper clothing? We will consider going camping in two lovely spots: the forest preserves along the Na Pali coast in Kauai, Hawaii or the sunny hiking and climbing region around Jack’s Canyon in southwest Colorado. Let’s examine the location and dispersion of hourly temperatures in these two regions for the month of July, 2010. We can access this information from the NOAA Climate Data Center that provides weather data for these two regions. Once we know the location and dispersion of these data, we can decide what clothes to bring. In Figure 5.1, I use geom_jitter() to plot the hourly temperatures measured at these two regions for July 2010. For this plot, we define three aesthetics: y = location x = temperature color = location (not necessary for the plot, but the color between variables can be helpful) Figure 5.1: Hourly temperature levels in Colorado and Hawaii for the month of July, 2010. First, if we are picking spots to go camping, both locations have daily average temperatures that seem very pleasant. The average is about 78.9 \\(^\\circ\\)F in Colorado and 78.6 \\(^\\circ\\)F in Hawaii. When we calculate an average value, we are creating an indicator of a variable’s location (in this case the central tendency). But if we look at the dispersion of temperature observations (i.e., how the temperatures vary across the month), we can make an important distinction: the range of observed temperature values in Hawaii is fairly narrow, whereas the range in Colorado spans from 64.4 to 91.8 \\(^\\circ\\)F! The conclusion to be drawn here is that the central tendencies of hourly temperature are nearly identical between Hawaii and Colorado, but the dispersion of the temperature data suggests that you might want to pack more layers to be comfortable in Colorado. See Figure 5.2. You will discover that there are many ways to communicate location (e.g., mode(), mean(), median()) and dispersion (e.g., range(), interquartile range IQR(), and standard deviation sd()). We will discuss several such descriptors in this course. Figure 5.2: The temperature data from Hawaii and Colorado have similar locations (central tendencies) but differing dispersion (spread). 5.3 Quantiles Let’s assume you have a sample of univariate data. A good starting point for exploring these data is to break them into quantiles and extract some basic information. Quantiles allow you to see things like the start, middle, and end rather quickly because the first step of quantile calculation is to sort your data from smallest to largest value. A “quantile” represents a fractional portion of an ordered, univariate distribution. Quantiles break a set of observations into similarly sized “chunks”, wherein each chunk represents an equal fraction of the total distribution, from start to finish. If you line your data up from smallest to largest and then slice that list into sections, you have created a set of quantiles. For example, when a distribution is broken into 10 equal chunks, or deciles, the first quantile (the 10th% or the 0.1 fraction) represents the value that bounds the lower 10% of the observed data; the second quantile (0.2 fraction) represents the 20th% value for the observed data, and so on. There are two important aspects about quantiles to remember: (1) each quantile is defined only by its upper-end value; and (2) quantiles are defined after the data have been rank-ordered from lowest to highest value. Quantiles are often used to communicate descriptive statistics for univariate data. The two most extreme quantiles define the range: Minimum,min(): the 0% or lowest value; the zeroth quantile Maximum,max(): the 100th percentile (or 1.0 in fractional terms); the highest value observed; the nth quantile If you break a distribution into quarters, you have created quartiles. We calculate quantiles using stats::quantile(), which comes loaded with base R. This function takes a numeric vector x = as a formal argument and, by default, returns quartiles (including min and max) for the data. If you want to extract specific quantile values, you can specify them with the optional probs = argument (default: probs = seq(from = 0, to = 1, by = 0.25)). Let’s generate a sample of random numbers between 0 and 100 and break the resulting data into quartiles. A random number generator, using the R function runif(), will create a uniform distribution across the sample range provided because all values within that range have equal probability of being chosen. With that in mind, you can probably guess what the quartiles will look like, given a sufficiently large sample… # &quot;set seed&quot; to generate the same results each time set.seed(1) # create uniform distribution univar &lt;- runif(n = 1000, min = 0, max = 100) # split uniform distribution into quantiles univar_quart &lt;- quantile(univar, probs = seq(0, 1, 0.25)) %&gt;% round(1) Figure 5.3: Quartiles estimated from n=1000 samples of a uniform distribution between 0 and 100. As expected, the 1st, 2nd, 3rd, and 4th quartiles from a uniform distribution between 0 and 100 fall into predictable chunks of approximately 25, 50, 75, and 100. The first, or lower, quartile contains the lower 25% of the distribution. In quantile terms, we define this quartile by its upper value, which occurs at the 25th percentile (or the 0.25 quantile). The second quartile contains the 25th to the 50th percentile values of the data. But, remember, that we define this quartile at the upper end of that range: the 0.5 quantile. Note the 0.5 quantile also represents the median of the distribution. The third quartile contains data for the 50th to the 75th percentile values (0.5 to 0.75 quantiles). The fourth quartile contains the upper 25% of the distribution: 0.75 to 1.0 quantiles. 5.3.1 Descriptive Statistics Quantiles allow us to calculate several important descriptive statistics for univariate data. For example, quartile calculations for highway fuel effeciencies from the mpg dataset allows us to report the following descriptive statistics: Table 5.1: Quantiles and descriptive statistics for hwy fuel efficiency from ggplot::mpg$hwy Quantile Descriptor Example Values 0 minimum 12 1 maximum 44 0.5 median 24 0.25 25th% 18 0.75 75th% 27 0.75 - 0.25 IQR 9 A couple notes: You might wonder why I didn’t include summary statistics like the mean() or standard deviation, sd(), in this list. I like using these descriptors, but their use implies that one knows the type or “shape” of the distribution being described (e.g., normal, log-normal, bi-modal). Use of descriptors like mean and standard deviation are very useful when applied properly but can be misleading when the distribution is skewed (more on that later). Quantile descriptors, on the other hand, are agnostic to the type of distribution they describe. For example, the median is ALWAYS at the 0.5 quantile, or 50th percentile, regardless of the shape of the distribution. The last descriptor in Table 5.1 is the interquartile range, or IQR for short. The IQR describes the spread of the data and communicates the range of values needed to go from the 25th% to the 75th% of the distribution. The IQR spans the “middle part” of the distribution. The IQR is similar in concept to a standard deviation but makes no assumptions about the type or shape of the distribution in question. You can calculate an IQR by subtracting the 0.75 quantile from the 0.25 quantile, or by using stats::IQR(). Now let’s create quantiles from a normal distribution of data with a mean of 50 and standard deviation of 15. We’ll start by randomly sampling 1000 values, using rnorm(), and then arranging them with the quanitle() function. Note that the second half of the code chunk below shows how to calculate quantiles manually. # &quot;set seed&quot; to generate the same results each time set.seed(2) # create normal distribution norm_dist &lt;- rnorm(1000, mean = 50, sd = 15) # split normal distribution into quantiles and output tabular result quantile(norm_dist, probs = seq(0, 1, 0.1)) %&gt;% round(0) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 9 32 38 43 46 51 54 59 64 71 95 # manual method for calculating quantiles; start with norms_dist values norm_manual &lt;- tibble::tibble( sample_data = norm_dist) %&gt;% # sort the data using dplyr::arrange(sample_data) %&gt;% # calculate cumulative fraction for each entry dplyr::mutate(cum_frac = seq.int(from = 1 / length(norm_dist), to = 1, by = 1 / length(norm_dist))) # create a quantile sequence corresponding to deciles deciles &lt;- seq.int(from = 0, to = 1, by = 0.1) # extract quantiles that match the decile values created above normal_data_deciles &lt;- norm_manual %&gt;% dplyr::filter(cum_frac %in% deciles) # %in% means: return whatever &quot;matches&quot; between these two vectors Normal distributions are more complicated than uniform distributions, and it’s hard to get a lot out of the decile summary when shown in tabular format. This brings us to my favorite part of exploratory data analysis: data visualization. 5.4 Univariate Data Visualization Data visualization is a powerful technique for exploring data. Visualizing univariate data is the first step in getting to know your data, once they have been imported and wrangled. Below, I introduce formal approaches to visualizing the location, dispersion, and shape of univariate data. Later, we will explore relationships between two or more variables in multivariate data analyses. The techniques shown below are simple, powerful, and often underutilized. 5.4.1 Summary vs. Enumerative Plots Univariate data visualizations can be broken down into two categories: summary plots and enumerative plots. A summary plot shows you the data summarized in some way; a data analysis or manipulation happens prior generating a summary plot. Enumerative is a fancy word meaning “to name or show” all of the data without modification. Enumerative plots visualize all the data in a single plot. There are strengths and weaknesses to both: Summary plots Strengths: Easy to read; easy to see the “main points”. Weaknesses: Don’t show all of the data; only show you a summary. You are more likely to miss nuances or special features when looking at a summary plot. Enumerative plots Strengths: Show all of the data without manipulation. This is considered a strength because many analysts like to see raw data; “the data don’t lie!” Weaknesses: Can take more time and energy to understand. Sometimes more data mean more work for your brain. 5.4.2 Cumulative Distribution Plot A cumulative distribution plot is an enumerative plot that lays out the raw quantiles, typically on the y-axis, against the observed data on the x-axis. Because these plots show all the data, each observation gets assigned a unit quantile; if there are 100 data points in a sample, each datum represents 1% of the total, or a quantile of 0.01 . Thus, quantiles are shown on the y-axis, and the x-axis shows the observed data, ordered from smallest to largest value. The cumulative distribution plot shows the rolling fraction (or percent) of data on the y-axis that are “less than or equal to” the x-axis values of that data. An annotated cumulative distribution plot is shown below for a sample of temperature data that follow a normal distribution. For this plot, I calculated the unit_quantiles manually to show you how this can be done: # randomly sample two variables from normal distributions with `rnorm()` normal_data &lt;- tibble::tibble(a = rnorm(n=1000, mean = 15, sd = 5), b = rnorm(n=1000, mean = 10, sd = 3)) # create unit quantile values for the `normal_data` normal_data_cdf &lt;- normal_data %&gt;% dplyr::select(a) %&gt;% dplyr::arrange(a) %&gt;% dplyr::mutate(unit_quantiles = seq.int(from = 1/length(a), to = 1, by = 1/length(a))) Figure 5.4: Annotated Cumulative Distribution Plot for Normally-Distributed Temperature Data As you can see, there is a lot of information contained in a cumulative distribution plot! We can see the range (minimum to maximum), median (0.5 quantile), and any other percentile values for the distribution. We can quickly pick out any value and find out what fraction of the data are below it. For example, picking out the quantile of 0.9 on the y-axis shows that 90% of the observed temperature values are below 21 \\(^\\circ\\)C. In fact, most of the basic descriptive statistics can be accessed on a cumulative distribution plot: range: minimum and maximum values at quantile 0 and 1.0, respectively median: x-axis value at quantile 0.5 25th and 75th percentiles (or quartiles): x-axis values at quantile 0.25 and 0.75, respectively interquartile range (IQR): x-axis distance between quantiles 0.25 and 0.75 Cumulative distribution plots also reveal details about the dispersion of the observed data, and, more subtly, about the nature or shape of underlying data distribution. For example, in the temperature example above (Figure 5.4), the curve has geometric symmetry about the median; this symmetry indicates that the dispersion of the data is equal as you move away from the median in either direction. 5.4.2.1 Example: Cumulative Distribution Plot The ggplot2 package can create cumulative distribution plots with different geom options: geom_step(aes(x = your_univariate_data), stat = \"ecdf\") stat_ecdf(aes(x = your_univariate_data)) In the example below, we create a cumulative distribution plot of annual salaries reported by individuals with a degree in Mechanical Engineering in the United States. These data come from the NSF Survey of College Graduates. I’ve also taken the liberty of identifying the minimum, maximum, and quartile values on the plot. # create cumulative fractions ordered_salaries &lt;- raw_salaries %&gt;% dplyr::select(salary) %&gt;% dplyr::filter(salary &lt; 500000) %&gt;% # to remove numeric identifiers dplyr::arrange(salary) %&gt;% dplyr::mutate(cum_pct = seq.int(from = 1/length(salary), to = 1, by = 1/length(salary))) # alternate method if you don&#39;t want to calculate cumulative fractions ordered_salaries %&gt;% ggplot2::ggplot(mapping = aes(x = salary)) + geom_step(stat = &quot;ecdf&quot;) + # &quot;empirical cumulative distribution function&quot; labs(x = &quot;Salaries of ME Graduates&quot;, y = &quot;Cumulative Fraction&quot;) + scale_y_continuous(limits = c(-0.05, 1.03), expand = c(0,0), breaks = seq(from = 0, to = 1, by = 0.1)) + scale_x_continuous(labels = scales::label_dollar(scale = 0.001, prefix = &#39;$&#39;, suffix = &#39;k&#39;), minor_breaks = seq(from = 0, to = 450000, by = 10000))+ geom_segment(data = data.frame(x = quantile(ordered_salaries$salary), y = rep.int(-.05, 5), xend = quantile(ordered_salaries$salary), yend = seq(from = 0, to = 1, by = 0.25)), aes(x = x, y = y, xend = xend, yend = yend), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_bw() ggsave(&quot;./images/cdf_me_salaries.png&quot;, dpi = 150) Figure 5.5: Annual Salaries (2017) for US Graduates (all ages) with a Mechanical Engineering Degree The good news is that if you can survive the undergraduate grind in Mechanical Engineering, you have a reasonable chance of making a great living wage. The median salary is $ 96,000. Note this is for graduates of all ages across the country in the year 2017. Don’t expect that kind of paycheck on your first day on the job! Look at the “tails” of the curve in Figure 5.5; the shape of the curves on either side of the median are not symmetrical. One tail looks longer than the other. This lack of symmetry means that the data are skewed. Why is that? We will explore skewed data at several points in this course and discuss some of the mechanisms that cause data to be skewed. 5.4.3 Histogram A histogram is a summary plot of counts, or frequency of observations, as a function of magnitude, or levels. Histograms are useful because they allow you to visualize the spread and shape of your data as a distribution. A histogram can tell you several important things about a variable: The location or central tendency: what’s the most common value? The approximate range of the data: what are the maximum and minimum values? The dispersion: how spread out are the data? The nature of the distribution: do the data appear normally distributed or skewed? The presence or absence of outliers in the data: what are those observations doing way out there? Below is a basic histogram that I’ve annotated to show some key features. This histogram was created by sampling a normal distribution rnorm(). I created the plot using geom_histogram(), which requires only an x = aesthetic to run. Examination of this histogram quickly reveals a lot about the underlying data. For example, the central tendency is located around a value of ~30 and the majority of the data appear to fall between a range of 20 - 40. Figure 5.6: A histogram is a plot of counts, or frequency of observations, as a function of magnitude, or levels, of univariate data. 5.4.3.1 What to look for in a histogram? In addition to being able to “see” key data descriptors such as mean, range, and spread, a histogram also allows one (a) to get a feel for skewness, that is whether the distribution is symmetric about the central tendency, (b) to see outliers, and (c) to visualize other potential interesting features in the dataset. Figure 5.7: A histogram depicting a skewed distribution. Figure 5.7 depicts just such a skewed distribution. Examination of Figure 5.7 also reveals the presence of potential outliers in the data: the apparent spike in observations around x = 200. Outliers are interesting features and should neither be ignored nor deleted outright. Outliers exist for some reason, and usually only through detective work or mechanistic knowledge can you elucidate their source. We will delve into managing outliers in Chapter 9; for now, it’s enough to know that they exist and may warrant attention. In Figure 5.7, the central tendency is evident, but the spread of the data is not symmetrical about that central tendency. One outcome of skewed data is that traditional location measures of central tendency such as mode, median, and mean do not agree with each other. According to Figure 5.7, the central tendency calculations differ based on type of location measure. Figure 5.8: Describing the spread of skewed data with a “standard deviation” can lead to confusion and trouble: in this case, the existence of physically impossible values. Another, potentially more troubling, outcome of skewed data is that certain measures of the spread of the data such as the standard deviation can lead to misleading conclusions. For example, if we calculate and extend “one standard deviation” to the mean of the data in Figure 5.8, this would imply the existence of negative values (87 - 113 = -26), which is often impossible in the case of physical measurements. This is a common mistake in data analysis: the application of “normal” descriptors like mean and standard deviation to “non-normal” data. Skewed distributions can be more challenging to handle because they are less predictable than normal distributions. That said, statisticians have figured out how to model just about any type of distribution, so don’t fret. In any case, we can always rely on quantile calculations for descriptors such as median and IQR because those descriptors do not make assumptions about the shape of the distribution. 5.4.3.2 What causes skewed histograms? I want to spend some time thinking about mechanisms that cause variability in an observation. These mechanisms are often responsible for whether a set of observations appears normally distributed or skewed. We will focus on two types of distributions for now: normal and log-normal data. Many other types of data distributions exist in the real world. A general “rule of thumb” is that additive variability tends to produce normally distributed data, whereas mechanisms that cause multiplicative variability tend to produce skewed data—often log-normal, as seen in Figure 5.8). By “additive”, I mean that variable x tends to vary in a “plus/minus” sense about its central tendency. Examples of additive variability include the distribution of heights measured in a sample of 3rd graders or the variability in the mass of a 3D-printed part produced 100 times by the same machine. Multiplicative (or log-normal) variability arises when the mechanism(s) controlling the variation of x are multipliers (or divisors) of x. Many real-world phenomena create multiplicative variability in observed data: the strength of a WiFi signal at different locations within a building, the magnitude of earthquakes measured at a given position on the Earth’s surface, the size of rocks found in a section of a riverbed, or the size of particles found in air. All of these phenomena tend to be governed by multiplicative factors. In other words, all of these observations are controlled by mechanisms that suggest \\(x = a * b * c\\) not \\(x = a\\cdot b\\cdot c\\). Explicit examples of multiplicative variability and how it leads to log-normal data are provided here. 5.4.3.3 Probability Density Function The histogram is really an attempt to visualize the probability density function (pdf) for a distribution of univariate data. The pdf is a mathematical representation of the likelihood of sampling a given value from the distribution, assuming a random draw. Unlike the histogram (which bins data into groups), the pdf is a continuous function that can be solved at any datum. A pdf can be expressed empirically (as a numeric approximation) or as an exact analytic equation (in the case normal, lognormal, and other known distributional forms). 5.4.4 Boxplot The boxplot is summary plot that allows one to view a distribution by its quartiles (Figure 5.9). The “box” in a boxplot consists of three parallel lines, usually oriented vertically, connected on their sides to form a box. The outer box lines, also called the “hinges”, represent the 0.25 and 0.75 quantiles, and the inner line represents the 0.5 quantile, or median. Thus, the height of the box depicts the inter-quartile range of the data. Boxplots are often drawn with lines extending out from each end of the box called “whiskers”. These whisker lines vary in their representation. For some plots, the whiskers represent either the maximum and minimum values, respectively, or a distance of \\(1.5\\cdot IQR\\) if either the maximum or minimum value are larger than \\(1.5\\cdot IQR\\). Other times the whiskers represent the 0.9 or 0.95 quantiles or \\(\\frac{1.58\\cdot IQR}{\\sqrt(2)}\\). Because of these variations, it’s best to define the whiskers for the viewer either in the text or the Figure caption. Here is the R definition from the help file when you type ?geom_boxplot into the console: The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called “outlying” points and are plotted individually. Figure 5.9: A generic boxplot. The box lines represent the lower, middle, and upper quartiles, respectively. The whiskers represent 1.5*IQR in each direction. Boxplots can be created for a single univariate distribution, as shown in Figure 5.9, but the strength of the boxplot really comes from drawing several together, so that one univariate distribution can be compared to another. In my opinion, single boxplots are quite boring by themselves, and the same information presented by a boxplot is just as easy to get from a cumulative distribution plot. Nonetheless, I think it’s faster to view quartiles and the IQR on the boxplot, so a plot with multiple boxplots has advantages. For example, let’s look at the data from Figure 5.1 as two boxplots. Figure 5.10: Boxplots of hourly temperatures in CO and HI for July, 2010. Whiskers represent 1.5*IQR. Sometimes, it’s helpful to combine enumerative and summary plots together. For example, we can add a Boxplot layer to Figure 5.1 so that we can visualize quartiles in addition to the location, dispersion, and shape of the raw data. The ability to layer geoms is one of the many strengths of the ggplot2 package. ggplot(data = noaa_temp, mapping = aes(x = temp_hr_f, y = location, fill = location)) + geom_boxplot() + geom_jitter(height = 0.25, alpha = 0.2, shape = &quot;circle filled&quot;, color = &quot;black&quot;) + labs(y = &quot;Region&quot;, x = &quot;Temperature (°F)&quot;) + theme_bw() Figure 5.11: Boxplots of hourly temperatures in CO and HI for July, 2010 5.4.5 Time-Series Plot The time-series plot (also called a “run sequence” plot) is something you have undoubtedly seen before. This plot, typically shown as an enumerative plot, depicts the value of a variable on the y-axis plotted in the sequence that the observations were made on the x-axis, often shown as units of time. A time-series plot allows you to do several things: Identify shifts in “location” of the variable: when and by how much do values change over time? Identify shifts in “variation” of the variable: does the data get more/less noisy over time? Identify (potential) outliers and when they occurred Identify the timing of events! Let’s plot the same temperature data from Hawaii as shown in Figure 5.1 as a time series. We will use geom_point() to add the data and geom_line() to connect the dots for this plot. Figure 5.12: Time series of hourly temperature measurements in Kauai, Hawaii for July 2010. What can we see in these data? Here are some quick conclusions: The temperature appears to rise and fall in a predictable pattern each day—duh! Over the course of the month, we can see a gradual increase in daily mean temperature. Neither of these conclusions are groundbreaking, but they both allude to an important outcome: there are patterns to be seen in these data! None of those patterns were evident in the cumulative distribution or histogram plots. If we zoom in on just the first 5 days of the month, we see the daily temperature patterns in more detail. Figure 5.13: Time series of hourly temperature measurements in Kauai, Hawaii for July 1-5, 2010. 5.4.6 Autocorrelation Plot Autocorrelation, or serial correlation, means correlated with oneself across time. Autocorrelation is a concept that suggests “whatever is happening in this moment is still likely to be happening in the next moment.” Lots of things are autocorrelated: your mood right now is likely related to your mood 15 minutes from now (or 15 minutes ago). The weather outside at 08:00 on a given day is a good predictor of the weather outside an hour later at 09:00, or two hours later at 10:00, and so on, but less so in fourteen days hence. Almost all physical phenomena have a degree of autocorrelation and that’s important because if a datapoint at time \\(i\\) is correlated with time \\(i + 1\\), then those two datapoints are NOT independent. Yet, many statistical tests assume that the underlying data ARE independent from one another! Thus, autocorrelation can pose a problem when we want to make inferences using data collected over a span of time. Fortunately, we have ways of detecting the presence and relative strength of autocorrelation with a simple exploratory plot. The autocorrelation plot is a summary plot that gives the Pearson correlation coefficient (r) between all measurements (xi) and their lags (xi+n), where x represents an observation and i represents a point in time. You can learn more about correlation coefficients (and how to calculate them) from the cor() function: type ?cor into the console. An autocorrelation plot can help answer the following questions: Are the data correlated with each other over time? Note: if the answer is yes, then your data are not necessarily independent measures. Are there patterns (periodicity) to discover? hourly? daily? weekly? seasonally? Let’s examine the Hawaii temperature data shown in Figures 5.1 and 5.12 using an autocorrelation plot. The ggplot2 package doesn’t have explicit geoms for autocorrelation plots (sigh), but there are simple alternatives. The acf() function from the stats package (base R) The ggAcf() function in the forecast package (requires a time-series object ts) Both of these functions calculate autocorrelation lags and plot them through time, wherein each unit of time represents the duration of a single lag. Here, the y-axis represents the Pearson correlation coefficient and the x-axis represents time lags for the basic unit of time in which the data are arranged. # subset data to contain only measures from Hawaii noaa_temp_hi &lt;- noaa_temp %&gt;% dplyr::filter(location == &quot;Hawaii&quot;) # call the autocorrelation plot from base R `stats` package stats::acf(noaa_temp_hi$temp_hr_f, main = &quot; &quot; , xlab = &quot;Lag (hours)&quot;, ylab = &quot;Correlation coefficient&quot;) The plot suggests a fairly strong level of autocorrelation in these temperature data. We should not be surprised, especially after seeing the time series shown in Figures 5.12 and 5.13. Can you explain why the Pearson correlation coefficient is negative at a 12-hr lag? Why does the coefficient trend back upwards to nearly perfect correlation at 24-hr lags? Other notes about autocorrelation plots: Bars above the blue horizontal lines indicate that autocorrelation is worth attention. One important implication of autocorrelated data is that the data wihtin a given lag duration are not independent. This lack of independence violates the assumptions of many modeling approaches. This is potentially bad. 5.4.7 Partial Autocorrelation Plot The partial autocorrelation plot is a summary plot that gives the correlation coefficient (r) between all timepoints (xi) and their lags (xi+n) after accounting for autocorrelation for all previous lags. The partial autocorrelation plot can help you decide: how far out in time should I go in order to assume that measurement (xi) is independent from (xi+n)? # create partial autocorrelation plot stats::pacf(noaa_temp_hi$temp_hr_f, main = &quot; &quot;, xlab = &quot;Lag (hours)&quot;, ylab = &quot;Partial Correlation Coefficient&quot;) In this case, after a 2-hr lag, the data loses most autocorrelation, except for some periodicity around 12- and 24-hr lags. 5.5 Exploring Data The whole point of this chapter comes down to this: when you have new, unfamiliar data, the best thing to do is to explore that data graphically. Create a panel of plots from available candidates: histogram, cumulative distribution, time-series, boxplot, autocorrelation. Together, these plots allow you to see basic features and extract important descriptors such as central tendency, spread, shape, and quantiles. EDA plots also frequently lead to more questions; the answers to which will bring you even closer to understanding what the data are trying to tell you. Let your eyes do the listening at first. For example, here is a “3-plot” for the ME Salary data originally shown in Figure 5.5: Figure 5.14: EDA Plots of Annual Salaries among Mech Eng Graduates 5.6 Chapter 5 Homework You will continue to work with the ozone measurement data introduced in the previous chapters. Download the R Markdown template from Canvas. This homework assignment is due at the start of the class when we begin Chapter 6. Your work will be considered late if the latest knit occurs after the deadline. "],
["rprog3.html", "Chapter 6 Strings, Dates, and Tidying 6.1 Chapter 6 Objectives 6.2 Strings 6.3 Dates and Date-times 6.4 lubridate 6.5 Tidy Data", " Chapter 6 Strings, Dates, and Tidying 6.1 Chapter 6 Objectives This Chapter is designed around the following learning objectives. Upon completing this Chapter, you should be able to: Define the meaning of “strings” and “date time” objects in R Manipulate character strings using the stringr and tidyr packages of functions Parse strings using regular expressions (“regex”) Describe how R stores a POSIXct date and time object internally Convert a character vector to a date format using functions from the lubridate R package Extract information from a date object (e.g., month, year, day of week) using lubridate functions Search, organize, and visualize data that are linked to date objects Apply functions from the dplyr and tidyr packages to make dataframes “tidy” 6.2 Strings A string is a character variable like “John”, or “blue”, or “John’s sample 8021A turned blue”. String variables are defined in R using quotes \" \" and stored as a character class; they often show up in data analysis in one of two ways: As metadata. Metadata means: “data that describe other data”. A readme.txt file is metadata; notes and code comments are metadata. All of these types of data usually come in the form of strings and are included with the data your are analyzing but not in the dataset itself. As vectorized data. In R programming, “vectorized” means: stored as a column of data. Examples of vectorized string variables you might find include things like: “participant names”, or “survey responses to question 1”, or “mode of failure”. The examples below show how string variables are created in R. # examples of vectorized string data names_respond &lt;- c(&quot;Ahmed&quot;, &quot;Josh&quot;, &quot;Mateo&quot;, &quot;William&quot;, &quot;Ali&quot;, &quot;Wei&quot;, &quot;Steve-O&quot;, &quot;John&quot;) q1_responses &lt;- c(&quot;Because you told me to do it.&quot;, &quot;It seemed like the right thing to do at the time.&quot;, &quot;Because I had been over-served.&quot;, &quot;I don&#39;t know. I just did it.&quot;, &quot;I got caught up in the heat of the moment.&quot;, &quot;I was given an opportunity. I took my shot.&quot;, &quot;I plead the 5th.&quot;, &quot;I could ask you the same question.&quot;) failure_mode &lt;- c(&quot;fracture&quot;, &quot;yielding&quot;, &quot;deflection&quot;, &quot;fatigue&quot;, &quot;creep&quot;) # proof of vector class class(names_respond) ## [1] &quot;character&quot; The first step in analyzing a string is to parse it. To parse means to examine the individual components. For example, when you read this sentence you parse out the words and then assign meaning to those words based on your memory, your understanding of grammar, and the context in which those words occur. Context is often critical to understanding because the meaning of words can change from one context to the next (i.e., whether you are reading an instruction manual, a text message, a novel, or a warrant for your arrest). Strings can be challenging to analyze because computers are built on logical operations and mathematics; strings are neither of those. Computers have fantastic memory, are OK at grammar, and are comically poor at contextualization. Taken together, this means that strings can be challenging (but not impossible) to analyze using computers. Are you active on social media platforms like Instagram or Twitter? You can bet that a computer program has downloaded and parsed all of your posts, each one as a string. You can learn a lot about a person (and their buying habits) from what they post online! In this chapter, we will introduce a few simple string functions from base R and the stringr package. We will also introduce the concept of regular expressions as a means to perform more advanced string manipulation. 6.2.1 String detect, match, subset One of the simplest string operations is to search whether a string contains a pattern of interest. The stringr package (part of the Tidyverse) was developed to simplify the analysis of strings. Most of the functions in stringr begin with str_ and end with a specific function name. A full list of functions is provided here. Some examples: str_detect() returns a vector of logical values (TRUE/FALSE) indicating whether the pattern was detected within each string searched. The function takes two arguments, the string to be searched and the pattern for which to search. Let’s search for the pattern \"Josh\" in the character vector of strings, names_respond, that we created above: stringr::str_detect(string = names_respond, pattern = &quot;Josh&quot;) ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE As expected, only one string in the vector produced a match. An added benefit of logical functions like str_detect() is that return values of TRUE are coded as 1 and FALSE as 0. Thus, if we sum() the result of the str_detect() search, we will get the cumulative number of matches to \"Josh\" from within our data. stringr::str_detect(string = names_respond, pattern = &quot;Josh&quot;) %&gt;% sum() ## [1] 1 In other words, logical functions like str_detect() allow us to do math on string data! For example, we can now calculate the proportion of \"Josh\" entries within our sample: stringr::str_detect(string = names_respond, pattern = &quot;Josh&quot;) %&gt;% sum() / length(names_respond) ## [1] 0.125 str_extract() takes the same arguments as str_detect() but returns a vector of the matched values (by string index). By “matched values”, I mean only the portion of the string for which the search created a match. stringr::str_extract(string = names_respond, pattern = &quot;Jo&quot;) ## [1] NA &quot;Jo&quot; NA NA NA NA NA &quot;Jo&quot; str_subset() returns only the entries that were matched (i.e., if a match was detected, then the entire string that was matched is returned). If we subset our short list of names to the pattern of letters \"li\", we get: stringr::str_subset(string = names_respond, pattern = &quot;li&quot;) ## [1] &quot;William&quot; &quot;Ali&quot; To note, there are base R versions of all these stringr functions. Most are performed with the grep family of functions. The term “grep” is an acronym for Global Regular Expression Pattern (more on regular expressions below). Many “old-school” coders use this family of functions, therefore, you will encounter them in the wild, so it’s worth knowing about them. stringr_funcs base_funcs str_detect(x, pattern) grepl(pattern, x) str_match(x, pattern) regexec(pattern, x) + regmatches() str_subset(x, pattern) grep(pattern, x, value = TRUE) 6.2.2 Regular Expressions Before going much farther, we should spend some time discussing regular expressions or regex for short. When we pass a pattern argument to a function like str_detect(), the function treats that argument like a “regex”. Up until this point, I have only passed simple character strings as pattern arguments (i.e., pattern = \"Josh\"). In reality, we can create much more advanced search criteria using regex syntax within our search patterns. A regular expression is a sequence of characters that define a search pattern to be implemented on a string. In the R programming language, regular expressions follow the POSIX 1003.2 standard; regex can have different syntax based on the underlying standard. Regex are created by including search syntax (i.e., symbols that communicate search parameters) within your quoted string. For example, square brackets [] in a search pattern indicate a search for any of the characters within the brackets; conversely, to match all the characters you simply include them in quotes. A key strength of regex patterns is that they allow you to use logical and conditional relations. For example, the following text search patterns can be coded as regex: Desired Search Pattern Regex in R any letter followed by the numbers 3, 4, or 5 “[:alpha:][345]” strings that start with ‘ID’ and are followed by 4 numbers “^ID[:digit:]{4}” One challenging aspect of string searching in R, however, is that certain “special characters” like the quote \" and the backslash \\ symbol must be explicitly identified within the string in order to be interpreted by R correctly. To identify these special characters in a string, you need to “escape” that character using a backslash \\. Thus, if you want to search for a quote symbol, you would type in \\\". Whenever a regex requires the use of a \\, you have to identify it within a string as \\\\. The table below shows some basic regex syntax and how they would be implemented as a search pattern in R. Table 6.1: Basic Regex Search Syntax and Example Implementation in R Regex syntax String to be matched Example in R \\d Any numeric digit “\\\\d” or “[:digit:]” [abc] matches a, b, or c “[abc]” [a-z] matches every character between a and z “[a-z]” [^abc] matches anything except a, b, or c “[^abc]” (abc) creates a “capture group” whereby abc must occur together “(abc)” ^b starts with: look for “b” at the start of a string “^b” $b ends with: look for “b” at the end of a string “$b” a|b match a or b “a|b” Regex sequences have seemingly no end of sophistication and nuance; you could spend dozens of hours learning to use them and hundreds more learning to master them. We will only introduce basic concepts here. More in-depth introductions to regex syntax and usage can be found on Hadley Wickham’s R course, on the stringr cheatsheet developed by RStudio, and through practice with, my personal favorite, a game of Regex Golf. 6.2.3 String split, replace str_split() will split a string into two (or more) pieces when a match is detected. The string will always be split at the first match and again at each additional match location, unless you specify that only a finite number of n matches should occur. A couple points to note: str_split() splits on each side of the match, which itself is not included in the output. because both sides of the split string are returned, your output will take the form of a list. stringr::str_split(string = names_respond, pattern = &quot;t&quot;) ## [[1]] ## [1] &quot;Ahmed&quot; ## ## [[2]] ## [1] &quot;Josh&quot; ## ## [[3]] ## [1] &quot;Ma&quot; &quot;eo&quot; ## ## [[4]] ## [1] &quot;William&quot; ## ## [[5]] ## [1] &quot;Ali&quot; ## ## [[6]] ## [1] &quot;Wei&quot; ## ## [[7]] ## [1] &quot;S&quot; &quot;eve-O&quot; ## ## [[8]] ## [1] &quot;John&quot; str_replace() searches for a match and then replaces the matched value with a new string of your choosing. The function takes three arguments: the string to be searched, the pattern to match, and the replacement string to be inserted. Let’s replace the first period detected in each of the q1_responses strings with a question mark. Both . and ? are special characters so we need to “escape” each of these symbols with two back-slashes \\\\. stringr::str_replace(string = q1_responses, pattern = &quot;\\\\.&quot;, replacement = &quot;\\\\?&quot;) ## [1] &quot;Because you told me to do it?&quot; ## [2] &quot;It seemed like the right thing to do at the time?&quot; ## [3] &quot;Because I had been over-served?&quot; ## [4] &quot;I don&#39;t know? I just did it.&quot; ## [5] &quot;I got caught up in the heat of the moment?&quot; ## [6] &quot;I was given an opportunity? I took my shot.&quot; ## [7] &quot;I plead the 5th?&quot; ## [8] &quot;I could ask you the same question?&quot; The str_replace() family of functions is useful for cleaning up misspellings (or other unwanted language) in strings. Note, however, that str_replace() will normally replace only one instance of a match; use str_replace_all() if you plan to encounter multiple matches that need replacing. The pattern = argument for matching can be a single string or a vector of strings. In the latter case, you can define a vector of keywords that you might be searching for across sentences. In that case, use pattern = c(“pattern_1”, “pattern_2”, “pattern_3”, …etc). 6.3 Dates and Date-times Working with dates and times can be challenging. This section begins with a discussion of how base R handles dates and times, since there is a ton of code out there that utilizes these older functions. We will then quickly transition to the lubridate family of functions (part of the Tidyverse) because of their versatility and ease-of-use. 6.3.1 Dates and Times in base R Dates and times in base R all proceed from an “epoch” or time origin. In R, the epoch or “dawn of time” occurred at midnight on January 1st, 1970. For the sake of the R programming world, the concept of time started at that precise moment and has moved forward ever since. To note: R can handle date-times before 1/1/1970; it just treats them as negative values! To see a date-time object, you can tell R to give you the current “System Time” by calling the Sys.time() function. Sys.time() ## [1] &quot;2020-10-03 16:31:16 MDT&quot; As you can see, we got back the date, time, and timezone used by my computer (whenever I last ran this code in bookdown). If you want to see how this time is stored in R internally, you can use unclass(), which returns an object value with its class attributes removed. When we wrap unclass() around Sys.time(), we will see the number of seconds that have occurred between the epoch of 1/1/1970 and right now: unclass(Sys.time()) ## [1] 1601764277 That’s a lot of seconds. How many years is that? Just divide that number by [60s/min \\(\\cdot\\) 60min/hr \\(\\cdot\\) 24hr/d \\(\\cdot\\) 365d/yr] =&gt; 50.791612 years. This calculation ignores leap years, but you get the point… 6.3.2 Date-time formats Note that the Sys.time() function provided the date in a “year-month-day” format and the time in an “hour-minute-second” format: 2020-10-03 16:31:16. Not everyone uses this exact ordering when they record dates and times, which is one of the reasons working with dates and times can be tricky. You probably have little difficulty recognizing the following date-time objects as equivalent but, for some computer programs, not so much: Table 6.2: Date-time objects come in different forms 12/1/99 8:46 PM 1-Dec-1999 20:46 UTC December 1st, 1999, 20:46:00 You will often see time referenced with a “UTC”, which stands for “Universal Time, Coordinated”. UTC is preferred by programmers because it doesn’t have a timezone and it doesn’t follow Daylight Savings Time conventions. Daylight savings is the bane of many coders. In practice, UTC is the same time as GMT (Greenwich Mean Time, pronounced “gren-itch”) but with an important distinction. GMT is one of the many time-zones laid out across Earth’s longitude, whereas, UTC has no timezone; UTC is the same time for everyone, everywhere. In Colorado, we are UTC-6 hours during daylight savings (March-Nov) and UTC-7 during standard time (Nov-March). This means that most Coloradoans eat dinner at 12am UTC (6pm MST). 6.3.3 Date-time classes in R R has several classes of date-time objects, none of which are easy to remember: POSIXct - stored as the time, in seconds, between the epoch of 1970-01-01 00:00:00 UTC and the date-time object in question. the ‘ct’ stands for “continuous time” to represent “continuous seconds from origin”; A POSIXct object is a single numeric vector, thus useful for efficient computing. POSIXlt - stored as a list of date-time objects. the ‘lt’ stands for “list time”. A POSIXlt list contains the following elements: sec as 0–61 seconds min as 0–59 minutes hour as 0–23 hours mday as 1–31 day of the month mon as 0–11 months after the first of the year year as Years since 1900 wday as 0–6 day of the week, starting on Sunday yday as 0–365 days of the year. isdst as a flag for Daylight savings time. Positive if in force, zero if not, negative if unknown. POSIXt - this is a virtual class. POSIXt (without the “l”) is an internal way for R to convert between POSIXct and POSIXlt date-time objects. Think of the POSIXt as a way for R to perform operations/conversions between a POSIXct and POSIXlt object without throwing an error your way. As a reminder, here are some of the most common vector classes in R: Class Example character “Chemistry”, “Physics”, “Mathematics” numeric 10, 20, 30, 40 factor Male [underlying number: 1], Female [2] Date “2010-01-01” [underlying number: 14,610] logical TRUE, FALSE date-time “2020-06-23 11:05:20 MDT” To discover the class of a vector (including a column in a dataframe—remember each column can be thought of as a vector), you can use class(): class(Sys.time()) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; Both the POSIXct and POSIXlt class of objects return the same value to the user; the difference is really in how these classes store date-time objects internally. To examine them, you can coerce Sys.time() into each of the two classes using as.POSIXct and as.POSIXlt functions and then examine their attributes. time_now_ct &lt;- as.POSIXct(Sys.time()) unclass(time_now_ct) ## [1] 1601764277 time_now_lt &lt;- as.POSIXlt(Sys.time()) str(unclass(time_now_lt)) # the `str()` function makes the output more compact ## List of 11 ## $ sec : num 16.9 ## $ min : int 31 ## $ hour : int 16 ## $ mday : int 3 ## $ mon : int 9 ## $ year : int 120 ## $ wday : int 6 ## $ yday : int 276 ## $ isdst : int 1 ## $ zone : chr &quot;MDT&quot; ## $ gmtoff: int -21600 ## - attr(*, &quot;tzone&quot;)= chr [1:3] &quot;&quot; &quot;MST&quot; &quot;MDT&quot; It’s easy to see why the POSIXct object class is more computationally efficient, but it’s also nice to see all the date-time information packed into the POSIXlt. This is why R keeps a key to unlock both using POSIXt. As my father used to say: clear as mud? 6.3.4 Reading and classifying date-times Oftentimes, when data is read into R, there are column elements that contain date and time information. These dates and times are often interpreted by R as character vectors, which means they have lost their relational attributes For example, you cannot subtract “Monday 08:00” from “Wednesday 12:00” and get “2 days 4 hours”. If we want to analyze dates and times in a relational way, we need to instruct R to recognize these as date-time objects (i.e., as either the POSIXct or POSIXlt class). Thus, to convert a character vector into date or date-time object requires a change of that vector’s class. Date-time elements can be tricky to work with for a few reasons: Different programs store and handle dates and times in different ways The existence of time zones means that date-time values can change with location Date-time strings can be separated with spaces, colons, commas, slashes, dashes, or a mix of all those together (see Table 6.2) The base R function to convert between character classes and date-time classes is the function strptime(), which is short for “string parse into date-time”. I mention this function not because I encourage you to use it but because I want you to be able to recognize it. The function has over 39 conversion specifications that it can take as arguments. That is to say, this function not simple to master. If you are a glutton for punishment, I invite you to read the R Documentation with ?strptime. Here are a few base R functions for working with date-time objects that are worth knowing: Table 6.3: Basic Date-time functions {base} R Function Value Returned Example Sys.Date() Current system date “2020-06-23” Sys.time() Current system date-Time “2020-06-23 11:05:20 MDT” Sys.timezone() Current system timezone “America/Denver” as.POSIXct() date-time object of class POSIXct “2020-06-23 11:05:20 MDT” as.POSIXlt() date-time object of class POSIXlt “2020-06-23 11:05:20 MDT” strptime() date-time object of class POSIXlt “2020-06-23 11:05:20 MDT” 6.4 lubridate The lubridate package was developed specifically to make it easier to work with date-time objects. You can find out more information on lubridate here. 6.4.1 Parsing functions in lubridate One of best aspects of lubridate is its ability to parse date-time objects with simplicity and ease; the lubridate parsing functions are designed as “named-to-order”. Let me explain: Parse: to break apart and analyze the individual components of something, like a character string. If a character vector is written in “year-month-day” format (e.g., \"2020-Dec-18\"), then the lubridate function to convert that vector is ymd(). If a character vector is written in “day-month-year” format (e.g., \"18-Dec-2020\"), then the lubridate function to convert that vector is dmy(). Try it out: # create a character vector date_old &lt;- &quot;2020-Dec-18&quot; # prove it&#39;s a character class class(date_old) ## [1] &quot;character&quot; # convert it to a `Date` class with `ymd()` date_new &lt;- lubridate::ymd(date_old) # prove it worked class(date_new) ## [1] &quot;Date&quot; That little conversion exercise may not have blown you away, but watch what happens when I feed the following set of wacky character vectors into that same lubridate parsing function, ymd(): messy_dates &lt;- c(&quot;2020------Dec the 12&quot;, &quot;20.-.12.-.12&quot;, &quot;2020aaa12aaa12&quot;, &quot;20,12,12&quot;, &quot;2020x12-12&quot;, &quot;2020 .... 12 ...... 12&quot;, &quot;&#39;20.December-12&quot;) ymd(messy_dates) ## [1] &quot;2020-12-12&quot; &quot;2020-12-12&quot; &quot;2020-12-12&quot; &quot;2020-12-12&quot; &quot;2020-12-12&quot; ## [6] &quot;2020-12-12&quot; &quot;2020-12-12&quot; Boom. That’s right, the ymd() parsing function figured them all out correctly with almost no effort on your part. But wait, there’s more! The lubridate package contains parsing functions for almost any order you can imagine. Parsing Function Format to Convert ymd() year-month-day mdy() month-day-year dmy() day-month-year And if you need to parse a time component, simply add a combination of _hms to the function call to parse time in “hours-minutes-seconds” format. Some additional examples of how you would parse time that followed from a ymd format: Parsing Function Format to Convert ymd_h() year-month-day_hours mdy_hm() year-month-day_hours-minutes dmy_hms() year-month-day_hours-minutes-seconds The beauty of the lubridate parsers is that they do the hard work of cleaning up the character vector, regardless of separators or delimiters within each string, and return either a Date or Date-time object class. 6.4.2 Date-time manipulation with lubridate To convert the date column in the daily_show data into a Date class, you can run: library(package = &quot;lubridate&quot;) # check the class of the &#39;date&#39; column before mutating it class(x = daily_show$date) ## [1] &quot;character&quot; daily_show &lt;- mutate(.data = daily_show, date = mdy(date)) head(x = daily_show, n = 3) ## # A tibble: 3 x 5 ## year job date category guest_name ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1999 actor 1999-01-11 Acting Michael J. Fox ## 2 1999 Comedian 1999-01-12 Comedy Sandra Bernhard ## 3 1999 television actress 1999-01-13 Acting Tracey Ullman # check the class of the &#39;date&#39; column after mutating it class(x = daily_show$date) ## [1] &quot;Date&quot; Once you have an object in the Date class, you can do things like plot by date, calculate the range of dates, and calculate the total number of days the dataset covers: # report the min and max dates range(daily_show$date) # calculate the duration from first to last date using base max(daily_show$date) - min(daily_show$date) We could have used these to transform the date in daily_show, using the following pipe chain: daily_show &lt;- readr::read_csv(file = &quot;data/daily_show_guests.csv&quot;, skip = 4) %&gt;% dplyr::rename(job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) %&gt;% dplyr::select(-YEAR) %&gt;% dplyr::mutate(date = lubridate::mdy(date)) %&gt;% dplyr::filter(category == &quot;Science&quot;) # show first two rows of dataframe head(x = daily_show, n = 2) ## # A tibble: 2 x 4 ## job date category guest_name ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 neurosurgeon 2003-04-28 Science Dr Sanjay Gupta ## 2 scientist 2004-01-13 Science Catherine Weitz The lubridate package also includes functions to pull out certain elements of a date, including: wday() return the day of the week pertaining to a Date object mday() return the day of the month pertaining to a Date object yday() return the day of the year pertaining to a Date object month() return the month pertaining to a Date object quarter() return the quarter of hte year pertaining to a Date object year() return the year pertaining to a date object For example, we could use wday() to create a new column with the weekday of each show: daily_show %&gt;% dplyr::mutate(show_day = lubridate::wday(x = date, label = TRUE)) %&gt;% dplyr::select(date, show_day, guest_name) %&gt;% dplyr::slice(1:5) ## # A tibble: 5 x 3 ## date show_day guest_name ## &lt;date&gt; &lt;ord&gt; &lt;chr&gt; ## 1 2003-04-28 Mon Dr Sanjay Gupta ## 2 2004-01-13 Tue Catherine Weitz ## 3 2004-06-15 Tue Hassan Ibrahim ## 4 2005-09-06 Tue Dr. Marc Siegel ## 5 2006-02-13 Mon Astronaut Mike Mullane R functions tend to use the timezone of YOUR computer’s operating system by default—or UTC, or GMT. You need to be careful when working with dates and times to either specify the time zone or convince yourself the default behavior works for your application. 6.5 Tidy Data “Tidy Data” is a philosophy for how to arrange your data in a intuitive, rectangular fashion, commonly in a dataframe. In his 2014 paper, Hadley Wickham states, “tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).” The definition of a tidy dataset is straightforward: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. The first two rules, each variable forms a column and each observation forms a row, are relatively easy to implement. In fact, most of the dataframes that we have used so far follow this convention. The trick is to note that some column variables might look independent from one another, when, in fact, the columns represent a single variable broken out by some hierarchical, or nesting, structure. For example, the (untidy) table below (6.4) shows exam grades for three students. Table 6.4 is untidy because the variable depicting the exam score shows up in three different columns. Another way to characterize the untidiness of this table is that we have three different exams (Exam1, Exam2, Exam3) that represent a variable (Exam_# or Exam_type), that is not represented with a column. Table 6.4: An Untidy Table of Exam Scores. Name Exam1_Score Exam2_Score Exam3_Score Harry 85 79 88 Ron 81 75 89 Hermione 95 97 99 Thus, to tidy table 6.4, we need to create a variable called Exam and move all the scores into a new column vector named Scores. This effectively makes the data “long” rather than “wide”. Table 6.5: A Tidy Table of Exam Scores Name Exam Scores Harry 1 85 Harry 2 81 Harry 3 95 Ron 1 79 Ron 2 75 Ron 3 97 Hermione 1 88 Hermione 2 89 Hermione 3 99 For our work, one of the goals of data cleaning is to create datasets that follow this tidy convention. Fortunately, the tidyr package contains a useful function called pivot_longer() for re-arranging dataframes from untidy (typically “wide) to tidy (”long\"). pivot_longer() is designed to “lengthen” a data frame by “increasing the number of rows and decreasing the number of columns”. This function creates two new columns as output and requires four arguments: data = the dataframe to be lengthened cols = a list of the columns that should be combined together (forming a single variable to be represented in a new, single column) names_to = this is an output column that contains the names of the old columns that are being combined. values_to = this is the other output column that contains the values from within the old columns that are being combined. The pivot_longer() function always creates two new columns: - one column contains names_to = information. - the other column contains the values_to = data. In the case of Table 6.4, we are using cols = to combine Exam1_Score, Exam2_Score and Exam3_Score into a a column of names_to = \"Exam\" and the data represented in those three columns gets moved into a column of values_to = \"Scores\". We also add a call to dplyr::mutate() and dplyr::case_when() to convert strings to numbers. The case_when() function allows you introduce multiple “if/else” statements into a vectorized operation, such as within this call to dplyr:mutate(). This function takes a series of logical arguments as cases, with each case seeking to find a “match”. Once a “match” occurs, an outcome instruction is followed. In the code chunk below, the left-hand-side of each “case” evaluates whether each entry in the Exam column equates to a particular string (like \"Exam_1_Score\"). On the right-hand-side of each case, separated by a tilde ~, contains the mutate() instruction. In plain speak, the function reads: If the vector contains a string that matches “Exam_1_Score”, then replace that entry with a 1; else, if the vector contains a string that matches “Exam_2_Score”, then replace that entry with a 2; else, if the vector contains a string that matches “Exam_3_Score”, then replace that entry with a 3. tidygrades &lt;- tidyr::pivot_longer(data = grades_untidy, cols = Exam1_Score:Exam3_Score, names_to = &quot;Exam&quot;, values_to = &quot;Scores&quot;) %&gt;% dplyr::mutate(Exam = dplyr::case_when( Exam == &quot;Exam1_Score&quot; ~ 1, Exam == &quot;Exam2_Score&quot; ~ 2, Exam == &quot;Exam3_Score&quot; ~ 3 )) Table 6.6: Pivot longer applied to untidy data Name Exam Scores Harry 1 85 Harry 2 79 Harry 3 88 Ron 1 81 Ron 2 75 Ron 3 89 Hermione 1 95 Hermione 2 97 Hermione 3 99 Now these data are tidy, wherein each variable forms a column, each row forms an observation, and the table is specific to one observational unit. This may seem obvious, but most data analysis problems occur because of poor data management techniques. Remember: What might seem useful at the time—like a color-coded Excel spreadsheet—is a pain for computers and not reproducible. Do yourself a favor and begin with the end in mind: tidy data. "],
["eda2.html", "Chapter 7 Multivariate Data Exploration 7.1 Ch. 7 Objectives 7.2 Bivariate Data 7.3 Scatterplot 7.4 Exploring Multivariate Data 7.5 Ch-7 Exercises 7.6 Ch-7 Homework", " Chapter 7 Multivariate Data Exploration 7.1 Ch. 7 Objectives After this chapter, you should (know / understand / be able to ): Define correlation, causation, and their difference Conduct a formal exploratory data analysis on multivariate data using geoms from ggplot Create and interpret a scatterplot between two variables Create and interpret a Q-Q plot Create and interpret directional bias in a Tukey mean difference plot Create and extract descriptive statistics and qualitative information from Boxplots 7.2 Bivariate Data Whereas univariate data analyses are directed at “getting to know” the observations made for a single variable, bivariate (and multivariate) analyses are designed to examine the relationship that may exist between two (or more) variables. Like the Chapter on Univariate data, we will focus first on data exploration - a key step towards “getting to know” your data and one that should always proceed inferential statistics or making conclusions about your data. Bivariate means two variables where the observations are paired (i.e., each observation samples both variables so that they are linked). 7.3 Scatterplot Undoubtedly, you have seen scatterplots many times before; we will give them a formal treatment here. The scatterplot allows you to assess the strength, direction, and type of relationship between two variables. This can be important for determining factors like: Correlation Linearity Performance (of a measurement) in terms of precision, bias, and dynamic range Traditionally, a scatterplot shows paired observations of two variables with the dependent variable on the y-axis and the independent variable on the x-axis. Creating a plot in this way means that, before you begin, you must make a judgement call about which variable depends on which. The roots of this terminology/protocol lie in the practice of linear regression and the scientific method, the former of which we will discuss in more detail later. For the purposes of exploratory data analysis, however, it actually doesn’t matter which variable goes on which axis. That said, since we don’t wish to break with tradition, let’s agree to follow the guidelines on dependent/independent variables so as not to invoke the wrath of the gods. Statistics: The independent variable (x-axis) is thought to have some influence/control over the dependent variable (y-axis). Scientific Method: The experimenter manipulates the control variable (independent, x-axis) and observes the changes in the response variable (dependent, y-axis). Exploratory Data Analysis: We throw two variables onto a plot to investigate their relationship. We make a guess about which one is the independent variable (x-axis) and which one is the dependent variable (y-axis) and we hope that nobody calls us out if we got it wrong. 7.3.1 Causality All this talk about dependent and independent variables is fundamentally rooted in the practice of causal inference reasoning: the ability to say that “action A” caused “outcome B”. Discovering (or proving) that one thing caused another to happen can be an incredibly powerful event. Proving causality leads to the awarding of Nobel Prizes, the creation of new laws and regulations, the judgment of guilt or innocence in court, the changing and convincing of human minds and behaviors, and simply put: more understanding. A full treatment of causal inference reasoning is beyond the scope of this course, but we will, from time to time, delve into this topic. The art of data science can be a beautiful and compelling way to demonstrate causality….but most of us need to learn to crawl before we can walk, run, or fly. For now, let’s put aside the pursuit of causation and begin with correlation. 7.3.2 Correlation The scatterplot is a great way to visualize whether (and, to an extent, how) two variables are correlated. Correlation: a mutual relationship or connection between two or more things; the process of establishing a relationship or connection between two or more measures. Below are four examples of bivariate data with differing degrees of correlation: perfect, strong, moderate, and none. These are qualitative terms, of course, what is “moderate” to one person may be poor and unacceptable to another. Later on, in the modeling section, we will discuss ways to assess the strength of correlation more quantitatively. Figure 7.1: Scatterplot examples showing bivariate data with varying degrees of correlation. In addition to the strength of the correlation, the sign and form of the correlation can vary, too: - positive correlation: the dependent variable trends in the same direction as the independent variable - negative correlation: the dependent variable decreases when the independent variable increases - linear correlation: the relationship between the two variables can be shown with a straight line - non-linear correlation: the relationship between the two variables is curvilinear Figure 7.2: Scatterplot examples showing bivariate data with varying types of correlation. 7.3.3 Correlation \\(\\neq\\) causation Causation: the process or condition by which one event, a cause, contributes to the occurence of another event, the effect. In this process the cause is partly or wholly responsible for the effect. Let’s take a closer look at the dangers of mistaking a correlated relationship as causal relationship between two variables. Shown below is a scatterplot that builds off the mpg dataset we first discussed in Chapter 4. Using the mpg data frame, we will plot the relationship between the number of cylinders in an engine (cyl), the independent variable, and that vehicle’s fuel economy (hwy), the dependent variable). Figure 7.3: Scatterplot of Engine Displacement vs. Fuel Economy Looking at this plot, there appears a clear correlation between the number of cylinders in a vehicle and its fuel efficiency. A linear fit through these data gives a Pearson correlation coefficient of -0.76: not a perfect relationship but a significant one nonetheless. Does this mean that a causal relationship exists? If so, then we only need to mandate that all future vehicles on the road be built with 4-cylinder engines, if we want more a fuel-efficient fleet! That mandate, of course, would likely produce minimal effect. Just because two variables are correlated doesn’t mean that a change in one will cause a change in the other. Those who understand internal combustion know that the number of cylinders is a design parameter related more to engine power than to engine efficiency (i.e., the number of cylinders helps determine total displacement volume). Indeed, the causal relationship for fuel efficiency, in terms of miles traveled per gallon, is due more directly to engine conversion efficiency, vehicle drag coefficient, and vehicle mass. If you want more fuel-efficient cars and trucks, you need more efficient engines that weigh less. In the 1990s and early 2000s nearly all engine blocks were made from cast iron. Today, nearly all engine blocks are made from aluminum. Can you guess why? Did you know that being a smoker is correlated with having a lighter in your pocket? Furthermore, it can be shown that keeping a lighter in your pocket is correlated with an increased risk of developing heart disease and lung cancer. Does this mean lighters in your pocket cause lung cancer? 7.4 Exploring Multivariate Data With multivariate data we often consider more than just 2 variables of interest; however, visualizing more than 2 variables in a single plot can be challenging. There are advanced statistical approaches to exploring such data (e.g., multivariate regression, principal components, machine learning, etc.), but these techniques are beyond the scope of this course. Here, I will introduce a few graphical techniques that are useful for multivariate data exploration. 7.4.1 Faceting One easy way to evaluate two or more variables is to create multiple plots (or facets) through the ggplot2::facet function. This function creates a series of plots, as panels, where each panel represents a different value (or level) of a third variable of interest. For example, let’s create a ggplot2 object from the mtcars data set that explores the relationship between a vehicle’s fuel economy and its weight. First, let’s create a simple bivariate scatterplot of these data (mpg vs. wt) and fit a linear model through the data (note: we haven’t yet discussed modeling but more on that later). # fit a linear model g1_model &lt;- lm(mpg ~ wt, data=mtcars) #create a ggplot2 object g1 &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth(model = g1_model, method = &quot;lm&quot;) + ylab(&quot;Fuel Economy (mi/gal)&quot;) + xlab(&quot;Vehicle Weight (x1000 lb)&quot;) g1 Figure 7.4: Scatterplot of fuel economy vs. vehicle weight from the mtcars dataset. However, looking back at Figure 7.3, we know that the number of cylinders (cyl) is also associated with fuel efficiency and many of the vehicles from mpg have different cyl numbers. To examine these three variables together (mpg, wt, and cyl) we can create a scatterplot that is faceted according to the cyl variable. This is relatively easy to do in ggplot2 by adding a facet_grid() layer onto our ggplot object. The key details to pass to facet_grid() are: Whether we want to see the facets as rows or columns, and The variable being used to create the facets. These two specifications can be made as a single argument to facet_grid() in the form: facet_grid(rows = vars(variable)) or facet_grid(cols = vars(variable)) where variable is the name of the column vector used to define the facets. In this case, seeing the plots in columns seems fine, so we would add: facet_grid(cols = vars(cyl), to the ggplot() object as follows: g1 + facet_grid(cols = vars(cyl), labeller = label_both) #this code adds names &amp; values each panel label Figure 7.5: Scatterplots of fuel economy vs. vehicle weight by number of cylinders in the engine (data from the mtcars dataset). Interestingly, but perhaps not surprising, we can see that the vehicles with different cylinder numbers tend to have different fuel efficiency, but even within these facets we still see a relationship between efficiency and vehicle weight. Note also, that the faceting call led to the fitting of three different linear models, one for each facet. Here are the same data in a plot that is faceted by rows instead of columns. g1 + facet_grid(rows = vars(cyl), labeller = label_both) Figure 7.6: Scatterplots of fuel economy vs. vehicle weight by number of cylinders in the engine (data from the mtcars dataset). 7.4.2 Coloring We can also use color to indicate variation in data; this can be useful for introducing a third variable into scatter and time-series plots. Note: when introducing color as a variable into a plot, you must do so through an aesthetic, such as: geom_point(aes(color = cyl)). Let’s recreate Figure 7.4 and highlight the cyl variable using different colors. The addition of color provides us with the same level of insight as did the facets above. # instruct R to treat the cyl variable as a factor with discrete levels # this, in turn, tells ggplot2 to assign discrete colors to each level mtcars$cyl &lt;- as.factor(mtcars$cyl) g3 &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point() + ylab(&quot;Fuel Economy (mi/gal)&quot;) + xlab(&quot;Vehicle Weight, (x1000 lb)&quot;) g3 Figure 7.7: Vehicle fuel economy vs. weight and colored by number of engine cylinders (data from mtcars) When using color, be aware that many people are unable to distinguish red from green or blue from yellow. Many options exist to avoid issues from color blindness (e.g., the viridis palette) and websites like color-blindness.com allow you to upload image files as a test against common forms. Here is an updated version of Figure 7.7 that avoids issues with color blindness and, better yet, differentiates the cyl variable with both colors and symbols. #&quot;#330099&quot;,&quot;#CC0066&quot;,&quot;#FF6633&quot;, &quot;#0099CC&quot;, &quot;#FF9900&quot;,&quot;#CC6633&quot;, &quot;#33CC99&quot;, ggplot(data = mtcars, aes(x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point(size = 2.5) + ylab(&quot;Fuel Economy (mi/gal)&quot;) + xlab(&quot;Vehicle Weight, (x1000 lb)&quot;) + scale_colour_manual(values = c(&quot;sandybrown&quot;, &quot;orangered&quot;, &quot;steelblue2&quot;)) + theme_classic() Figure 7.8: Vehicle fuel economy vs. weight and colored by number of engine cylinders (data from mtcars) Whenever you use color to differentiate variables, use symbols, too. 7.4.3 Contour Plots 7.4.4 Time-Series Density 7.5 Ch-7 Exercises 7.5.1 In-class: fueleconomy.gov This in-class exercise is will conduct an exploratory, multivariate data analysis on vehicle fuel economy. We will begin by downloading a .zip file from fueleconomy.gov - a Federal program that tracks the fuel economy of all vehicles sold in the US. The .zip file contains a .csv with fuel economy information for nearly every vehicle manufactured between 1984 and today. We will use the readr and dplyr packages to load and clean the data, respectively. A data dictionary (something that defines and explains each variable in the data set) is also available at the website above. The first code chunk will download the fueleconomy.gov data directly into a temp file using download.file() from base R. We will then unzip() that temp file into a .csv and use readr to read that .csv into a data frame named raw_data. #create a temp file to hold the zipped data temp &lt;- tempfile() #download the file into temp file download.file(url = &quot;https://www.fueleconomy.gov/feg/epadata/vehicles.csv.zip&quot;, destfile = temp, mode=&quot;wb&quot;) temp2 &lt;- unzip(temp, &quot;vehicles.csv&quot;, exdir = &quot;./data/&quot;) #unzip .csv to /data dir raw_data &lt;- read_csv(temp2) #read the csv into a data frame unlink(temp) #delete the temp file rm(temp, temp2) #remove the two temp objects from local environment Looking at the raw_data data frame, we see there are 83 variables with over 42,000 observations. That’s a LOT of vehicles! In most analyses of large data sets, we don’t need to inspect every variable. Let’s create a vector of variables (vars_needed) that we want to keep and pass that vector to dplyr::select() to retain only the variables we want. To pass a character vector as an argument to dplyr::select(), instead of just a single column name, we use the all_of() function - a {tidy-select} argument modifier. You can type ?tidyr_tidy_select to learn more. vars_needed &lt;- c(&quot;id&quot;, &quot;make&quot;, &quot;model&quot;, &quot;year&quot;, &quot;cylinders&quot;, &quot;displ&quot;, &quot;drive&quot;, &quot;trany&quot;, &quot;VClass&quot;, &quot;fuelType1&quot;, &quot;comb08&quot;, &quot;highway08&quot;, &quot;city08&quot;, &quot;co2TailpipeGpm&quot; ) df_mpg&lt;-raw_data %&gt;% dplyr::select(all_of(vars_needed)) #all_of tells dplyr::select to expect a character vector of column names rm(raw_data) #remove large file from memory Some of these variables can be coded as factors (categorical variables that can be classified into discrete levels). For example, there are a finite number of vehicle transmission trany or drivetrain drive types on the market and by telling R to code these data as factors, we can analyze these variables in categorical form. First, we will create a vector of variable names that we want to code as factors, vars_factr. The we will apply the as.factor() function to those variables using dplyr::mutate(across()). The across() function allows one to apply the same transformation to multiple columns in a data frame. We will also take the opportunity to rename a few of these variables (following our naming guidelines discussed earlier) and to filter the data to retain only vehciles made after the year 2000. # ID the columns that we want as class:factor vars_factr &lt;- c(&quot;make&quot;, &quot;drive&quot;, &quot;trany&quot;, &quot;VClass&quot;, &quot;fuelType1&quot;) df_mpg %&gt;% # mutate(across(a)) applies the as.factor function only to the vars of interest dplyr::mutate(across(all_of(vars_factr), .fns = as.factor)) %&gt;% #create simpler names dplyr::rename(fuel_type = fuelType1, cyl = cylinders, tran = trany, v_class = VClass) %&gt;% # easier string to type # keep only data collected after 2000 for the sake of simplicity dplyr::filter(year &gt;= 2000) -&gt; df_mpg rm(vars_needed, vars_factr) # won&#39;t be needing these anymore Begin as we always do, by simply looking at some of the data. head(df_mpg) ## # A tibble: 6 x 14 ## id make model year cyl displ drive tran v_class fuel_type comb08 ## &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 15589 Acura NSX 2000 6 3 Rear… Auto… Two Se… Premium … 18 ## 2 15590 Acura NSX 2000 6 3.2 Rear… Manu… Two Se… Premium … 18 ## 3 15591 BMW M Co… 2000 6 3.2 Rear… Manu… Two Se… Premium … 19 ## 4 15592 BMW Z3 C… 2000 6 2.8 Rear… Auto… Two Se… Premium … 19 ## 5 15593 BMW Z3 C… 2000 6 2.8 Rear… Manu… Two Se… Premium … 19 ## 6 15594 BMW Z3 R… 2000 6 2.5 Rear… Auto… Two Se… Premium … 19 ## # … with 3 more variables: highway08 &lt;dbl&gt;, city08 &lt;dbl&gt;, co2TailpipeGpm &lt;dbl&gt; Next, let’s take a look at some of the factor levels. There are lots of ways to do this in R. #print a character vector of levels for the drive variable in df_mpg levels(df_mpg$drive) ## [1] &quot;2-Wheel Drive&quot; &quot;4-Wheel Drive&quot; ## [3] &quot;4-Wheel or All-Wheel Drive&quot; &quot;All-Wheel Drive&quot; ## [5] &quot;Front-Wheel Drive&quot; &quot;Part-time 4-Wheel Drive&quot; ## [7] &quot;Rear-Wheel Drive&quot; #print a character vector of levels for the fuel_type variable in df_mpg levels(df_mpg$fuel_type) ## [1] &quot;Diesel&quot; &quot;Electricity&quot; &quot;Midgrade Gasoline&quot; ## [4] &quot;Natural Gas&quot; &quot;Premium Gasoline&quot; &quot;Regular Gasoline&quot; #print a character vector of levels for the v_class variable in df_mpg levels(df_mpg$v_class) ## [1] &quot;Compact Cars&quot; &quot;Large Cars&quot; ## [3] &quot;Midsize Cars&quot; &quot;Midsize Station Wagons&quot; ## [5] &quot;Midsize-Large Station Wagons&quot; &quot;Minicompact Cars&quot; ## [7] &quot;Minivan - 2WD&quot; &quot;Minivan - 4WD&quot; ## [9] &quot;Small Pickup Trucks&quot; &quot;Small Pickup Trucks 2WD&quot; ## [11] &quot;Small Pickup Trucks 4WD&quot; &quot;Small Sport Utility Vehicle 2WD&quot; ## [13] &quot;Small Sport Utility Vehicle 4WD&quot; &quot;Small Station Wagons&quot; ## [15] &quot;Special Purpose Vehicle&quot; &quot;Special Purpose Vehicle 2WD&quot; ## [17] &quot;Special Purpose Vehicle 4WD&quot; &quot;Special Purpose Vehicles&quot; ## [19] &quot;Special Purpose Vehicles/2wd&quot; &quot;Special Purpose Vehicles/4wd&quot; ## [21] &quot;Sport Utility Vehicle - 2WD&quot; &quot;Sport Utility Vehicle - 4WD&quot; ## [23] &quot;Standard Pickup Trucks&quot; &quot;Standard Pickup Trucks 2WD&quot; ## [25] &quot;Standard Pickup Trucks 4WD&quot; &quot;Standard Pickup Trucks/2wd&quot; ## [27] &quot;Standard Sport Utility Vehicle 2WD&quot; &quot;Standard Sport Utility Vehicle 4WD&quot; ## [29] &quot;Subcompact Cars&quot; &quot;Two Seaters&quot; ## [31] &quot;Vans&quot; &quot;Vans Passenger&quot; ## [33] &quot;Vans, Cargo Type&quot; &quot;Vans, Passenger Type&quot; Next, let’s see whether any variables contain missing data (NAs). A simple way is to map the combined functions of sum() and is.na() to each column of the data frame. We do this using the map_dfc function (reads: map a function across columns of a dataframe) from the purr:: package. df_mpg %&gt;% map_dfc(~sum(is.na(.))) ## # A tibble: 1 x 14 ## id make model year cyl displ drive tran v_class fuel_type comb08 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 231 230 4 9 0 0 0 ## # … with 3 more variables: highway08 &lt;int&gt;, city08 &lt;int&gt;, co2TailpipeGpm &lt;int&gt; If we filter the df_mpg data for entries that contain NA we discover that most of them are due to electric vehicles. This may be a variable level that we choose to exclude from certain analyses later on… missing_data &lt;- df_mpg %&gt;% filter_all(any_vars(is.na(.))) head(missing_data, n=10) ## # A tibble: 10 x 14 ## id make model year cyl displ drive tran v_class fuel_type comb08 ## &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 16423 Niss… Altr… 2000 NA NA &lt;NA&gt; &lt;NA&gt; Midsiz… Electric… 85 ## 2 16424 Toyo… RAV4… 2000 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 72 ## 3 17328 Toyo… RAV4… 2001 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 72 ## 4 17329 Ford Th!nk 2001 NA NA &lt;NA&gt; &lt;NA&gt; Two Se… Electric… 65 ## 5 17330 Ford Expl… 2001 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 39 ## 6 17331 Niss… Hype… 2001 NA NA &lt;NA&gt; &lt;NA&gt; Two Se… Electric… 75 ## 7 18290 Toyo… RAV4… 2002 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 78 ## 8 18291 Ford Expl… 2002 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 39 ## 9 19296 Toyo… RAV4… 2003 NA NA 2-Wh… &lt;NA&gt; Sport … Electric… 78 ## 10 30965 Ford Rang… 2001 NA NA 2-Wh… Auto… Standa… Electric… 58 ## # … with 3 more variables: highway08 &lt;dbl&gt;, city08 &lt;dbl&gt;, co2TailpipeGpm &lt;dbl&gt; A nice way to begin EDA with a large, multivariate data set is to understand how the different variables are ‘spread out’ across the data set. Let’s begin with a time series (by year) of all the combined fuel economy values, comb08, for all vehicle observations. The fuel economy data will be shown with boxplots overlain with violin plots. We will use a log-scale y-axis due to the large variation expected and outliers will be made more transparent to soften their effect. cdf1 &lt;- ggplot(data = df_mpg) + stat_ecdf(aes(x = comb08, color = &quot;Combined&quot;)) + stat_ecdf(aes(x = highway08, color = &quot;Highway&quot;)) + stat_ecdf(aes(x = city08, color = &quot;City&quot;)) + scale_color_manual(name = &quot;Type&quot;, values = c(&quot;Combined&quot; = &quot;navy&quot;, &quot;Highway&quot; = &quot;darkgreen&quot;, &quot;City&quot; = &quot;purple&quot;)) + scale_x_log10(breaks = breaks_log(8)) + theme_bw(base_size = 11) + xlab(&quot;Fuel Economy, mi/gal&quot;) + ylab(&quot;Quantile&quot;) + theme(legend.position = &quot;NULL&quot;) blue50 &lt;- rgb(0, 0, 255, max = 255, alpha = 125, names = &quot;blue50&quot;) hist1 &lt;- ggplot(data = df_mpg) + geom_histogram(aes(x = comb08, fill = &quot;Combined&quot;), alpha = 0.75, bins = 50) + geom_histogram(aes(x = highway08, fill = &quot;Highway&quot;), alpha = 0.75, bins = 50) + geom_histogram(aes(x = city08, fill = &quot;City&quot;), alpha = 0.75, bins = 50) + scale_fill_manual(name = &quot;Type&quot;, values = c(&quot;Combined&quot; = &quot;blue&quot;, &quot;Highway&quot; = &quot;darkgreen&quot;, &quot;City&quot; = &quot;purple&quot;)) + scale_x_log10(breaks = breaks_log(8)) + theme_bw(base_size = 11) + xlab(&quot;Fuel Economy, mi/gal&quot;) + ylab(&quot;Counts&quot;) + theme(legend.position = &quot;NULL&quot;) box1 &lt;- ggplot(data = df_mpg) + geom_boxplot(aes(x = comb08, y = &quot;Comb&quot;, fill = &quot;Combined&quot;), alpha = 0.75, position = &quot;dodge2&quot;) + geom_boxplot(aes(x = highway08, y = &quot;Hwy&quot;, fill = &quot;Highway&quot;), alpha = 0.75, position = &quot;dodge2&quot;) + geom_boxplot(aes(x = city08, y = &quot;City&quot;, fill = &quot;City&quot;), alpha = 0.75, position = &quot;dodge2&quot;) + scale_fill_manual(name = &quot;Type&quot;, values = c(&quot;Combined&quot; = &quot;blue&quot;, &quot;Highway&quot; = &quot;darkgreen&quot;, &quot;City&quot; = &quot;purple&quot;)) + scale_x_log10(breaks = breaks_log(8)) + theme_bw(base_size = 11) + xlab(&quot;Fuel Economy, mi/gal&quot;) + ylab(&quot;&quot;) + theme(legend.position = c(1.25, 0.5)) grid.arrange(cdf1, hist1, box1, widths = c(1,1,0.5), layout_matrix = rbind(c(1, NA, NA), c(2, NA, NA), c(3, NA, NA))) e1 &lt;- ggplot(data = df_mpg, aes(x = year, y = comb08)) + geom_violin(aes(group = year), outlier.shape = NA, fill = &quot;royalblue2&quot;) + geom_boxplot(aes(group = year), fill = NA, outlier.alpha = 0.2) + scale_y_log10(limits = c(10,100)) + theme_bw() ## Warning: Ignoring unknown parameters: outlier.shape e1 ## Warning: Removed 144 rows containing non-finite values (stat_ydensity). ## Warning: Removed 144 rows containing non-finite values (stat_boxplot). This DOE website outlines the EPA Corporate Average Fuel Economy (CAFE) standards that require vehicles to meet set fuel economy levels (in terms of miles-per-gallon; mpg) across the ‘fleet’ of available vehicles. Let’s load a .csv file named cafe and look at the requirements by year. cafe &lt;- read_csv(&quot;./data/CAFE_stds.csv&quot;, col_names = c(&quot;year&quot;, &quot;mpg_avg&quot;), skip = 1) ggplot(cafe, aes(year, mpg_avg)) + geom_col(fill = &quot;maroon&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs( y = &quot;Required Average Fuel Efficiency (mpg)&quot;, x = &quot;Year&quot;, title = &quot;Federal Combined Average Fuel Economy (CAFE) Standards&quot;) This is a nice explanatory story of why the average fuel economy numbers don’t match well to the published CAFE standards. Create a series of geom_density plots showing combined fuel economy comb08 across all vehicles and years as a function of fuel_type. Break out the different fuel_type categories into facets (ncol = 3). g1 &lt;- ggplot(data = df_mpg) + geom_density(aes(comb08, fill = fuel_type)) + facet_wrap(~fuel_type, ncol = 3) + scale_x_log10() + theme_bw() + theme(legend.position = &quot;none&quot;) g1 Show the same plot without facets. g2 &lt;- ggplot(data = df_mpg, aes(x=comb08)) + geom_density(aes(fill = fuel_type), position = &quot;identity&quot;, alpha = 0.6, adjust = 1) + scale_x_log10() + theme_bw() g2 Q1: Among 4-cylinder vehicles with Front-Wheel Drive, what make/model has the best highway fuel economy in 2018? df_mpg %&gt;% filter(cyl == 4, drive == &quot;Front-Wheel Drive&quot;, year == 2018) %&gt;% slice_max(order_by = highway08, n=1) %&gt;% select(make, model, drive, year, highway08) ## # A tibble: 1 x 5 ## make model drive year highway08 ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hyundai Ioniq Blue Front-Wheel Drive 2018 59 Q2: Among 8-cylinder vehicles with Rear-wheel Drive, what make/model has the worst city fuel economy in 2019? df_mpg %&gt;% filter(cyl == 8, drive == &quot;Rear-Wheel Drive&quot;, year == 2019) %&gt;% slice_min(order_by = city08) %&gt;% #top_n(n = -1, wt = city08) %&gt;% select(make, model, drive, year, city08) ## # A tibble: 1 x 5 ## make model drive year city08 ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bentley Mulsanne Rear-Wheel Drive 2019 10 Figure 7.9: The Bentley Mulsanne: $330k lets you park it on the sidewalk! 7.6 Ch-7 Homework Create a data fram from df_mpg that contains observations only for “Front-Wheel Drive” vehicles (one of the drive levels) that run on “Regular Gasoline” (a fuel_type level). With these data: Create a series of boxplots showing combined fuel efficiency on the y-axis and year on the x-axis. "],
["rprog4.html", "Chapter 8 Functional Programming 8.1 Ch. 8 Objectives 8.2 What is functional programming? 8.3 Writng Functions 8.4 The purrr:: package 8.5 Exercises 8.6 Homework", " Chapter 8 Functional Programming 8.1 Ch. 8 Objectives This chapter is designed around the following learning objectives for functional programming in R. Upon completing this chapter, you should be able to: Describe the basic tenets of functional programming Define functions and their basic attributes Create simple functions using named and optional arguments Define the meaning of “vectorized operations” in R Interpret and apply the purrr::map family of functions 8.2 What is functional programming? Functional programming means just that: programming with functions. A basic philosophy of functional programming in R is to replace “for-loops” with functions. While there isn’t anything inherently wrong with for-loops, they can be difficult to follow, especially when they are nested within one to another. R is well-poised to support the elimination of “for-loops” because R is a vectorized language. To be vectorized means that you don’t need a for-loop to execute this code: evens &lt;- seq(from = 2, to = 20, by = 2) evens_squared &lt;- evens^2 with code like this: evens_squared &lt;- for( i in length(evens)){ evens_squared &lt;- evens[i] * evens[i] } The top code performs the squared operator across each entry in the vector, one after another. The bottom set uses a for-loop to accomplish the same task, albeit in a longer (and harder to follow) version. The pipe operator %&gt;% is an ally in this endeavor because it allows you to pass an object through series of functions onto a vector (or data frame) in serial order. The pipe function is also easier to follow (with your mind) because it allows you to interpret the code as if you were reading instructions in a “how-to” manual: “take this object, then do this, then do this, then do that”. Without the %&gt;% operator, we are forced to nest functions together or write long and verbose code that steps through treatments slowly (and somewhat painfully). Looking at the code below should make clear which set is easier to comprehend: #nested code daily_show_2000 &lt;- select(filter(rename(daily_show, year = YEAR, job = GoogleKnowlege_Occupation, date = Show, name = Raw_Guest_List), year == 2000), job, date, name) #piped code daily_show_2000 &lt;- daily_show %&gt;% rename(year = YEAR, job = GoogleKnowlege_Occupation, date = Show, name = Raw_Guest_List) %&gt;% filter(year == 2000) %&gt;% select(job, date, name) In the nested example above, we “see” the dplyr::select() function first, yet yet the arguments to this function show up last (ar the end of the nested code). This separation between functions and their arguments only gets worse as the degree of nesting increases. The piped code, however, produces the same result as the nested code but in a much more “readable” fashion. A similar analogy holds for the nesting of “for-loops” (which, when nested, are even harder to follow!). 8.3 Writng Functions “If you have to do something more than twice, write a function to do it.” - Programming Proverb Up to this point, we have relied entirely on functions sourced from base R or from packages like Tidyverse;:(and for good reason - they are terribly useful). Every programmer, howver, at some point in their journey, discovers that the function they are desiring simply doesn’t exist - at least not in the way that suits their vision. To that end, its worth learning how to create your own functions. The syntax for function generation is relatively straight forward: #example code; will not run function_name &lt;- function(argument1, argument2, ...) { # insert code to manipulate arguments here # maybe some code to check validity of input arguments # specify a return value, if desired } Each function has arguments as input, body code as the working parts, and an environment where the function resides. These components can be accessed by passing the function name as an argument to: formals(), to see arguments body(), to see the function’s code or type just the function name, without (), into the console but note that many base R functions are coded in C and can be accessed on GitHub. environment(), to see where the function resides. more on R environments below 8.3.1 Example Function: my_mean Let’s create a simple function, named my_mean, to calculate the arithmetic mean for a numeric vector. This function takes only one argument, a vector x, for which we calculate the mean (sum(x) / length(x)), assign it to a value y, and the return y as output. Note that all the “action” for my_mean happens within the curly braces { } that follow the function assignment. my_mean &lt;- function(x) { y &lt;- sum(x) / length(x) # calculate mean(x) if true return(y) } my_mean(1:5) ## [1] 3 While this function is simple and straightforward, it does have some limitations. For example, what happens if we pass a character vector to my_mean or a numeric vector that contains NA values? In the former case, we will get an error because our function uses the base R function sum(), which requires numeric, logical, or complex vectors as inpout (so our function inherits the properties of that function). If any NA values are present, however, we are less fortunate: only NA is returned. This many not seem like a big deal but if your analyss depends on calling my_mean in several locations (over and over), you might have a hard time debugging it… my_mean(c(1, 2, 3, 4, 5, NA)) ## [1] NA For this reason, functions often contain optional arguments that allow the user to specify how to handle such errors. The function my_mean2 below contains an optional argument, na.rm = TRUE, that defaults to remove NA values when present. This function uses if else logic to handle the na.rm = TRUE argument, since this argument can only have one of two values. my_mean2 &lt;- function(x, na.rm = TRUE) { if (na.rm == FALSE) { y &lt;- sum(x) / length(x) return(y) } else { y &lt;- sum(na.omit(x)) / length(na.omit(x)) return(y) } } Now, when we pass a vector containing NAs to my_mean2, we get a numeric result. my_mean2(c(1:5,NA), na.rm = TRUE) ## [1] 3 A couple points worth noting about the functions above. First, take note that most homemade functions rely on other functions called within their body text, and so they inherit the properties of those functions. Second, you may have noticed that while the body() code in my_mean2 assigns the mean of x to a new variable, y, this function-specific variable does not show up in the “Global Environment” pane after the function is executed. This is due to how environments are created in R: an environment essentially draws walls around objects. Indeed, objects that are defined within functions are part of the evaluation environment within that function, even if the function returns the value of the object as output (“what happens inside functions stays inside functions”). A detailed discussion on R environments is beyond the scope of this book, look here for an introduction to environments and here for a more detailed tutorial. The take-home point is that a homemade function can exist within the global environment and return values to the global environment, even if the objects created within that function don’t. 8.3.2 Example Function: “import.w.name” Oftentimes, you will have a list of files of the same type/structure that you want to import and analyze in a single data frame. This exercise (and the section that follows) will demonstrate how you can streamline that process using functional programming. Let’s create a function to \"import a file with its name appended\". For this example, assume that your files represent data from network of sensors, where the ID assigned to each sensor is included in its filename but not in the file itself. To give you an example of what we are working with, lets use list.files() to look at the file names and paths. For this exercise we show a short list of 8 files (4 each from two sensors) but one could imagine this list being hundreds of entries.To Note: these are real data collected with using sensors from this citizen-science network and published by our research group here in 2019. The “PA” in each filename stands for Purple Air. list.files(&#39;./data/purpleair/&#39;, full.names=TRUE) ## [1] &quot;./data/purpleair//PA019_20181022.csv&quot; ## [2] &quot;./data/purpleair//PA019_20181023.csv&quot; ## [3] &quot;./data/purpleair//PA019_20181024.csv&quot; ## [4] &quot;./data/purpleair//PA019_20181025.csv&quot; ## [5] &quot;./data/purpleair//PA020_20181022.csv&quot; ## [6] &quot;./data/purpleair//PA020_20181023.csv&quot; ## [7] &quot;./data/purpleair//PA020_20181024.csv&quot; ## [8] &quot;./data/purpleair//PA020_20181025.csv&quot; Thus, we wish to write a function that not only imports these files into a data frame but also extracts the part of the filename (i.e., PA019 and PA020) as one of the data columns (otherwise, when the data were, combined we might not know what data was associated with a given sensor!). In this function we will also include a step to clean up the newly created data frame with a call to dplyr::select() to retain only a few variables of interest. Seeing that these files are .csv, we can leverage readr::read_csv # create an object that tracks the file names and file paths file_list &lt;- list.files(&#39;./data/purpleair/&#39;, full.names=TRUE) # function to import a .csv and include part of the filename as a data column import.w.name &lt;- function(pathname) { #create a tibble by importing the &#39;pathname&#39; file df &lt;- read_csv(pathname, col_names = TRUE) df &lt;- df %&gt;% # use stringr::str_extract &amp; a regex to extract sensor ID from file name mutate(sensor_ID = str_extract(pathname, &quot;(?&lt;=//)[:alnum:]+(?=_)&quot;)) %&gt;% # clean up the resultant data frame with dplyr::select select(UTCDateTime, current_temp_f, current_humidity, pressure, pm2_5_atm, sensor_ID) return(df) } After sourcing this function, we can test it out on the first entry of our list of files. We specify the first entry with a subset to file_list[1]. PA_data_1 &lt;- import.w.name(file_list[1]) head(PA_data_1) ## # A tibble: 6 x 6 ## UTCDateTime current_temp_f current_humidity pressure pm2_5_atm sensor_ID ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2018/10/22T17:52… 90 12 852. 2.45 PA019 ## 2 2018/10/22T17:53… 86 13 852. 3.43 PA019 ## 3 2018/10/22T17:55… 87 13 852. 2.76 PA019 ## 4 2018/10/22T17:56… 87 13 852. 2 PA019 ## 5 2018/10/22T17:58… 87 13 852. 1.82 PA019 ## 6 2018/10/22T17:59… 87 13 852. 1.98 PA019 The import.w.name() function is useful, but not versatile; it was written to handle a special type of file with a particular naming convention. Those limitations aside, one could imagine how this function could be easily adapted to suit other file types and formats. As you develop your coding skills, a good strategy is to keep useful functions in a .R script file so that you can call upon them when needed: source(import.w.name.R) 8.4 The purrr:: package The purrr:: package was designed specifically with functional programming in mind. Similar to the discussion of vectorized operations above, purrr:: was created to help you apply functions to vectors in a way that is easy to implement and easy to “read”. 8.4.1 Function Mapping The map_ family of functions are the core of the purrr package. These functions are intended to map functions (i.e., to apply them) to individual elements in a vector (or data frames); the map_ functions are similar to functions like lapply() and vapply() from base R (but more versatile). “Mapping” a function onto a vector is a common theme of functional programming. To illustrate how the map_ functions work, its best to visualize the process first. knitr::include_graphics(&quot;./images/map_anno1.png&quot;) Figure 8.1: The map functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input. As shown above in Figure 8.1, the generic form of map() always returns a list object as the output. Working with lists is one of the mental hurdles to overcome when learning map(). However, there are variants in the map_ family of functions that return various object types. purrr_variants &lt;- tibble( name = c(&quot;map()&quot;, &quot;map_lgl()&quot;, &quot;map_int()&quot;, &quot;map_dbl()&quot;, &quot;map_chr()&quot;, &quot;map_dfr()&quot;, &quot;map_dfc()&quot;), returns = c(&quot;list&quot;, &quot;logical&quot;, &quot;integer&quot;, &quot;numeric&quot;, &quot;character&quot;, &quot;data frame, by rows&quot;, &quot;data frame, by columns&quot;) ) knitr::kable(purrr_variants, col.names = c(&quot;`Purrr::` Function&quot;, &quot;Object Returned&quot;)) Purrr:: Function Object Returned map() list map_lgl() logical map_int() integer map_dbl() numeric map_chr() character map_dfr() data frame, by rows map_dfc() data frame, by columns file_list &lt;- list.files(&#39;./data/purpleair/&#39;, full.names=TRUE) PA_data_merged &lt;- map_df(file_list, import.w.name) 8.4.2 Working with Lists 8.5 Exercises 8.6 Homework Write a function that takes a vector of numbers as input and outputs that vector sorted from smallest to largest value. Do this with only base R code and then with dplyr::arrange(). Hint: for the latter case you should look up the require() function. Modify the function import.w.name to include two additional steps: 1. Convert the character vector UTCDateTime into a DateTime class object. 2. Import the “date” part of the filename (in addition to the sensor ID) and create a new column variable with this information. Hint: provide regex pattern. "],
["transform.html", "Chapter 9 Transformations and other Tricks 9.1 Ch. 9 Objectives 9.2 Transformation 9.3 Normalization and Standardization 9.4 Stratification 9.5 Outliers and Censoring 9.6 Ch-9 Exercises 9.7 Ch-9 Homework", " Chapter 9 Transformations and other Tricks This chapter provides an introduction to several common techniques to organize, transform, and/or manipulate univariate data. I use the word manipulate here with good reason; many of these techniques can be used inappropriately or lead to inadvertent consequences, so tread carefully! That warning aside, these techniques are useful and often allow you to gain insight into “what the data are trying to tell you”. 9.1 Ch. 9 Objectives This Chapter is designed around the following learning objectives. Upon completing this Chapter, you should be able to: Define common approaches to data transformation Explain the differences between centering, rescaling, and standardizing univariate data. Stratify data into relevant groups for analysis Discuss the risks and potential approaches to handling outliers 9.2 Transformation The phrase “Data Transformation” means different things in different fields so we will define it here, in the statistical sense, as: applying one ore more mathematical operations to a variable (or in our case, all elements of numeric vector). You are undoubtedly familiar with data transformation, even if you haven’t previously recognized it. Some examples include: - adding, subtracting, or multiplying a constant to all elements in a vector; - taking the log, square root, or reciprocal of a variable; - some combination of these approaches. For example, your data are transformed whenever you convert temperature units from °F to °C, length units from inches to millimeters, or render a variable dimensionless (e.g., converting to units of percent, parts per million, or some other unitless value). In addition to these “units transformations”, we also conduct transformations to make the location and spread of data consistent, or to change the shape of a distribution of data (e.g., to reduce skewness or to linearize a variable). These latter approaches are often employed during multivariate analyses to help your variables “play nicely together” within the analytic framework you set forth. More on this to come. There are two key facets to data transformation: justification and documentation. Whenever data are transformed, you must provide a good reason for doing so (hint: there are many good reasons and also a few bad ones). Furthermore, you must adequately document “how” and “why” the data were transformed, so that others can understand your work and reproduce your results if needed. 9.2.1 Centering Centering means to subtract a constant from all values in a vector. The constant to be subtracted could be an estimate of the data’s central tendency, like the mean or median (hence the term center-ing), or it could be some residual value that you want to remove from all observations. If you subtract the mean from every value of a vector, then the new vector will have a mean of 0; this can be useful when analyzing multiple variables (for example in a multivariate regression model), each of which has a different location. Centering changes the location of a variable but spread stays the same. In engineering, the most common reason for centering data is not to subtract the mean or median value, but instead to “background subtract” or “zero” a set of univariate observations. If you have ever used a digital scale to measure the weight of an object, you’ve probably noticed that when you turn it on, the scale does not always read zero. Most of the time the instrument has a way to “re-zero” itself prior to use, but this is not always the case. If your scale shows a mass of 298 g when nothing is placed on it, then you probably want to (1) document that value before taking measurements and (2) subtract a value of 298 from all subsequent weights that you perform with this scale. That subtraction is an example of a “centering” transformation. Figure 9.1: Data are sometimes centered to remove spurious values 9.2.2 Rescaling To rescale a variable means to divide (or multiply) all values by a constant. Whereas centering is an arithmetic transformation, the process of rescaling data is multiplicative. Rescaling changes both the location and spread of the data. Some reasons to rescale data include: to show variables of different magnitudes together on the same axis of a plot; to make the spread of two or more variables similar (e.g., when conducting an analysis of variance or multivariate regression with variables that each have different scales); when you want to render a variable dimensionless (e.g., when converting to percentage units). 9.3 Normalization and Standardization In a generic sense, to “normalize” a variable means: to transform that variable so that it can be compared, evaluated, or comprehended more easily. Normalization often involves a combination of centering and rescaling operations that not only change the location and spread of the data, but also the meaning of data. Normalization is performed all the time. Like “transformation”, the word “normalization” is defined differently in different fields, so be explicit when using this term in your work. To some, the word normalization means to “make a variable normally distributed”; to others, it means to “normalize a database” so that redundancies are eliminated. There are many types of normalization operations; we will discuss a few examples below. 9.3.1 Rate normalization To perform a rate normalization is to divide one variable into another, often to report something in terms of a rate. Two cars can have the same range (how many miles they can travel on one tank of gas) but different rates of fuel efficiency (miles traveled per gallon) Two countries can have the same GDI (gross domestic income; the sum of all wages earned) but widely different rates of “per capita” income (GDI/total population size). 9.3.2 Standardizing Standardization is a special case of transformation/normalization where each value in a set of observations (or vector) is subtracted by some level (often the central tendency) and divided by the spread. When working with normally distributed data, the standardization of variable xi into zi is: \\(z_{i} = \\frac{x_{i}-\\overline{x}}{\\sigma_{x}}\\) where the mean and standard deviation of x are \\(\\overline{x}\\) and \\(\\sigma_{x}\\), respectively. The standardization of a variable renders the mean to 0 and the standard deviation to 1. This type of normalization is useful when you wish to study relationships between multiple variables, each with different scale. Note that standardization doesn’t have to use the mean and standard deviation as the centering and normalizing variables, but those are most common. 9.3.3 Reducing Skewness Sometimes your data are not normally distributed, even though you want them to be. In that case you still have options. Some transformations, like the log, square root, or inverse, will make the spread of the data symmetric and approximately Gaussian. These transformations are commonly used to meet the needs (read: underlying assumptions) of many statistical models. We discuss model assumptions (and their evaluation) in the modeling chapter. Figure 9.2: The log(x) transform is often used to reduce skewness in a variable. 9.4 Stratification To stratify a sample means: to divide the sample into subsets (aka strata). Stratified analyses can be performed many ways; one particularly useful way is graphically, by creating a stratification plot. A stratification plot is like any other visualization that you have created, except that the strata are identified (i.e., called out) through the use of color, lines, symbols, etc. Let’s use the following example to demonstrate: A manufacturing operation is investigating the production of an expensive titanium alloy, where a 25% increase in the production yield of the alloy manufacturing process means the difference between profit and loss. The materials group believes that sample purity should have an effect on the yield and for the past month, all three reactors at the plant have been producing alloys from Ti feedstock of varying purity. After one month, you decide to visualize the relationship between process yield and sample purity with a scatterplot, geom_point(). Figure 9.3: Yield vs. Purity from the plant’s three reactors over one month Examination of Figure 9.3 reveals that, while there is considerable variation in the data, a clear relationship is not apparent between sample purity and process yield. However, it occurs to you that perhaps there is variation in results between the three different reactors in the plant. Therefore, you decide to repeat this analysis stratifying the data by reactor type. In this case, you repeat the ggplot() call with an extra aesthetic of color = reactor in the scatterplot. Figure 9.4: Yield vs. Purity over one month, stratified by reactor type Figure 9.4 tells a very different story! Now we can see that two relationships are clearly evident. There is a definite inverse relationship between sample purity and the process yield and For a given purity level, each reactor has a different output. Clearly, there is something different about each of these reactors that merits further study… Figure 9.4 raises a subtle, but very important, point: data often have hierarchy and that hierarchy may be influential. In this case, only when we account for the hierarchy (the reactor that produced each sample), do we see an important relationship arise. Hierarchical data means data that can be ordered into different ranks (or levels). Students at a university might be ordered by their their college, their major, or the year in which they matriculated. Those ranks may be important when analyzing student data; if so, your analysis should account for that! 9.5 Outliers and Censoring Have you ever heard the phrase “don’t make the exception the rule”? I think this phrase means, don’t let your judgment be governed solely by outliers. Whether or not this is good advice probably depends on the situation, but I do think one should have the ability to detect when an outlier has leverage over a situation. An outlier is an observation that lies an abnormal distance from other values in a random sample. 9.5.1 Detecting Outliers Data visualization (like histograms, density plots, time-series, and boxplots) can be useful for detecting outliers, because such graphs make it clear that one or more observations “don’t seem to belong with the rest”. For example, let’s create an artificial dataframe called asthma.data. Within this dataframe are two variables: asthma.rate: the percentage of kids with asthma in a given school district black.carbon: the concentration of airborne black carbon (a type of air pollutant generated by incomplete combustion, similar to what you see exhausted from a diesel truck) measured outside of a school at the center of each district. asthma.data &lt;- tibble( asthma.rate = c(8, 6, 12, 18, 9, 8, 15, 14, 14, 10, 16, 9, 12, 15, 9), black.carbon = c(3.2, 2.5, 4.6, 6.1, 3.3, 3.1, 6.1, 5.3, 4.5, 3.3, 6.4, 3.7, 5.1, 6.8, 12) ) The plot below shows two boxplots: one depicts asthma.rate for the 15 school districts; the other depicts black.carbon. I’ve circled the apparent outlier. Outliers are easy to see in boxplots because they fall outside the whiskers. Figure 9.5: Boxplots for two sets of data; the one on the right has an outlier. Data visualization is useful for detecting outliers, but not definitive. Indeed, there are no definitive methods for detecting outliers in data (mostly because that process is too contextual). That said, there are some guidelines. The “1.5 x IQR” rule is what was used in the boxplot of Figure 9.5. This rule states that outliers are any data points more than 1.5 times the inter-quartile range away from the upper and lower quartiles. This means data that are: greater than: [0.75 quantile value] + 1.5*IQR less than: [0.25 quantile value] - 1.5*IQR For the black.carbon data, we could calculate these thresholds with the following: upper &lt;- quantile(x = asthma.data$black.carbon, probs = 0.75, names = FALSE) + 1.5*IQR(x = asthma.data$black.carbon) lower &lt;- quantile(x = asthma.data$black.carbon, probs = 0.25, names = FALSE) - 1.5*IQR(x = asthma.data$black.carbon) upper ## [1] 10.3 lower ## [1] -0.9 Interestingly, this rule suggests that outliers on the lower end need to be negative. Is that even possible for air pollution concentrations? (hint: NO) Other statistical approaches for detecting outliers include: a threshold of standard deviations away from the mean (i.e., computing z-scores); Grubbs’ test; Tietjen-Moore Test, and others. In the end, however, common sense and your contextual knowledge of the data are usually more important here. 9.5.2 Censoring outliers What should we do with outliers in our data? The answer depends on context (are these life and death data we’re dealing with or just the size of tomatoes from your garden?) and on the questions you are asking. Here’s an example: The black carbon boxplot (right side of 9.5) has a clear outlier that falls far outside of the rest of the data. This might not seem like a big deal but take a look at what happens when we fit a regression line through the data with the outlier included and without the outlier. Figure 9.6: Effect of an outlier on a linear model with n=15 data points. Examination of Figure 9.6 suggests that the outlier is having a strong effect on the relationship between air pollution and childhood asthma. If the censored data are correct, we have just detected a strong correlation between asthma rates and air pollution. If the uncensored data are correct then the association is positive but very weak. So how do we handle this apparent outlier? Unfortunately, this sort of conundrum happens more often than we might like in the real world. My advice would be to collect more data to support/discount the outlier. If that isn’t possible (and no other explanation was forthcoming), I would probably show the analysis both ways. Censoring data is dangerous business. If you are going to censor an outlier, make sure to document how you discovered/defined the outlier, why you believe it should be censored, and “whether or not that censoring had an effect on your results/conclusions”. Then make sure to broadcast your thinking to everyone who comes in contact with your analysis If you try to hide outliers, you are being unethical and setting yourself up for disaster. 9.6 Ch-9 Exercises 9.7 Ch-9 Homework "],
["measure.html", "Chapter 10 Measurement 10.1 Ch. 10 Objectives 10.2 Sampling 10.3 Figures of Merit 10.4 Ch-10 Exercises 10.5 CH-10 Homework", " Chapter 10 Measurement Measurement is a fundamental tool for characterization and verification in engineering (and many fields). This chapter will provide a quick introduction to the science of measurement. 10.1 Ch. 10 Objectives This Chapter is designed around the following learning objectives. Upon completing this Chapter, you should be able to: Explain the difference between a sample and a population; Define randomization and describe how it can be applied to simple, stratified, and cluster sampling; Explain the difference between cluster and stratified sampling methods; Describe how to estimate precision and bias for a measurement system Calculate uncertainty and describe how to propagate it across a measurement process Define limit of detection and estimate it for a measurement process 10.2 Sampling To conduct univariate analyses, we need data (i.e., observations). To get those data, we need a sample. To get that sample, we need to identify a population from which to sample. Allow me to digress… 10.2.1 Samples vs. Populations At some point in the history of science someone came up with the idea that very few things are known for certain: we can only make estimates or educated guesses. This idea (which is hard to refute) gave rise to the concept of samples vs. populations. In statistics, the population of an observed quantity or variable is the totality of every instance of that variable in the universe (…whoa…). Take the molecular diameter of an oxygen molecule. Even if we could measure the diameters of ALL the oxygen molecues in our atmosphere that would still represent only a tiny fraction (i.e., a sample) of all the oxygen molecules bound up in the planet (and all of Earth’s oxygen is but a tiny fraction of our solar system’s, and so on). The population of all oxygen molecules across the universe is impossible to measure or “know” given our current capabilities. If, however, we measure a sufficient sample of oxygen molecules, we could probably infer the size of all the other O2 molecules within reason. This is to say that the population is the thing we strive to make conclusions about but the sample is the thing (read: the data) that we have to work with. 10.2.2 Collecting a sample Samples are collected to provide estimates of populations so that those estimates can be used for inference (a fancy word that means: to make conclusions). Many statistical procedures are based on the assumption that your sample is representative of the population you are trying to model/understand. Assuming that your sample is good (read: representative) is a dangerous thing to do. In fact, MOST samples have some degree of un-good. Just ask the polling experts from the 2016 U.S. presidential election, or the now humiliated and vilified doctor who concluded that vaccines were the cause of childhood autism based on a sample of 12 kids taken from a single birthday party… A sample is representative if each measure is drawn in a way that makes it independent from the next AND if the measurement method itself is unbiased (i.e., the making of the measurement does not alter the way the sample is counted). We will talk more about bias later in this Chapter, but suffice it so say that if I made you watch a 10-minute video on catastrophic airplane disasters and then gave you a survey about your fears of flying, I might be introducing some bias into that survey measurement. Representative sampling is hard to do in practice. A good sampling strategy employs randomization: allow random chance to decide when and where measurements are made. The problem with this strategy, however, is that we have to place boundaries on the exact when and where of sampling due to limited resources. In reality, most samples are neither independent, nor randomly drawn; they are instead drawn from a single location, one after the next, for a short period of time. Such a convenience sample raises the possibility (1) that the data are non-independent (because any given measure is related [correlated] to the next one taken) and (2) that the sample lacks generalizability to the greater population (due to the focused [unique] nature of how/where the sample was collected. Scientific studies occasionally reach incorrect conclusions not because their statistical models were flawed, but instead because their sample was biased and/or unrepresentative of the true population. I can randomly survey 100 people at a political rally but that doesn’t mean the opinions I gather from these individuals are representative of all eligible voters in that part of the country. This is a much bigger problem than you might realize, because true random, unbiased sampling is often impossible or impractical in the real world. How would the Tesla Corporation collect a random, independent sample of 1000 Model 3 vehicles after 10 years of real-world driving? Even with a random number generator applied to vehicle ID numbers, Tesla no longer owns those cars. They can’t just take them back… They could ask car owers to complete a customer satisfaction survey but then they might only get results from people who want to complain…(bias) They could ask dealers to inspect cars but then they would be sampling only from the subset of owners that liked to do business with dealers…(potential for bias AND non-independence) They could ask repair shops for data but then they would only be sampling from cars that had broken down for some reason…(potential for bias and non-independence) So…if you cannot collect a truly representative sample, how can you trust your results? Suffice it to say that there is an art to empirical research (empirical is a fancy word for observational). If you study how data were collected (especially from a sampling strategy) you will often realize how a “wrong conclusion” came to be. Figure 10.1: Biased samples are easy to come by Hope is a dangerous word in science because it can create implicit bias. The experimenter who hopes for a certain outcome might unknowingly influence their sampling and/or analysis strategy to promote that outcome. The best experiments are designed as “win/win”: they produce valuable information, regardless of the outcome (yes/no, null/alternative hypothesis, etc.). Such designs, though often difficult to conceive, remove the need for hope altogether. Let’s review a few common sampling strategies here. 10.2.3 Simple Random Sampling Simple random sampling means that samples are randomly collected from across the population and over a period of time where the population characteristics (especially those characteristics being measured) remain constant. The main advantage of simple random sampling is that, when done correctly, your data should be unbiased and representative of the population. The main issue with simple random sampling is that it can be hard to do “correctly”. To collect a simple random sample, you need to ensure three things: that your sampling method gives every single member of the population an equal chance of being selected. that your population remains relatively unchanged during the sampling window. that your sample size is sufficient to make inferences (read: conclusions) with confidence. How large of a sample size do you need to collect? That depends on a lots of things: what you want to do with the data (read: what conclusions/questions are you after?) Are you estimating location and/or spread, are you comparing location/spread, or are you fitting a model? What are the risks/consequences of making incorrect conclusions? Do you need 95% confidence in your answer, 99%, 99.999%? How much variability do you expect to see in your data? These factors all work together to frame sample size considerations. There are many ways to estimate sample size (too many for this course), so I will use only one here: sample size (n) for a t-test comparing two means: \\(n = \\frac{(Z_{\\alpha/2}\\cdot+Z_{\\beta/2})^2\\cdot2\\cdot\\sigma^2}{d^2}\\) where, \\(\\alpha\\) represents the Type-1 error rate (1 - confidence level); 0.05 is typical \\(\\beta\\) represents the Type-2 error rate (1 - power); 0.2 is typical \\(Z_{\\alpha/2}\\) represents the critical value of the Normal distribution at \\(\\alpha/2\\) \\(Z_{\\beta/2}\\) represents the critical value of the Normal distribution at \\(\\beta/2\\) \\(\\sigma^2\\) is the population variance (you often need to guess/estimate \\(\\sigma^2\\)) \\(d\\) is the difference (in means) that you are trying to detect 10.2.4 Systematic Sampling Systematic sampling is often employed when simple random sampling is not feasible. With systematic sampling, the population is first ordered according to a characteristic of interest (e.g., first to last, front to back, smallest to largest, youngest to oldest, etc.) and then samples are drawn at regular intervals from across the ordered range of values. For example, if you wanted to know the opinions of marathoners at the finish line of a particular race, you might decide to randomly sample one person passing through the finish line every five minutes starting with the winner’s group and ending with the last finisher. The main disadvantage to systematic sampling is the creation of an ordering scheme, which can introduce bias into the results (whether known or unknown). For example, in the NBA’s annual 3-point shooting contest, every 5th ball is worth extra points to the shooter; if we conducted systematic sampling of every 5th shot, we would only select the shots worth extra points (i.e., the shots that the shooter cares the most about). 10.2.5 Stratified Sampling Stratified sampling is often used when (1) you know “something” (a priori) about the population that you want to sutdy (or at least account for in your sample) and (2) when that “something” can be mapped onto the population to form groups. With stratified sampling, you break the population into strata (a fancy word for groups) and then sample randomly (in equal proportion) from within each strata. This technique is used because the strata are believed to be important relative to the the variables of interest. The strata can be time-based (e.g., the quality of parts produced during the morning, afternoon, and night shift at a factory), place-based (e.g., the quality of parts produced at assembly plant A, B, C), or based on other known characteristics. For example, when people are the subject of study, strata are often formed about age groups, gender groups, income brackets, and educational attainment, to name a few. Stratified random sampling can be more tedious to conduct and is sometimes biased if not all of the subgroups within a population are identified. For example, a survey of the effects of housing on health (stratified into homeowners vs. renters) would ignore a key subgroup: those who are experiencing homelessness. 10.2.6 Cluster Sampling Cluster sampling is often used to save on time and effort during a sampling campaign when it’s possible to break the population into similar groups, or clusters. The population is divided into clusters (based on some grouping characteristic) and then a proportion of those clusters are fully sampled. For example, if your company had 100 distribution centers across the country, and you wished to study some measure of process efficiency, then you might select 10 centers as clusters, and fully sample each. Cluster sampling is considered less precise (i.e., more samples needed for a given question) than simple random sampling, but oftentimes its unfeasible or too costly to sample the whole population. If cluster units are apparent, they can provide considerable cost savings. The important difference between stratified and cluster sampling is how each design defines the groups. With stratified sampling, the distinction between groups (i.e., how the data varies from one group to the next) is important; with cluster sampling, the distinction between groups should be one of convenience (and hopefully not meaningful as it relates to data variability). 10.3 Figures of Merit All measurements are estimates of a “true value” that you seek to discover. To ensure that measurements are sound, we often estimate the figures of merit for a measurement system (or an instrument). There are many different figures of merit; we will only examine a few here, namely: limit of detection, dynamic range, precision, and bias. Other figures of merit for measurement systems include things like linearity, sensitivity, and specificity - those topics will be covered in the advanced version of this course (which doesn’t yet exist). 10.3.1 Limit of Detection &amp; Quantification Imagine that I asked you to hold out your hand and close your eyes. Could you tell if I added a single grain of flour to your hand? Likely not. The mass of that grain is about 1/50th that of an eyelash. What if I added 100 grains? Maybe you could tell. If I added 1000 grains you could likely feel it. This imaginary exercise outlines the concept of “limit of detection”, or “LOD”, which represents the threshold at which an instrument can distinguish between “something” and “nothing” being measured. The concept of LOD applies best to instruments that measure continuous properties like time, distance, energy, force, etc. There are several approaches to estimating an instrument’s lower limit of detection. The simplest approach is first to challenge that instrument to measure increasingly more “stuff” and observe it’s response. This is shown graphically below with “instrument response” on the y-axis and “amount of stuff being measured” on the x-axis. We use a logarithmic scale for the x axis because when we measure “stuff” in engineering we usually have the ability to observe several orders of magnitude (the length of a baseball stitch, a baseball, a baseball field, or a baseball stadium). Many instruments respond with an “S” shaped curve where you initially see no (or asymptotic response), followed by a linear response, followed by another asymptotic tail at the upper detection limit. Figure 10.2: Experimental investigations into LOD and dynamic range To calculate the lower LOD, we often measure a series of “blanks” and then calculate the mean and standard deviation about those data. What is a blank? A blank is an attempt to challenge the instrument to measure nothing from something. Let’s say you had a scale to weigh the mass of cherries picked at an orchard. Each load of cherries is weighed in a small paper bag (so that they don’t spill everywhere). Then weighing the empty paper bag would be considered a “blank” for that instrument. 10.3.2 Dynamic Range The LOD/LOQ concept is most often applied to an instrument’s minimum detectable quantity, but it’s important to realize that instruments also have a maximum detectable quantities, too. The dynamic range of an instrument, thus, describes the range of values (min/max) that an instrument can measure. It’s important to know an instrument’s dynamic range so that you don’t misuse it. For example, you wouldn’t use the scale in a doctor’s office to weigh grains of rice just as you wouldn’t ask human beings to report on the loudness of dog whistles. The dynamic range of an instrument represents the measurement range between a lower and upper detection limit. 10.3.3 Measurement Precision In measurement science the term precision means “repeatability” or “instrument variability”. A precise measurement (made by a single instrument) is one that provides the same (or nearly the same) answer each time you take the measurement. This assumes, of course, that the thing being measured remains unchanged over time. Human vision is something that suffers from imprecision, especially as you get older (your eyesight usually gets worse). Look at the image below quickly and try to decode the letters you see. If I asked you to identify this text quickly (in 10 seconds or less) once per day for a week, you might give me 7 different answers. That’s not a precise measurement. Figure 10.3: Blurry vision is imprecise: repeated measures could lead to different answers Different authorities have varying opinions about the best way to quantify precision. A simple, “top-down” approach is to take repeated measures of something (for which that instrument is designed to measure) several times over and then report the spread of those data (often as a standard deviation, but range, IQR, and 95% or 99% confidence intervals are also used). Alternatively, a “bottom-up” approach is to propagate the errors associated with each facet of the measurement. As an empiricist (a fancy word for experimental-type), I prefer to gather data, so I often employ the “top-down” approach to precision. There are dangers to this approach however; these dangers are a direct result of the sampling strategy used to make repeated measures (see the following section for a more in-depth analysis). In the case of a temperature sensor, we might boil a large pot of water and repeatedly measure the temperature with a single probe (here in Fort Collins, Colorado, water boils at ~ 94 °C). Those measurements are shown below: temperatures &lt;- tibble( data_seq = seq(from = 1, to = 25, by =1), data_temp = c(93.4, 93.7, 96.0, 93.1, 93.8, 94.2, 93.1, 94.6, 93.8, 95.1, 93.1, 94.6, 94.7, 93.6, 92.8, 95.7, 94.0, 93.0, 93.7, 94.6, 94.4, 94.1, 94.2, 92.3, 94.4)) sd(temperatures$data_temp) %&gt;% round(2) ## [1] 0.88 Let’s create a 4-plot of those data to visualize basic characteristics. p1 &lt;- ggplot(data = temperatures) + geom_line(aes(x=data_seq, y=data_temp), color = &quot;grey&quot;) + geom_point(aes(x=data_seq, y=data_temp), size = 2, shape = 1) + xlab(&quot;Sequence Order&quot;) + ylab(&quot;Temperature, °C&quot;) + theme_classic() p2 &lt;- ggplot(data = temperatures) + stat_ecdf(aes(x=data_temp)) + xlab(&quot;Temperature, °C&quot;) + ylab(&quot;Quantile&quot;) + theme_classic() p3 &lt;- ggplot(data = temperatures) + geom_histogram(aes(x = data_temp), bins = 6, color = &quot;white&quot;) + xlab(&quot;Temperature, °C&quot;) + ylab(&quot;Frequency&quot;) + theme_classic() p4 &lt;- ggAcf(x = temperatures$data_temp) + theme_classic() + ggtitle(label = NULL) grid.arrange(p1, p2, p3, p4) Examination of the 4-plot reveals: no major shifts or events in the time series; an approximate normal distribution (but a small sample size so hard to tell for sure); values centered on 94 (spread from 92-96); no apparent autocorrelation in the data. These results are all “good”, as we are expecting the data to be tightly clustered together and somewhat boring. We are now ready to estimate the precision of these repeated measurements. Since the data are approximate normal, a mean and standard deviation seem appropriate. Mean: 94 °C Std Dev: 0.9 °C Thus, we report a precision of ± 0.9 °C around a value of 94 °C. Note that we report precision about a value (0.9 °C in this case), since our data are limited to a certain range. In other words, we don’t know if our estimate of precision is applicable to 0 or 200 °C conditions. Precision is often reported in relative terms, too, as a percentage about the location where the measurements were made. For the example above, we would report \\(\\frac{\\widehat{\\sigma}}{\\widehat{\\mu}}\\cdot100\\), which R calculates as 0.9574468, but which we would report as ±1% at 94 °C. If you measure temperature with a k-type thermocouple and report a value of 93.28876 °C, you are admitting to the world your ignorance of precision (and the science of temperature measurement). No thermocouple reading is repeatable to 5 decimal places in °C; the best thermocouples are accurate to one decimal degree. Always make sure to align your measurement precision with the significant digits you report. 10.3.4 Measurement Bias (accuracy) The word bias has several meanings depending on the field of use (e.g., a biased opinion, a racial or gender bias, or a biased voltage). Here, we define bias as the quantifiable difference between a measurement and a true value. The terms bias and accuracy are closely related, but we prefer to use bias because that word implies a magnitude and direction, whereas accuracy is more qualitative. Precision and bias are easy to view graphically. In Figure 10.4 I show the performance of four measurement systems. Each image shows a green circle in space that represents a “true value” that we are trying to measure. Each measurement attempt is depicted by an “x”. If a measurement system (i.e., an instrument) is precise and unbiased, then each of the repeated measures would show up in the green circle, because they are close to the “true value” and to each other. This is the case in the lower right panel. The upper-left panel, on the other hand, demonstrate a biased and imprecise instrument - biased because the average of the 20 measurements would not come close to the center and imprecise because the location of each individual measurement tends to vary by a distance larger than the size of the circle. The lower-left is unbiased but imprecise - unbiased because the average of the 20 measurements would land close to the center of the circle but imprecise because the measurements vary widely. The upper-right panel demonstrates a precise (the repeated measures do not vary much) but biased (“off the mark” from center) measurement system. Figure 10.4: Examples of different combinations of bias and precision Like precision, 10.3.5 Measurement Uncertainty Bias and precision, together, make up uncertainty. A good way to estimate the uncertainty around a measurement is through error propagation. 10.4 Ch-10 Exercises 10.5 CH-10 Homework "],
["model.html", "Chapter 11 Modeling 11.1 Ch. 11 Objectives 11.2 Process Modeling 11.3 Model Fitting 11.4 OLS Regression 11.5 Example: OLS Linear Regression 11.6 Calibration 11.7 Ch-11 Exercises 11.8 Ch-11 Homework", " Chapter 11 Modeling A model is a system of postulates, data, and inferences presented as a mathematical description of an entity or state of affairs. In this chapter we will discuss how models are conceptualized and “fit” to represent the data we have available. Models are useful because they provide a mathematical basis for how we see the world (i.e., an explanation for the observational data we collect). This means that models can help you: quantify patterns or trends in your data; extrapolate your data (i.e., to guess an outcome outside the range of observed values); interpolate your data (i.e., to guess an outcome within the range of observed values but not at a location where data has been observed) predict future outcomes; conect theory (what you think should happen) to observation (what you actually see happen); “All models are wrong, but some are useful” - George Cox, pioneering statistician 11.1 Ch. 11 Objectives This chapter is designed around the following learning objectives. Upon completing this chapter, you should be able to: Describe the steps of model conceptualization, fitting, evaluation, and validation. Develop and fit linear models to continuous, bi-variate data Evaluate the assumptions of linear regression with visual diagnostics Apply transformation techniques to meet linear model assumptions Describe the process of parameter estimation using ordinary and weighted least squares Estimate probability density functions for univariate distributions 11.2 Process Modeling Figure 11.1 below provides a general process diagram for modeling (adapted from the NIST Handbook on Statistics); these are the steps you should follow when developing a model. Even before you begin this process, however, you should ask yourself the following three questions: Why am I developing this model? What do I hope to learn from this model? What do I plan to do with this model? The answer to these questions will undoubtedly guide your thinking as you move through the process steps; it’s important to have objectives and expectations at the start. Without purpose, a process model often comes up short. Figure 11.1: A generic process diagram for model conceptualization, development, and evaluation. A couple points worth making about the process outline in Figure 11.1. First, observing a process and matching your observations to your training and experience is an invaluable tool for model construction. Second, most models of “real-world things” like Newton’s work on gravity or Milliken’s model of elctron charge are empircal - that is, they are based on observation (not pure math). Purely analytic models exist (mostly as mathematical “proofs”) and such models don’t need data to be “fit” but do need data to be validated. Third, many people choose to validate their model with the same data they used to fit the model (using an approach like blocking or cross-validation). While I acknowledge that this approach is convenient (and sometimes the means available), I disagree with it. You should always try to validate your model with an independent sample (better yet, with a sample that you didn’t collect). Why? Although we put a lot of emphasis on “random sampling”, we rarely succeed at it in the real world. Having someone else validate your model with their (independent) data will go a long way towards convincing others that your model is both useful and generalizable. 11.2.1 Assumptions An assumption is a fact or condition that is taken for granted. If you think about it, nearly every action you undertake carries assumptions. If someone throws a baseball to you and you reach out your hand to catch it, you are assuming that you will indeed make that catch. Otherwise, you would probably duck! When you walk across a high bridge, you undoubtedly assume that it will hold your weight. Sure, you might have a compelling reason to cross the bridge at any cost, but in that case you have assumed that the reason for crossing the bridge outweighs other risks. Those of us who live in Colorado have, at one point, made the incorrect assumption about our car or truck’s braking ability in the snow. All scientific models are based on assumptions; many of those assumptions are necessary (read: critical) for the model or theory to be “correct”. Although assumptions can be taken for granted when constructing a model or theory, that doesn’t mean they should be ignored. Indeed, evaluating the assumptions of a model (or theory) is one of the first things that you should do when developing OR evaluating OR applying it. Testing assumptions is the hallmark of a talented researcher, and yet, few researchers test their model assumptions regularly. In this chapter, I will teach you about the assumptions of linear regression, but the take-home lesson is this: those who recognize and validate their assumptions will be rewarded for their efforts, prepared for what is to come, and will rarely depend on luck for success. 11.3 Model Fitting “The purpose of models is not to fit the data but to sharpen the questions.” - Samuel Karlim, mathematician and genomicist The best models are those that advance your understanding of a phenomenon so well that you soon leave the model behind, because the questions have since changed. We will not aspire to such great heights here, but it’s worth understanding how models can be fit to data. Model construction, fitting, and validation could represent an entire semester (or more) of work. Here, will will discuss only two introductory applications: fitting of a linear model (using ordinary least squares) and basic distribution fits (using LOESS approaches). You have undoubtedly used a computer program to create a linear fit between an X and Y variable. Microsoft Excel will do this for you with two columns of data and a few simple mouse clicks. How that fit is achieved (and what assumptions underpin that fit) is what we will discuss here. 11.4 OLS Regression OLS stands for “Ordinary Least Squares”. The OLS technique is one of the most commons ways to fit a linear regression. To keep things simple, let’s discuss fitting a model with a single independent variable (\\(X\\)) as a predictor for our (dependent) outcome variable of interest (\\(Y\\)). A model with one instance of a single independent variable means we have, at maximum, only two parameter estimates to fit: the intercept term when \\(X = 0\\) (which we call \\(\\beta_{0}\\)) and the slope term for all values of \\(X\\) (which we call \\(\\beta_{1}\\)). Thus, we are fitting a straight-line equation between two variables with the following notation: \\[Y = \\beta_{0} + \\beta_{1}\\cdot X + \\epsilon\\] where: \\(Y\\) = the dependent variable (i.e, the outcome - what we are trying to model) \\(\\beta_{0}\\) = the intercept parameter (to be fit by OLS model) \\(\\beta_{1}\\) = the slope parameter (to be fit by the OLS model) \\(X\\) = the independent variable (i.e., the predictor variable) \\(\\epsilon\\) = the error term (i.e., what is left over; what the model doesn’t explain about \\(Y\\)) The OLS approach is straightforward: select model parameters (\\(\\beta_{0}\\), \\(\\beta_{1}\\)) so that the model produces as little error as possible. With OLS, the model error is calculated as a sum-of-squares error (SSE; explained below). Graphically, this is shown in Figure 11.2, where the solid blue circles represent the actual data (\\(X_{i}, Y_{i}\\)). The grey line represents the “best fit” line that gives the smallest SSE possible. The model residuals (what are used to calculate the SSE) are denoted by vertical lines connecting the data points to the “best-fit line”. The optimization algorithm is executed with matrix algebra. Figure 11.2: Graphical depiction for an OLS regression fit to minimize the sum of squared residuals The Y values predicted by the model are denoted as \\(\\hat{Y_{i}}\\) (we say “Y hat”). Mathematically, each residual (\\(\\hat{\\epsilon}\\)) is defined as: \\[\\hat{\\epsilon_{i}} = Y_{i} - \\hat{Y_{i}}\\] where: \\(\\hat{\\epsilon_{i}}\\) is the residual for each (ith) observation \\(Y_{i}\\) represents each (ith) observation of the dependent variable, \\(Y\\) \\(\\hat{Y_{i}}\\) represent the predicted value of \\(Y_{i}\\) for each data point: \\(\\hat{Y_{i}} = \\beta_{0} + \\beta_{1}\\cdot X_{i}\\) We use vertical distance to define residuals because we are trying to predict the \\(Y\\) values, so we care only about how far away (in \\(Y\\) space) our predictors, \\(\\hat{Y_{i}}\\), are relative to “true” \\(Y_{i}\\) data. Another way to say this is that OLS assumes that our \\(X_{i}\\)’s are perfect observations (\\(X\\) is measured without error). The sum of squares error (SSE) is then: \\[SSE = \\sum_{i = 1}^{n} \\left(\\hat{\\epsilon_{i}}\\right)^{2}\\] for all \\(n\\) observations in the data set. The SSE is a really useful term; it represents the overall variance in the \\(Y\\)-data that was explained by the model. Good models explain variance in your data, because variance means: changes in behavior of your data. The overall model fit, \\(R^{2}\\), is closely related to the SSE term. Once you have SSE, you calculate the total sum of squares (TSS) as: \\[TSS = \\sum_{i = 1}^{n} \\left(Y_{i} - \\overline{Y} \\right)\\] and then \\(R^{2}\\) is calculated from their ratio: \\[R^2 = 1- \\left(\\frac{SSE}{TSS}\\right)\\] The total sum of squares represents a measure of all of the variability in the data; the residual sum of squares represents a measure of the variability that remains after the linear model has been applied. Thus, the \\(R^{2}\\) term represents the proportion of variability in your data that was explained by the linear model. The terms “residual sum of squares” and “error sum of squares” are interchangeable; you will see them both in the wild and they mean the same thing. 11.4.1 OLS Assumptions and Diagnostics Now that you have an idea “how” OLS works, let’s talk about the assumptions of OLS linear models. I will break the assumptions of OLS linear regression into two groups: front-end and back-end. The front-end assumptions are ones that apply to the model and the data at the outset. Most of the front-end assumptions can be tested before you fit the model. The back-end assumptions are ones that you test after the model has been built - these assumptions apply to the error term (\\(\\epsilon\\)). I will write them out here and provide a brief explanation of how to evaluate each (and why you should care about doing so). 11.4.1.1 Front-end assumptions: The form of the model is linear in the coefficients. Linear in the coefficients means that the parameter estimates (\\(\\beta_{0}\\), \\(\\beta_{1}\\),…\\(\\beta_{n}\\)) all show up in the final model as being added together. An important point here is that the model variables do not need to be linear terms. You can use linear regression with non-linear predictor variables, such that the following model would be considered “linear” in the parameters: \\(Y = \\beta_{0} + \\beta_{1}\\cdot{X_{1}^{2}} + \\beta_{2}\\cdot{X_{2}^{3}} + \\epsilon\\). This model is still “linear” because the \\(\\beta\\)-terms are being added together. How to check: It’s easy to tell if your model is linear in the coefficients by just looking at it, since this is a structural requirement. If the \\(\\beta\\)-terms show up in non-arithmetic ways (e.g., \\(\\cos(\\beta_{1})\\cdot X^{\\beta_{2}}\\)) then you are developing a non-linear model (signifying that it’s time to take a statistics course). Why you care: You model will not run (at least not correctly) if not set up correctly. Attempting to fit an OLS linear fit on a non-linear model is bad news. The model is correctly specified. This assumption speaks to the actual variables included in the model and their form. How many independent variables are needed and to what power do you raise each (if not to the first power)? Is the function \\(Y = f(X_{1},X_{2})\\) appropriate or is \\(log(Y) = f(X_{1}^{2})\\) or \\(Y = f(\\frac{X_{1}}{X_{2}^{2}})\\)? How to check: This is a tough assumption to evaluate in advance. The best way to specify the model correctly is through a combination of (1) checking the other assumptions, (2) using your experience/process knowledge to inform the model design, (3) conducting an exploratory data analysis to inform your thinking and (4) following an established model selection procedure (this last one also suggests that you to take a formal class on statistics). Why you care: If the model is not specified correctly it might perform poorly under validation, or, it might suggest that a variable is not important, when in fact it is! No collinearity between predictor variables. The no collinearity assumption only applies when your model has more than one predictor variable (\\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\), etc.). An example would be using waist size (as \\(X_{1}\\)) and body mass (as \\(X_{2}\\)) to predict a person’s height (\\(Y\\)). The two predictor variables (waist size and mass) are likely to be strongly correlated (more on that topic below). How to check: Run a correlation analysis on pairs of the independent variables (i.e., calculate a Pearson or Spearman correlation coefficient) and create a correlation plot matrix (a panel of scatterplots) for all pairs of independent variables to examine how they are correlated. Why you care: When two or more predictor variables are highly correlated, the model will struggle to fit parameter estimates (e.g., \\(\\beta_{1}\\), \\(\\beta_{2}\\)) for the correlated variables (read: the parameters for the correlated variables will become very sensitive to small changes in the model, which is NOT good). Note that if you don’t care about individual parameters (you only care about the model output) then violating this assumption is less important. If you do care about parameter estimates for individual predictors, the model could very well assign the wrong sign to one of the \\(\\beta\\) coefficients. 11.4.1.2 Back-end assumptions: The error term (4) has a mean of zero, is (5) normally distributed, (6) homoscedastic, and has (7) no autocorrelation or (8) correlation with predictor (independent) variables. There’s a lot riding on that little error term, \\(\\epsilon\\)!. Mean of residuals is zero. The first requirement for a mean of zero (\\(\\overline{\\epsilon} = 0\\)) will be fulfilled if the model is set up and the OLS algorithm is implemented correctly. How to check: calculate the model residuals and take their mean(). Why you care: if \\(\\overline{\\epsilon} \\neq 0\\), or if \\(\\overline{\\epsilon}\\) is not very close to zero, then something has gone seriously wrong with your model implementation. Residuals are normally distributed. The general opinion among statisticians is that non-normal residuals are not a deal-breaker for your model. Non-normal residuals will affect your ability to generate confidence intervals about your estimates, but the estimates themselves should still be “BLUE”, which stands for “Best Linear Unbiased Estimator”. How to check: create a Q-Q plot about your residuals. Why you care: violation of the normality assumption can affect the precision of your estimates. Residuals are homoscedstic (not heteroscedastic). The term “homoscedastic” means to have equal variance. In the case of residuals, we want to see that their magnitude remains relatively constant as the dependent variable increases, \\(Y\\). How to check: create a scatterplot of residuals (y-axis) vs. the fitted values, \\(\\hat{Y_{i}}\\) (x-axis; yes, I know that sounds weird to plot the Y-variable on the x-axis…). The residuals should appear evenly scattered in both directions about zero with no apparent change in magnitude as the Y variable increases. Why you care: Heteroscedasticity (one of my favorite “don’t I sound smart?” words) can affect the precision of your estimates and can also inflate your confidence in your results (read: you might think you are on to something when actually, you aren’t). If your residuals are heteroscedastic, consider applying a transformation to your dependent variable to normalize the error variance. The BoxCox method one of my favorites. No residual autocorrelation. Autocorrelation among residuals is a strong indicator that your model is not correctly specified. How to check: Create autocorrelation and partial autocorrelation plots for your residuals. Perform a Durbin-Watson test. Why you care: Autocorrelation among residuals is a strong indicator that your model is not correctly specified. No residual correlation with independent variables. Likewise, residual correlation with independent variables is a strong indicator that your model is not correctly specified. How to check: Conduct a correlation analysis on your residuals and independent variables, create scatterplots to investigate the nature of the correlation (when present). Why you care: Correlation between residuals and independent variables is a strong indicator that your model is not correctly specified. You can do better! 11.5 Example: OLS Linear Regression Let’s conduct a simple linear regression with two variables that we know are correlated: a person’s waist size and their weight. We will use data collected by the US Centers for Disease Control as part of the National Health and Nutrition Examination Survey (NHANES) - a detailed annual survey of ~5,000 people living in the US. 1. Specify the model Before we begin let’s “look” at the data and apply some process knowledge. A scatterplot of waist size vs. body mass is shown below. Figure 11.3: Scatterplot of body mass (kg) vs. waist circumfrence (cm) for US adults. Clearly, a strong relationship exists between the two variables - one that we could probably approximate as linear. However, as engineers who have studied physics, we are taught that the mass of an object is the product of density, \\(\\rho\\), and volume, \\(V\\): \\[mass = \\rho\\cdot V\\] Human beings are mostly water, so we can assume that density is a constant from one person to the next. Our “body volume” is what changes. If we approximate our bodies as cylinders, then the volume (and mass) of our bodies is linearly related to the product of cross-sectional area and height. \\[mass = \\rho\\cdot Area \\cdot Height\\] However, we are modeling mass against waist circumference (not area), so our process knowledge tells us that waist size and body mass should NOT follow a linear relationship. \\[Area = \\frac{Circumference^{2}}{2\\pi}\\] Thus, substituting one equation into another, we arrive at the conclusion that mass should be proportional to the square of waist size. \\[mass \\sim Circumference^{2}\\] Or, another way to say this is that the square root of mass is linearly related to body circumference. \\[\\sqrt{mass} \\sim Circumference\\] Let’s transform \\(mass \\rightarrow \\sqrt{mass}\\) and then examine the two scatterplots side by side. Figure 11.4: Side-by-side comparison of two model specifications. Does one look more linear than the other? Looking at the right plot in Figure 11.4, where \\(\\sqrt(mass)\\) is plotted, helps make the slight curvature in the left-side plot more evident. Both plots appear “mostly” linear but the one on the right “looks better”, at least to me! Our process knowledge suggests that the model on the right is better specified. Let’s fit both models and find out! The NHANES data we use here is from 2017-2018 (saved as bodysize.csv and read into a data frame labeled: data_18). These data have been filtered to include adults only (filter(age &gt; 18)). The first few lines of data look like this: head(data_18) ## # A tibble: 6 x 7 ## id mass waist height gender age sqrt_mass ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 93705 79.5 102. 158. 2 66 8.92 ## 2 93706 66.3 79.3 176. 1 18 8.14 ## 3 93708 53.5 88.2 150. 2 66 7.31 ## 4 93709 88.8 113 151. 2 75 9.42 ## 5 93711 62.1 86.6 171. 1 56 7.88 ## 6 93712 58.9 72 173. 1 18 7.67 11.5.1 The lm() function The lm() function in R stands for “linear model” and will perform an OLS fit on any linear model you specify. The function requires two arguments: a formula (i.e., the model specification) and the data with which to fit that model. Formulas are specified as y ~ x, so our call to lm() looks like this for each model: model1 &lt;- lm(mass ~ waist, data = data_18) model2 &lt;- lm(sqrt_mass ~ waist, data = data_18) Now that we have stored each model as an object (model1, model2), so we can examine what they contain. The output of lm() is a list of class “lm”. If we type view(model1) the contents of the list become apparent. Figure 11.5: The lm() function provides a self-contained list of the model, data frame, parameter estimates, residuals, and more. As you can see, there is a wealth of information contained in the lm() object. The first list entry contains the model parameter estimates (model1$coefficients) in rank order: \\(\\beta_{0}\\) (model intercept): -34.37 \\(\\beta_{1}\\) (slope, waist coefficient): 1.16 These are the primary outputs of the OLS fit; they define the line of “best fit” for which the SSE have been minimized. To examine the overall fit of the model, we can call the summary() function with the model object as an argument. Here is the summary for model1: summary(model1) ## ## Call: ## lm(formula = mass ~ waist, data = data_18) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.810 -6.721 -0.231 6.354 52.621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -34.371523 0.816004 -42.12 &lt;2e-16 *** ## waist 1.164661 0.008029 145.05 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.948 on 5178 degrees of freedom ## (353 observations deleted due to missingness) ## Multiple R-squared: 0.8025, Adjusted R-squared: 0.8025 ## F-statistic: 2.104e+04 on 1 and 5178 DF, p-value: &lt; 2.2e-16 and here is the summary for model2: summary(model2) ## ## Call: ## lm(formula = sqrt_mass ~ waist, data = data_18) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.73665 -0.35271 -0.00434 0.35029 2.27134 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.7417507 0.0430251 63.72 &lt;2e-16 *** ## waist 0.0624005 0.0004233 147.40 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5245 on 5178 degrees of freedom ## (353 observations deleted due to missingness) ## Multiple R-squared: 0.8075, Adjusted R-squared: 0.8075 ## F-statistic: 2.173e+04 on 1 and 5178 DF, p-value: &lt; 2.2e-16 Interestingly, both models have nearly equal fits to the data. The \\(R^{2}\\) for model1 is 0.8025 and the \\(R^{2}\\) for model2 is 0.8075 (a negligible difference in my opinion). This brings up an interesting question: which model is better? The answer to that question depends, of course, on (1) what you want to learn from the model, (2) what you plan to do with the model? Sound familiar? Before getting to those questions, let’s continue with the modeling process by evaluating our assumptions… 11.5.2 OLS Diagnostics In this section, we will run through our list of OLS assumptions for each of the models to prove that our models are proper and to help us decide which is better. Assumption #1: The form of the model is linear in the coefficients. This one is easy to answer because both models were set up with only one independent variable and an intercept term: \\(Y = \\beta_{0} + \\beta_{1}\\cdot X\\). The \\(\\beta\\)’s are added together so this assumption is valid for both model1 and model2. Assumption #2: The model is correctly specified. This one is hard to answer (right now) but our process knowledge (the physics of volume~mass relationships) tells us that model2 is more representative of reality than model1. Advantage: model1. Assumption #3: No collinearity between predictor variables. This assumption is automatically valid because we have only one independent predictor variable in each model, ruling out any possibility of collinearity. Assumption #4: The error term has a mean of zero. We can calculate this from our two model objects. mean(model1$residuals) ## [1] -6.665725e-17 mean(model2$residuals) ## [1] 6.226994e-19 Those are pretty small numbers. Check and check. Assumption #5: The error term is normally distributed This assumption can be checked several ways, but my preference is to create a Q-Q plot about the residuals (the Q’s stand for “quantile-quantile”). A Q-Q plot is a scatterplot that relates the quantile values of your data against the quantiles of an “expected distribution” - in this case the normal distribution. If “your data” follow the shape of the “expected data” then both datasets can be said to follow the same distribution. Thus, when you plot residuals on a “normal Q-Q plot”, you are looking for your data to fall along a straight line through the center. The {ggplot} package allow you to create Q-Q plots using geom_qq() function, which creates the plot, and geom_qq_line(), which adds the “expected” line to aid your eye. The {stats} package in base R also allows you to do this with the qqnorm() and the qqline() functions. Here are the Q-Q plots for model1 and model2. p3 &lt;- ggplot(data = model1$model, aes(sample = model1$residuals)) + geom_qq(alpha = 0.1, color = &quot;maroon4&quot;) + geom_qq_line(color = &quot;grey&quot;) + ggtitle(&quot;Model 1: mass ~ waist&quot;) + theme_classic() p4 &lt;- ggplot(data = model2$model, aes(sample = model2$residuals)) + geom_qq(alpha = 0.1, color = &quot;royalblue2&quot;) + geom_qq_line(color = &quot;grey&quot;) + ggtitle(&quot;Model 2: sqrt(mass) ~ waist&quot;) + theme_classic() grid.arrange(p3, p4, ncol = 2) Figure 11.6: Q-Q plots of residuals for two linear models. Which set of residuals better approximates a normal distribution? Both models reasonable follow the expected quantile plots, although model1 shows more deviation from normality at the upper tail. If I hadn’t seen model2, which looks nearly perfect, I probably would have said that model1 was adequate. Advantage: model2. Assumption #6: The error term is homoscedatic To evaluate this assumption, we create a scatterplot of residuals vs. the fitted values, \\(\\hat{Y_{i}}\\). Both of these data are contained in the model output lists. p5 &lt;- ggplot(data = model1$model) + geom_point(aes(x = model1$fitted.values, y =model1$residuals), alpha = 0.25, color = &quot;maroon3&quot;) + geom_hline(yintercept = 0) + theme_classic() + theme(aspect.ratio = 0.5) p6 &lt;- ggplot(data = model2$model)+ geom_point(aes(x = model2$fitted.values, y =model2$residuals), alpha = 0.25, color= &quot;royalblue2&quot;) + geom_hline(yintercept = 0) + theme_classic() + theme(aspect.ratio = 0.5) grid.arrange(p5, p6, ncol = 1) Figure 11.7: Residuals vs. fitted values as a check for homoscedasticity. Looking at these plots, the residuals don’t show major changes across the range of fitted values (for both plots) - this is a good outcome because it implies the residuals are homoscedastic. There are slight differences between the two models, however. For example, model1 tends to have positive residuals at low and high values of \\(\\hat{Y_{i}}\\), whereas, model2 seems to have more constant error across the range of \\(\\hat{Y_{i}}\\). Boring residuals are what you want. Advantage: model2. Assumption 7: No autocorrelation among residuals This assumption is evaluated by making a partial autocorrelation plot of your residuals. We will accomplish these plots using pacf() function from the {stats} package. stats::pacf(model1$residuals, main = &quot;Model 1 Partial Autocorrelation Plot&quot;) Figure 11.8: Partial autocorrelation plots of residuals. stats::pacf(model2$residuals, main = &quot;Model 2 Partial Autocorrelation Plot&quot;) Figure 11.9: Partial autocorrelation plots of residuals. Neither plot shows a strong degree of autocorrelation, though both plots suggest that lag-5 autocorrelation is borderline significant (moreso for model1). Autocorrelation among the residuals suggests that your model is not correctly specified and these results suggest that model2 is slightly better specified than model1. Perhaps something is missing from these models? Assumption 8: Residuals are not correlated with predictor variables. This assumption can be evaluated through a correlation analysis. The cor() function from the {stats} package allows you to calculate correlation coefficients (e.g., Pearson, Spearman) among variables in a data frame. If you supply a data frame as an argument to cor() it will calculate correlations among all possible variable combinations. Otherwise, you can supply it with two vectors: x = and y =. cor(x = model1$residuals, y = model1$model[,2], method = &quot;pearson&quot; ) ## [1] -1.420982e-16 cor(x = model2$residuals, y = model1$model[,2], method = &quot;pearson&quot; ) ## [1] -1.012153e-16 The correlation coefficients are both nearly zero, so this last assumption is validated. Which model is best? Both models explained ~80% of the variance in our dependent variable. Based on our process knowledge and the fit diagnostics, we can conclude that model2 is better specified than model1. In examining our process knowledge and the residuals, however, we cannot help but wonder if a better model exists out there to predict body mass based on measurement variables? 11.6 Calibration Calibration is a common technique in science and engineering used to aid measurement. To calibrate an instrument means to compare its measurements to those of a better one (when measuring the same thing). What qualifies as a better instrument? That depends on on the thing being measured but we usually utilize metrics like precision, bias, linearity, and dynamic range to judge whether one instrument is better than another. In this section, we will outline the approach for calibration using linear modeling as a tool. The following steps are needed to calibrate an instrument that reports continuous readings: Decide on the form of the reference (to which your instrument is being compared). The two most common approaches are to use either a standard reference measurement or a standard reference material. To use a standard reference measurement means to use a better (trusted) instrument to perform your calibration. The term standard reference in science or engineering means “commonly accepted as a best available technique”. Standard reference instruments often come with some sort of certification of authenticity or performance (i.e., accurate to within X % and precise to within Y %). To use a standard reference material means to have known quantities of the thing being measured. Standard reference materials are nice because, if you have them, then you don’t need a better instrument with which to perform the calibration. Unfortunately, standard reference materials are not always available (commercially) and, when they are, tend to be expensive. Decide on the range and number of measurements needed for the calibration. Although a calibration can technically be performed with just a single data point, we often want to confirm that our measurement device can handle a wide range of levels. Thus, the range should span all values of the thing being measured that you expect to encounter, and then some. If, for example, you wanted to measure the mass of objects between 50 and 100g, you might consider calibrating your instrument from 0 to 150g. The range and number of measurements needed to perform a calibration is a judgment call; there is no “correct” number of samples needed to perform a calibration. That said, I can offer some advice: The calibration range should not exceed the dynamic range of your instrument (if known), nor should it greatly exceed the range of measurements you expect to encounter (the latter is a waste of time/effort). If you wish to check the calibration of a device (that you otherwise tend to trust), then a 6-point calibration is common (a “zero point” plus five measurement points across the selected range). When performing a calibration, it’s a good idea to take multiple samples at each level of interest. These repeated measures can be used to estimate method precision. I recommend 3-4 repeats at each level, which with a 6-point calibration, would give you 20 data points. If you wish to establish or evaluate figures of merit like dynamic range or linearity of your measurement method, you likely need a more comprehensive calibration dataset. Collect the calibration data. This step is relatively straightforward. Set up an experiment so that you can take the required number of measurements with your instrument. Note that a randomized sampling strategy is preferred. Create a calibration curve. Fit a linear model between the instrument being calibrated (x-axis) and your reference data (y-axis). Check model assumptions. Evaluate method performance. Decide on whether your instrument is biased. Is the slope of the line equal to 1.0? Is the intercept of the model fit equal to 0? Do the residuals show a pattern with changing sample magnitude? Are the residuals correlated with something (i.e., another variable)? Calculate instrument precision and linearity. Decide on whether the dynamic range of the instrument was exceeded by any of calibration levels. 11.6.1 Example Calibration In this example, we will evaluate the calibration of a low-cost instrument designed to measure aerosol optical depth, or AOD. The term aerosol optical depth is a term in atmospheric science that describes whether air pollution contributes to loss of visibility in the atmosphere (it is like a measure of haziness of the sky). We often experience high levels of AOD in Colorado when smoke from wildfires shows up (and that happens more often each year). 11.7 Ch-11 Exercises hh1 - fit a two parameter model that suffers from multicollinearity (height ~ weight + waist) and use diagnostics to discover the problem. Discuss how process knowledge suggests that a negative beta in this case makes absolutely no sense. Validate the mass-waist model using NHANES data from a different year hh2 - fit a model that suffers from heteroscedasticity; transform the dependent variable to normalize the residuals. Discuss issues with interpreting transformed models. Create a new variable (volume = waist * height) and see if that variable explains more of the variance in mass from one of the NHANES data frames. Use the employees.csv data to fit a model (vendor ~ metal). Discover autocorrelation among the residuals. Calculate residuals from a model manually and prove that they are equal to the model$residuals values. 11.8 Ch-11 Homework "],
["simulate.html", "Chapter 12 Simulation 12.1 Ch. 12 Objectives 12.2 Introduction to simulation 12.3 Simulation Example: 12.4 Ch-12 Exercises 12.5 Ch-12 Homework", " Chapter 12 Simulation Text 12.1 Ch. 12 Objectives 12.2 Introduction to simulation 12.3 Simulation Example: 12.4 Ch-12 Exercises 12.5 Ch-12 Homework "],
["dist.html", "Chapter 13 Appendix: Reference Distributions 13.1 Uniform Distribution 13.2 Normal Distribution 13.3 Log-normal Distribution 13.4 Statistical Terms", " Chapter 13 Appendix: Reference Distributions In this chapter I discuss a handful of reference distributions that you may encounter while working thorugh this course. I don’t go into great detail on any of these distributions (or their mathematical structure) because smarter, better, and more authoritative descriptions can be found elsehwere in reference texts or online. This is the $1 tour. 13.1 Uniform Distribution The uniform distribution describes a situation where all obervations have an equal probability of occurrence. Examples of uniform distributions include: the outcome of a single roll of a 6-sided die, or the flip of a coin, or the chance of picking a spade, heart, diamond, or club from a well-shuffled deck of cards. In each of these cases, all potential outcomes have an equal chance of occuring. A uniform distribition may be specified simply by setting the range of possible outcomes (i.e., a minimum, maximum, and anything in between). The “in-between” part also lets you specify whether you want to allow outcome values that are continuous (like from a random-number generator), integers (like from a dice roll), or some other format (like a binary 0 or 1; heads or tails). Below, we create a probability density function for the first roll of a six-sided die; this is a discrete uniform distribition since we only allow integers to occur. A uniform distribution is appropriate here because any of the numbers between 1 and 6 has equal probability of being rolled. Notice the shape of the histogram…flat. #create a uniform distribition for the first roll of a 6-sided die six_sided &lt;- tibble( rolls = ceiling(runif(10e4, min=1, max=7)) ) #create a histogram of the probability density for a uniform distribution ggplot(data = six_sided, aes(x = rolls)) + geom_histogram( breaks = seq(1,7,1), fill = &quot;grey&quot;, color = &quot;white&quot;) + xlab(&quot;Number&quot;) + scale_x_continuous(breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5), labels = as.factor(seq(0,6,1))) + theme_bw(base_size = 12) Figure 13.1: Outcome probability for the first roll of a 6-sided die 13.1.1 Characteristic Plots: Uniform Distribution Below, we show the cumulative distribution plot and probability density function for a uniform distribution between 0 and 1. Figure 13.2: Characteristic Plots for a Uniform Distribution 13.2 Normal Distribution The normal distribution arises from phenomena that tend to have additive variability. By “additive”, I mean that the outcome (or variable of interest) tends to vary in a +/- fashion from one observation to the next. Lots of things have additive variability: the heights of 1st graders, the size of pollen grains from a tree or plant, the variation in blood pressure across the population, or the average temperature in Fort Collins, CO for the month of June. Let’s examine what additive variability looks like using the 6-sided dice mentioned above. Although a dice roll has a uniform distribution of possible outcomes (rolling a 1,2,3,4,5, or 6), the variability associated with adding up the sum of three or more dice creates a normal distribution of outcomes. If we were to roll four, 6-sided dice and sum the result (getting a value between 4 and 24 for each roll), and then repeat this experiment 10,000 times, we see the distribution shown below. The smooth line represents a fit using a normal distribution - a pretty nice fit considering that we are working with a discrete (integer-based) dataset! Figure 13.3: A Normal Distribution 13.2.1 Normal Distribution: Characteristic Plots Unlike the uniform distribution, the normal distribution is not specified by a range (it doesn’t have one). The normal distribution is specified by a central tendancy (a most-common value) and a measure of data’s dispersion or spread (a standard deviation). A normal distribution is symmetric, meaning that the spread of the data is equal on each side of the central tendency. This symmetry also means that the mode (the most common value), the median (the 50th percentile or 0.5 quantile) and the mean (the average value) are all equal. A series of normal distributions of varying dispersion is shown in the panels below. Figure 13.4: Characteristic Plots for a Normal Distribution 13.3 Log-normal Distribution Multiplicative variation is what gives rise to a “log-normal” distribution: a special type of skewed data. Let’s create two normal distributions for variables ‘a’ and ‘b’: #create two variables that are normally distributed normal_data &lt;- tibble(a = rnorm(n=1000, mean = 15, sd = 5), b = rnorm(n=1000, mean = 10, sd = 3)) Individually, we know that these data are normally distributed (because we created them that way), but what does the distribution look like if we add these two variables together? #add those variables together and you get a normal distribution normal_data %&gt;% mutate(c = a + b) -&gt; normal_data ggplot2::ggplot(data = normal_data) + geom_histogram(aes(c), bins = 30, fill = &quot;navy&quot;, color = &quot;white&quot;) + xlab(&quot;Sum of a + b&quot;) + theme_minimal(base_size = 12) Figure 13.5: The Sum of Two Normally Distributed Variables #ggsave(&quot;./images/hist_a_b.png&quot;, dpi = 150) Answer: still normal. Since all we did here was add together two normal distributions, we simply created a third (normal) distribution with more additive variability. What happens, however, if we multiply together a series of normally distributed variables? #multiply together three normal variables normal_data %&gt;% mutate(d = sample(a*b*c, 1000)) -&gt; log_data ggplot2::ggplot(data = log_data) + geom_histogram(aes(d), bins = 30, fill = &quot;orange&quot;, color = &quot;white&quot;) + xlab(&quot;a * b * c&quot;) + theme_minimal(base_size = 12) Figure 13.6: The Product of Three Normally Distributed Variables Multiplied Together #ggsave(&quot;./images/hist_skew_out.png&quot;, dpi = 150) Answer: the additive variability becomes multiplicative variability, which leads to a skewed (in this case, log-normal) distribution. Multiplicative (or log-normal) variability arises when the mechanism(s) controlling the variation of x are multipliers (or divisors) of x. Many real-world phenomena create multiplicative variability in observed data: the strength of a WiFi signal at different locations within a building, the magnitude of earthquakes measured at a given position on the Earth’s surface, the size of rocks found in a section of a riverbed, or the size of particles found in air. All of these phenomena tend to be governed by multiplicative factors. In other words, all of these observations are controlled by mechanisms that suggest \\(x = a * b * c\\) not \\(x = a\\cdot b\\cdot c\\). 13.4 Statistical Terms 13.4.1 Measures of Central Tendency When we think about a distribution of data (in a univariate sense), the central tendency represents the center, or location, of the data. The central tendency is the answer to the question: where do the values typically fall? We will use the terms mean, median, and mode to describe a distribution’s central tendency. They are defined as follows. Let \\(x\\) be the variable of interest and assume that we have \\(n\\) observations of x stored in R as a vector. Thus, each individual observation would be \\(x_{i}\\) where \\(i\\) goes from \\(1\\) to \\(n\\). In an R programming sense, \\(n =\\) length(x). Mean: The average value, often termed as \\(\\bar{x}\\). Calculated in {base} R using the mean() function. \\[\\bar{x} = \\frac{\\sum_{i=1}^{n}x_{i}}{n}\\] Mode: The most commonly observed value for \\(x\\) among all the \\(x_{i}\\) values. The mode can be calculated using mode() or often seen via a histogram of x. Note that with continuous data (and precise measurements), the mode() can be confusing because no two values of x are the same…unless you group the observations into discrete bins (as is done in a histogram). Median: The 50th percentile value of \\(x\\) when ordered from smallest to largest value. The value of \\(x_{i}\\) that splits an ordered distribution of \\(x\\) into equal halves. The median is the same as the 0.5 quantile of \\(x\\). The median can be calculated directly using median() or the quantile() function. 13.4.2 Measures of Dispersion The dispersion of a univariate distribution of data refers to its variability. We will use the following terms to describe dispersion. This is not a comprehensive list by any means, but these terms are common: Range: The range is defined by the minimum and maximum value observed for the distribution of \\(x\\). For a large enough sample size, the range would contain nearly ALL possible observations of the data. Lots of functions can be used to calculate the range: range(), max() and min(), or quantile(x, probs = c(0,1)). Inter-quartile Range (IQR): The IQR describes the variation in \\(x\\) needed to go from the 25th% to the 75th% of the distribution. The IQR spans the “middle part” of the distribution of \\(x\\) and is calculated with IQR(). Standard deviation: The standard deviation is a common measure of dispersion, but one that is easily misused, since the “standard” part of this term implies the data are normally distributed (hint: not all data are normally distributed). Still, this term is so common that one should know it. The sample standard deviation of \\(x\\), denoted as \\(\\hat{\\sigma_{x}}\\), is calculated in R using sd() from the following formula: \\[\\hat{\\sigma_{x}} = \\sqrt {\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1}}\\] The units of \\(\\hat{\\sigma_{x}}\\) are the same as \\(x\\), so we can interpret the standard deviation as a measure of dispersion about the mean. Thus, we often see \\(\\bar{x}\\pm\\hat{\\sigma_{x}}\\) reported for a univariate distribution. Note: the “hat” symbol, \\(\\hat{}\\), over the \\(\\sigma\\) denotes that we are estimating the standard deviation based on a sample of \\(x_{i}\\) values. Statisticians created these hats to remind us that measurements (aka: samples, observations) are only estimates of a true population value. More on samples and populations here. Variance: The variance of \\(x\\) is the average of the squared difference from the mean for all values of \\(x\\). The sample variance, denoted as \\(\\hat{\\sigma}^2\\), is also the square of the standard deviation. Variance is calculated in R using the var() function. \\[\\hat{\\sigma_{x}}^{2} = \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1}\\]} Why do we take the square of \\(x_{i}-\\bar{x}\\) when calculating these measures of dispersion? Answer: Because when we are taking the sum, \\(\\sum_{i=1}^n\\), if we didn’t calculate squares then the positive and negative deviations would cancel each other out and mislead our estimate of dispersion. This is the reasoning behind all root-mean-square calculations. The mean and standard deviation are great measures of central tendency and dispersion when you are working with data that (approximately) follow a normal distribution. When data are skewed, the mean() and sd() can lead to unexpected results. See Figure 5.8, as an example. 13.4.3 Pearson Correlation Coefficient The Pearson correlation coefficient, r, is a quantitative descriptor of the degree of linear correlation between two variables (let’s call them x and y). The Pearson correlation coefficient indicates the proportion of variation in \\(y\\) that can be explained by knowing \\(x\\), when the data are paired. Below, we show a series of scatter plots with varying levels of correlation between two vectors: x and y. Figure 13.7: Pearson correlation for variables with perfect, strong, moderate, and no correlation. Values of r range from -1 (perfect negative correlation) to 0 (no correlation) to 1 (perfect positive correlation). As an engineer, I would say that two variables are moderately correlated when they have a Pearson correlation coefficient (as an absolute value) \\(|r|\\), between 0.25 and 0.75. Two variables are strongly correlated when \\(|r|&gt;0.75\\). These are qualitative judgments on my part; someone in a different discipline (like epidemiology or economics) might get super excited by discovering an r = 0.3 between two variables. You will often see the square of Pearson correlation coefficient reported, \\(r^2\\). The \\(r^2\\) term is a direct indicator of how the variance in \\(y\\) is explained by knowing \\(x\\). Note that because of the square power, \\(r^2\\) values range from zero to 1. There are several ways to calculate r - all of these are mathematically equivalent. If we have n paired samples of x and y, then r is: \\[r = \\frac{n\\sum(x_{i}y_{i})-\\sum x_{i} \\sum y_{i} } {\\sqrt {n\\sum(x_{i}^{2})-\\sum(x_{i})^{2}} \\cdot \\sqrt {n\\sum(y_{i}^{2})-\\sum(y_{i})^{2}}}\\] This equation looks like a lot of work but it’s really just a lot of algebra to divide the covariance of x and y with the product of their standard deviations: \\(\\hat{\\sigma}_{x}\\), \\(\\hat{\\sigma}_{y}\\). \\[r = \\frac{cov(x,y)} {\\hat{\\sigma}_{x} \\cdot \\hat{\\sigma}_{y}}\\] You can rearrange the equation to calculate r using the mean and standard deviation as follows (note: the \\(n-1\\) parts get canceled out): \\[r = \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})\\cdot(y_{i} - \\bar{y}) } {\\sqrt {\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}} \\cdot \\sqrt {\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}}}\\] Figure 13.8: The Pearson Correlation Coefficient is not as bad as it loooks in algebraic form. You can calculate r using the cor() function and supplying x and y as arguments. "]
]
