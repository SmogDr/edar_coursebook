# Appendix: Stats & Reference Distributions {#dist}
In this chapter I discuss a handful of basic statistical concepts, including several reference distributions, that you may encounter while working through this course. I don't go into great detail on any of these topics (or their mathematical structure) because smarter, better, and more authoritative descriptions can be found elsewhere in reference texts or online. This is the $1 tour. None of the material that follows is required learning for R programming, but if you haven't previously studied statistics (or need a refresher), it can't hurt. The concepts outlined below, along with the discussion on [quantiles](#quantile) in Chapter \@ref(eda1), form the basis for many, many data analysis techniques.

Before we delve into distributions, however, we should begin with measures of *central tendency* and *spread*, since many reference distributions have those as defining properties.

```{r dist1, include=FALSE}
library(tidyverse)
library(bookdown)
library(gridExtra)
library(grid)
library(broom)
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

jvPalette <- c("#330099","#CC0066","#FF6633",
                      "#0099CC", "#FF9900","#CC6633",
                      "#FF3366", "#33CC99", "#33999")
```

## Measures of Central Tendency

When we think about a distribution of data (in a [univariate](#univar) sense), the ***central tendency*** represents the center, or *location*, of the data. The central tendency is the answer to the question: *where do the values typically fall*? We will use the terms **mean**, **median**, and **mode** to describe a distribution's central tendency. They are defined as follows. 

Let $x$ be the variable of interest and assume that we have $n$ observations of *`x`* stored in R as a vector. Thus, each individual observation would be $x_{i}$ where $i$ goes from $1$ to $n$. In an R programming sense, $n =$ `length(x)`.

  - **Mean**: The average value, often termed as $\bar{x}$. Calculated in `{base} R`
  using the `mean()` function.
  
  $$\bar{x} = \frac{\sum_{i=1}^{n}x_{i}}{n}$$

 - **Mode**: The most commonly observed value for $x$ among all the $x_{i}$ values. The mode can be calculated using `mode()` or often seen via a [histogram](#hist) of `x`. Note that with continuous data (and precise measurements), the `mode()` can be confusing because no two values of `x` are the same...*unless* you group the observations into discrete bins (as is done in a histogram).
 
 - **Median**: The 50^th^ percentile value of $x$ when ordered from smallest to 
 largest value.  The value of $x_{i}$ that splits an ordered distribution of $x$ 
 into equal halves. The median is the same as the 0.5 [quantile](#quantile) of $x$.
 The median can be calculated directly using `median()` or the `quantile()` 
 function.
 
## Measures of Dispersion
The dispersion of a univariate distribution of data refers to its variability. We will use the following terms to describe dispersion. This is not a comprehensive list by any means, but these terms are common:

  - **Range**: The range is defined by the minimum and maximum value observed for
  the distribution of $x$.  For a large enough sample size, the range would contain nearly ALL possible observations of the data.  Lots of functions can be used to calculate the range: `range()`, `max()` and `min()`, or `quantile(x, probs = c(0,1))`.
  
  - **Inter-quartile Range (IQR)**: The IQR describes the variation in $x$ needed to go from the 25^th^% to the 75^th^% of the distribution. The IQR spans
the "middle part" of the distribution of $x$ and is calculated with `IQR()`.

  - **Standard deviation**: The standard deviation is a common measure of dispersion, but one that is easily misused, since the *"standard"* part of this term implies the  data are ***normally distributed*** (*hint: not all data are normally distributed*). Still, this term is so common that one should know it. The sample standard deviation of $x$, denoted as $\hat{\sigma_{x}}$, is calculated in R using `sd()` from the following formula:
  
  $$\hat{\sigma_{x}} = \sqrt {\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}{n-1}}$$
  
The units of $\hat{\sigma_{x}}$ are the same as $x$, so we can interpret the standard deviation as a measure of **dispersion** about the **mean**.  Thus, we often see $\bar{x}\pm\hat{\sigma_{x}}$ reported for a univariate distribution.

```{block, type="rmdnote"}
Note: the "hat" symbol, $\hat{}$, over the $\sigma$ denotes that we are *estimating* the standard deviation based on a sample of $x_{i}$ values. Statisticians created these hats to remind us that measurements (aka: samples, observations) are only estimates of a true population value. More on samples and populations 
[here](#sample). 
```


  - **Variance**: The variance of $x$ is the *average of the squared difference from the mean* for all values of $x$.  The sample variance, denoted as $\hat{\sigma}^2$,
 is also the square of the *standard deviation*. Variance is calculated in R using the `var()` function. When we talk about variance we usually care about relative changes (i.e., *"most of the variance is attributed to variable $z$"*) because the units of variance are in terms $x^2$, which is not easily interpreted.
  
  $$\hat{\sigma_{x}}^{2} = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}{n-1}$$
  
***Why do we take the square of $x_{i}-\bar{x}$ when calculating these measures of***
***dispersion?*** <br> *Answer*: Because when we are taking the sum, $\sum_{i=1}^n$, if we didn't calculate squares then the positive and negative deviations would cancel each other out and mislead our estimate of dispersion.

```{block, type='rmdwarning'}
The mean and standard deviation are fine measures of central tendency and dispersion when you have data that (approximately) follow a normal distribution. When data are skewed, the `mean()` and `sd()` can lead to unexpected results.  See 
[Figure 5.8](#skew), as an example.
```
 

## Uniform Distribution {#unif_dist}
The uniform distribution describes a situation where all obervations have an equal probability of occurrence. Examples of uniform distributions include: the outcome of a single  roll of a 6-sided die, or the flip of a coin, or the chance of picking a spade, heart, diamond, or club from a well-shuffled deck of cards.  In each of these cases, all potential outcomes have an equal chance of occuring.  A uniform distribition may be specified simply by setting the range of possible outcomes (i.e., a minimum, maximum, and anything in between). The "in-between" part also lets you specify whether you want to allow outcome values that are continuous (like from a random-number generator), integers (like from a dice roll), or some other format (like a binary 0 or 1; heads or tails).  

Below, we create a [probability density function](#pdf)  for the first roll of a six-sided die; this is a *discrete uniform distribition* since we only allow integers to occur.  A uniform distribution is appropriate here because any of the numbers between 1 and 6 has equal probability of being rolled.  Notice the shape of the histogram...flat.

``` {r unif-dist, fig.cap = "Relative counts for the first roll of a 6-sided die, rolled 100,000 times", fig.width=4, fig.height=3, fig.align="center"}
#create a uniform distribition for the first roll of a 6-sided die
six_sided <- tibble(
  rolls = ceiling(runif(10e4, min=1, max=7))
)

#create a histogram of the probability density for a uniform distribution 
ggplot(data = six_sided, aes(x = rolls)) +
  geom_histogram(
      breaks = seq(1,7,1), 
      fill = "grey",
      color = "white") +
  xlab("Number") +
  scale_x_continuous(breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5),
                     labels = as.factor(seq(0,6,1))) +
  theme_bw(base_size = 12)
```
  
### Uniform Characteristic Plots
Below, we show the cumulative distribution plot and probability density function for a uniform distribution between 0 and 1.
``` {r unif-dist-plots2, echo=FALSE, fig.cap="Characteristic Plots for a Uniform Distribution"}
unif_dist2<-tibble(data = seq(0,1,0.001), 
                  quant = seq(0,1,0.001))
                  

unif_cdf <- ggplot(data = unif_dist2) +
  geom_point(aes(x=quant, y = data)) +
  labs(title = "Uniform Cumulative Distribution",
       y ="Cumlative Fraction",
       x = "Values")

unif_hist <- ggplot(data = unif_dist2) +
  stat_density(aes(x = data),
                 outline.type = "full",
               kernel = "gaussian",
               n = 4096,
               alpha = 0.75,
               bw = .001) +
  labs(title = "Uniform Probability Density",
       y = "Probability",
       x = "Values")

grid.arrange(unif_cdf, unif_hist, ncol=2)

```

## Normal Distribution {#normal_dist}
The normal distribution arises from phenomena that tend to have *additive* variability. By "additive", I mean that the outcome (or variable of interest) tends to vary in a +/- fashion from one observation to the next. Lots of things have additive variability: the heights of 1^st^ graders, the size of pollen grains from a tree or plant, the variation in blood pressure across the population, or the average temperature in Fort Collins, CO for the month of June.  

Let's examine what *additive variability* looks like using the 6-sided dice mentioned above.  Although a dice roll has a uniform distribution of possible outcomes (rolling a 1,2,3,4,5, or 6), the variability associated with adding up the sum of three or more dice creates a normal distribution of outcomes.  If we were to roll four, 6-sided dice and sum the result (getting a value between 4 and 24 for each roll), and then repeat this experiment 10,000 times, we see the distribution shown below.  The smooth line represents a fit using a normal distribution - a pretty nice fit considering that we are working with a discrete (integer-based) dataset!
``` {r normal1, message=FALSE, echo=FALSE, fig.cap="A Normal Distribution", fig.width=4, fig.height=4, fig.align="center"}
#sample a uniform distribution between 0 and 6, replicate 1000 times
#use 'ceiling' function to round up results
four_dice <- tibble(sum = replicate(10000, runif(4, min=0, max=6) %>% 
                         ceiling() %>% 
                         sum() #sum the result
                       ))
ggplot(data = four_dice) +
  geom_histogram(aes(x = sum, after_stat(density)),
                 bins = 21,
                 fill = "navy",
                 color = "white") + 
  stat_function(fun = dnorm, n = 101, 
                args = list(
                  mean = mean(four_dice$sum), 
                  sd = sd(four_dice$sum)
                  )
                ) +
  xlab("Sum of four, 6-sided dice") +
  theme_bw()

```

### Normal Characteristic Plots

Unlike the uniform distribution, the normal distribution is not specified by a range (it doesn't have one).  The normal distribution is specified by a *central tendency* (a most-common value) and a measure of data's *dispersion* or spread (a standard deviation). A normal distribution is symmetric, meaning that the spread of the data is equal on each side of the central tendency.  This symmetry also means that the mode (the most common value), the median (the 50^th^ percentile or 0.5 quantile) and the mean (the average value) are all equal. A series of normal distributions of varying dispersion is shown in the panels below.

``` {r normal2, warning=FALSE, echo=FALSE, fig.width=6, fig.height=6, fig.align="center", fig.cap="Characteristic Plots for a Normal Distribution"}
norm2 <- tibble(a = rnorm(n = 20000, mean = 50, sd = 5),
                b = rnorm(n = 20000, mean = 50, sd = 10),
                c = rnorm(n = 20000, mean = 50, sd = 15),
                d = rnorm(n = 20000, mean = 50, sd = 20),
                e = rnorm(n = 20000, mean = 50, sd = 25))

normal_ecdf <- ggplot(data = norm2) +
  stat_ecdf(aes(x = a, color = "sd = 5")) +
  stat_ecdf(aes(x = b, color = "sd = 10")) +
  stat_ecdf(aes(x = c, color = "sd = 15")) +
  stat_ecdf(aes(x = d, color = "sd = 20")) +
  stat_ecdf(aes(x = e, color = "sd = 25")) +
  xlim(0,100) +
  labs(title = "\"Normal\" Cumulative Distribution",
       x = "Values",
       y = "Cumulative Fraction") +
  scale_color_manual(values = c("black", 
                                "blue", 
                                "orange", 
                                "magenta", 
                                "darkgreen"), 
                     breaks=c("sd = 5",
                              "sd = 10",
                              "sd = 15", 
                              "sd = 20", 
                              "sd = 25"),
                     name = "Mean = 50") +
  theme_bw()

normal_density<- ggplot(data = norm2) +
  geom_density(aes(x = a, color = "sd = 5"),
               adjust = 2) +
  geom_density(aes(b, color = "sd = 10"),
               adjust = 2) +
  geom_density(aes(c, color = "sd = 15"),
               adjust = 2) +
  geom_density(aes(d, color = "sd = 20"),
               adjust = 2) +
  geom_density(aes(e, color = "sd = 25"),
               adjust = 2) +
  xlim(0,100) +
  labs(title = "\"Normal\" Probability Density",
       x = "Values",
       y = "Probability") +
  scale_color_manual(values = c("black", 
                                "blue", 
                                "orange", 
                                "magenta", 
                                "darkgreen"), 
                     breaks=c("sd = 5",
                              "sd = 10",
                              "sd = 15", 
                              "sd = 20", 
                              "sd = 25"),
                     name = "Mean = 50") +
  theme_bw() 

grid.arrange(normal_density, normal_ecdf, 
             nrow=2, 
             respect=TRUE, 
             heights = c(0.5, 0.5))


```

### 68-95-99 rule

```{r normal-probs, echo=FALSE}
#function to create a shaded ribbon plot using dnorm inputs
#https://gist.github.com/jrnold/6799152
normal_prob_area_plot <- function(lb, ub, fill.c = "black", mean = 0, sd = 1, limits = c(mean - 3 * sd, mean + 3 * sd)) {
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, 
                       ymin = 0, 
                       ymax = dnorm(areax, 
                                    mean = mean, 
                                    sd = sd))
    (ggplot() +
        geom_line(data.frame(x = x, 
                            y = dnorm(x, 
                                      mean = mean, 
                                      sd = sd)),
                 mapping = aes(x = x, 
                               y = y)) +
        geom_ribbon(data = area, 
                   mapping = aes(x = x, 
                                 ymin = ymin, 
                                 ymax = ymax), 
                   alpha = 0.5, 
                   fill = fill.c)) 
}
```

```{r norm-shaded-plot, echo=FALSE, message=FALSE, include=FALSE}
#list arguments for normal_prob_area_plot()
low <- list(-1, -2, -3, 1, 2)
up <- list(1, -1, -2, 2, 3)
fill.c <- list(jvPalette[1], 
               jvPalette[2], 
               jvPalette[3], 
               jvPalette[2], 
               jvPalette[3])

#map the list entries as arguments to normal_prob_area_plot()
plots <- pmap(list(low, up, fill.c), normal_prob_area_plot)

#use the first list entry as a base and add the ribbon layers in succession
plots[[1]] + 
  plots[[2]]$layers[2] + 
  plots[[3]]$layers[2] + 
  plots[[4]]$layers[2] +
  plots[[5]]$layers[2] + 
  xlab("Standard Deviations") +
  ylab("Probability Density") +
  scale_x_continuous(breaks = seq(-3,3,1)) +
  theme_bw()

ggsave(filename = "./images/shaded_prob.png")
```

A ***standard*** deviation for a normal distribution relates the *spread* of the data about the mean (and *median*, since they are one-in-the-same for normal data). Since we often care about the probability of occurrence for some value, whether it be a more probable value close to the mean or an extreme value close to one of the tails, the standard deviation helps tell us precisely what the odds are for getting that value, given a random sample. We use the `pnorm()` function to estimate these probabilities. The `pnorm()` function returns the probability of returning a value less than the value indicated for a given mean and standard deviation. For example, from plot in Figure \@ref(fig:normal2) where $\mu=50$ and $\sigma=10$, the probability of sampling a value of 70 or less is:
```{r pnorm-function}
pnorm(q = 70, mean = 50, sd =10) %>%
  round(., 3)
```

The probability of sampling a value of greater than 70 is given when we set the argument `lower.tail = FALSE`. 
```{r pnorm-function2}
pnorm(q = 70, mean = 50, sd =10, lower.tail = FALSE) %>%
  round(., 3)
```

That second probability is equivalent to `1 - pnorm(q = 70, mean = 50, sd =10)`. Try it out for yourself.

These probabilities are called ***"one-tailed"*** **tests** because they only occur for a single value in one direction away from the mean of the distribution (i.e., they return the probability of being within or outside one of the "tails" of the distribution).

We also define ***"two-tailed"*** **tests**, where we look to see if a value is likely to occur within a range (or interval) about the mean value. A two-tailed test evaluates whether we are likely to find a value within a given number of standard deviations from the mean. 

Common "two-tailed" probability intervals are common knowledge for a *normal* distribution (read: you should be aware of them). These probabilities state that a known fraction of data are encapsulated as we move away from the mean in units of standard deviations ($\sigma$). This phenomenon is known as the **"68-96-99 rule"**:  

- One standard deviation about the mean (the interval from $\mu - 1\sigma$ to $\mu + 1\sigma$) contains 68.3% of the observed data.
- Two standard deviations about the mean (the interval from $\mu - 2\sigma$ to $\mu + 2\sigma$) contains 95.5% of the observed data.
- Three standard deviations about the mean ($\mu - 3\sigma$ to $\mu + 3\sigma$) contains 99.7% of the observed data. Figure \@ref(fig:shaded-prob-anno) below depicts this rule graphically. 

```{r shaded-prob-anno, echo=FALSE, fig.align="center", fig.cap="The 68-95-99 Rule for Normal Probability Distributions. Numbers represent the probability of finding a value within 1-3 standard deviations about the mean."}

knitr::include_graphics(path = "./images/shaded_prob_anno.png")
```

Inspection of Figure \@ref(fig:shaded-prob-anno) indicates that the probability of encountering a value $\pm1\sigma$ from the mean is fairly common, occurring about 32% of the time.  Encountering a value $\pm2\sigma$ from the mean, however, is rare - happening less than 5% of the time. When you hear someone refer to a "two sigma" displacement, this is likely the type of scenario they are depicting (though people rarely indicate whether they mean a one-tailed or two-tailed situation). A $\pm3\sigma$ deviation is extremely rare, happening less than 0.3% of the time. 

## Log-normal Distribution {#log_normal_dist}
Multiplicative variation is what gives rise to a "log-normal" distribution: a special type of skewed data.  

Let's create two normal distributions for variables `a` and `b`:  

``` {r a-b-normal_dist, warning=FALSE}
#create two variables that are normally distributed
normal_data <- tibble(a = rnorm(n=1000, mean = 15, sd = 5),
                      b = rnorm(n=1000, mean = 10, sd = 3))
```

Individually, we know that these data are normally distributed (because we created them that way), but what does the distribution look like if we add these two variables together?

``` {r a-b-histogram, warning=FALSE, message=FALSE, fig.cap="The Sum of Two Normally Distributed Variables", fig.width=4, fig.height=4, fig.align="center"}
#add those variables together and you get a normal distribution
normal_data %>% 
  mutate(c = a + b) -> normal_data

ggplot2::ggplot(data = normal_data) + 
  geom_histogram(aes(c),
                 bins = 30,
                 fill = "navy",
                 color = "white") +
  xlab("Sum of a + b") +
  theme_minimal(base_size = 12)
```
*Answer: still normal*.  Since all we did here was add together two normal distributions, we simply created a third (normal) distribution with more additive variability.  

What happens, however, if we randomly sample values from one or more normal distributions and multiply them together?  
``` {r a-b-lognormal, message=FALSE, fig.cap="The Product of Three Normally Distributed Variables Multiplied Together", , fig.width=4, fig.height=4, fig.align="center"}
#multiply together three normal variables
normal_data %>% 
  mutate(d = sample(a*b*c, 1000)) -> log_data

ggplot2::ggplot(data = log_data) + 
  geom_histogram(aes(d),
                 bins = 30,
                 fill = "orange",
                 color = "white") +
  xlab("a * b * c") +
  theme_minimal(base_size = 12) 
```
*Answer: the additive variability becomes **multiplicative variability**, which leads to a skewed (in this case, log-normal) distribution.*

Multiplicative (or log-normal) variability arises when the mechanism(s)
controlling the variation of $x$ are multipliers (or divisors) of $x$. Many
real-world phenomena create multiplicative variability in observed data: the
strength of a WiFi signal at different locations within a building, the
magnitude of earthquakes measured at a given position on the Earth's surface,
the size of rocks found in a section of a riverbed, or the size of particles
found in air. All of these phenomena tend to be governed by multiplicative
factors. In other words, all of these observations are controlled by mechanisms
that suggest $x = a\cdot b\cdot c$ not $x = a + b + c$. Note: we provide an example of fitting a log-normal distribution in Section \@ref(logfit).

``` {r a_b_lognormal_log_axis, eval=FALSE, echo=FALSE}
#plot the same data on a log scale (x axis)

ggplot2::ggplot(data = log_data) + 
  geom_histogram(aes(d),
                 bins = 25,
                 fill = "white",
                 color = "darkgrey") +
  theme_minimal() +
  scale_x_log10() +
  theme(text = element_text(size=20))
#ggsave("./images/hist_skew_out_logx.png", dpi = 150)
```

### Lognormal Characteristic Plots
The lognormal distribution is called such because you observe a "normal" distribution only after taking the log of each observation, $log(x_i)$. Similar to its normal counterpart, the lognormal distribution is characterized by a **mean** ($\mu$) and a **standard deviation** ($\sigma$), but in this case, $\mu$ and $\sigma$ are calculated using $log(x_i)$. Thus, to estimate $\mu$ and $\sigma$ we take the log of all our observations of $x$ and then calculate means and standard deviations of those logged values.

$$\hat{\mu} = \frac{\sum_{i=1}^{n}ln(x_{i})}{n}$$

 $$\hat{\sigma} = \sqrt {\frac{\sum_{i=1}^{n}(ln(x_{i})-\hat{\mu})^2}{n-1}}$$
```{block, type='rmdnote'}
Note that $\mu$ and $\sigma$ are dimensionless because they exist in log space.  We normally want to communicate these variables in the same units as they exist. For that reason, we define a **geometric mean** as $e^{\hat{\mu}}$ and a **geometric standard deviation** or GSD as $e^\hat{\sigma}$.
```

Let's take a look at a series of lognormal distributions where $\mu$ is held constant at 0 (if $\mu=0$ then $e^\mu=1$) and $\sigma$ is varied from 0.4 to 1.1 (which means that GSD will vary from 1.5 to 3).

```{r log-plot1, echo=FALSE, warning=FALSE, fig.cap="Characteristic Plots for a Series of Lognormal Distributions of Varying GSD", fig.align="center"}

y <- tibble(
  blue = rlnorm(20000, meanlog = 0, sdlog = log(1.5)),
  red = rlnorm(20000, meanlog = 0, sdlog = log(2.0)),
  green = rlnorm(20000, meanlog = 0, sdlog = log(3))
) %>%
  pivot_longer(cols = blue:green,
               names_to = "GSD",
               values_to = "Values") %>%
  mutate(GSD = case_when(
    GSD == "blue" ~ "1.5 (0.41)",
    GSD == "red" ~ "2.0 (0.69)",
    GSD == "green" ~ "3 (1.10)"
  )) %>%
  mutate(GSD = as.factor(GSD))

log.pdf <- ggplot(data = y) +
  geom_density(aes(x = Values, color = GSD),
               adjust = 2) +
  scale_color_manual(name = "GSD, (\U003BC)",
                    values = jvPalette[c(1,3,4)]) +
  ylab("Probability Density") +
  xlim(0.01, 5) + 
  ggtitle("Lognormal Probability Density") +
  theme_bw() +
  theme(legend.direction = "vertical", legend.box = "horizontal")

log.cdf <- ggplot(data = y) +
  stat_ecdf(aes(x = Values, color = GSD)) +
  scale_color_manual(name = "GSD, (\U003BC)",
                    values = jvPalette[c(1,3,4)]) +
  ylab("Cumulative Distribution (quantiles)") +
  xlim(0.1, 5) +
  ggtitle("Lognormal Cumulative Distribution") +
  theme_bw()


grid.arrange(log.pdf, log.cdf,
             nrow=2, 
             respect=TRUE, 
             heights = c(0.5, 0.5))

```
A key characteristic of lognormal distributions is that they are *left-skewed*, meaning that the plot has a tail that extends far to the right (towards increasing values).  Another interesting thing about lognormal distributions is that when plotted on a log-scale axis, they look normal!  Take a look at Figure \@ref(fig:log-plot1) when plotted with `scale_x_log10()` and compare it to Figure \@ref(fig:normal2).

```{r log-plot2, echo=FALSE, warning=FALSE, fig.cap="Lognormal Distributions look \"normal\" when plotted on a log scale axis.", fig.align="center"}

log.pdf2 <- ggplot(data = y) +
  geom_density(aes(x = Values, color = GSD),
               adjust = 2) +
  scale_color_manual(name = "GSD, (\U003BC)",
                    values = jvPalette[c(1,3,4)]) +
  ylab("Probability Density") +
  ggtitle("Lognormal Probability Density") +
  theme_bw() +
  theme(legend.direction = "vertical", legend.box = "horizontal") +
  scale_x_log10(limits = c(0.1, 10))

log.cdf2 <- ggplot(data = y) +
  stat_ecdf(aes(x = Values, color = GSD)) +
  scale_color_manual(name = "GSD, (\U003BC)",
                    values = jvPalette[c(1,3,4)]) +
  ylab("Cumulative Distribution (quantiles)") +
  ggtitle("Lognormal Cumulative Distribution") +
  theme_bw() +
  scale_x_log10(limits = c(0.1, 10))


grid.arrange(log.pdf2, log.cdf2,
             nrow=2, 
             respect=TRUE, 
             heights = c(0.5, 0.5))
```

```{block, type="rmdnote"}

The terms *meanlog* and *sdlog* represent log-space values (i.e., the data have been log transformed) - these unitless terms are used in R functions like rlnorm() and qlnorm(). The term *geometric mean* is presented in the units of the data (read: it's the easiest to comprehend). The  *geometric standard deviation* (GSD) is also unitless but different from *sdlog*. The **GSD is a ratio of quantiles**: it represents the ratio of data values at quantiles that are separated by a standard deviation: $GSD = \frac{q_{84}}{q_{50}}$ (the 0.84 quantile divided by the 0.5 quantile). GSD can also be calculated in the other direction,  $GSD = \frac{q_{50}}{q_{16}}$, the 0.5 quantile divided by the 0.16 quantile. 

```
## Pearson Correlation Coefficient {#pearson}
The Pearson correlation coefficient, ***r***, is a quantitative descriptor of the degree of **linear correlation** between two variables (let's call them $x$ and $y$).  

The Pearson correlation coefficient indicates the proportion of linear variation 
in $y$ that can be explained by knowing $x$, when the data are paired.  By "linear" we mean a straight-line fit between the two variables.  The assumptions underlying the
Pearson correlation coefficient are as follows:  

  - The variables x, y are continuous. 
  - Both x and y were collected as paired samples
  - The dataset is free of outliers (outliers overly influence results)
  - A linear relationship exists between x and y

Below, we show a series of scatter plots with varying levels of linear correlation 
between two vectors: 
`x` and `y`. 

``` {r corr_plots1, warning=FALSE, echo=FALSE, message=FALSE}
set.seed(7)
corr_plots1 <- tibble(
  x = 1:100,
  y1 = 90 - x,
  y2 = map_dbl(x, ~ (.x*(rnorm(1, mean = 1,  sd = 0.06)) 
                     + rnorm(1, mean = 0, sd = 5))),
  y3 = map_dbl(x, ~ (70 - .x*(rnorm(1, mean = 0.6,  sd = 0.15)) 
                     + rnorm(1, mean = 10, sd = 10))),
  y4 = runif(100, 5, 85)) %>%
  pivot_longer(y1:y4, names_to = "type") %>%
  mutate(type = as.factor(type))

levels(corr_plots1$type) <- c("Perfect Negative Correlation", 
                              "Strong Positive Correlation", 
                              "Modest Negative Correlation", 
                              "No Correlation")

cor_results <- corr_plots1 %>%
  group_by(type) %>%
  summarise(r.pearson = cor(x,value)) 
```


``` {r pearson-plot-correlation, warning=FALSE, echo=FALSE, fig.cap="Pearson correlation for variables with perfect, strong, moderate, and no correlation."}
pearson_plot <- ggplot(data = corr_plots1,
       aes(x = x, y = value)) +
  geom_point(aes(color = type),
             alpha = 0.5) +
  facet_wrap(facets = vars(type),
             ncol = 2) +
   geom_label(x = 25, y = 105, 
            data = cor_results, 
            aes(label = paste("r = ", round(r.pearson, 2))),
            size = 3) +
  theme_bw() +
  xlim(0,110) +
  ylim(0,110) +
  coord_fixed(ratio=1.0) +
  labs(x = "Independent Variable (x-axis)",
       y = "Dependent Variable (y-axis)") +
  theme(legend.position = "none")
 
pearson_plot 
```

Values of **r** range from -1 (perfect negative correlation) to 0 (no correlation) to 1 (perfect positive correlation). As an engineer, I would say that two variables are *moderately correlated* when they have a Pearson correlation coefficient (as an absolute value) $|r|$, between 0.25 and 0.75. Two variables are *strongly correlated* when $|r|>0.75$. These are qualitative judgments on my part; someone in a different discipline (like  epidemiology or economics) might get super excited by discovering an r = 0.3 between two variables.

You will often see the square of Pearson correlation coefficient reported, $r^2$. 
The $r^2$ term is a direct indicator of how the ***variance*** in $y$ is explained by
knowing $x$.  Note that because of the square power, $r^2$ values range from zero to 1.

There are several ways to calculate **r** - all of these are mathematically equivalent. 
If we have `n` paired samples of `x` and `y`, then ***r*** is:

$$r = \frac{n\sum(x_{i}y_{i})-\sum x_{i} \sum y_{i} } {\sqrt {n\sum(x_{i}^{2})-\sum(x_{i})^{2}} \cdot \sqrt {n\sum(y_{i}^{2})-\sum(y_{i})^{2}}}$$

This equation looks like a lot of work but it's really just a lot of algebra to divide  the *covariance* of `x` and `y` with the product of their standard deviations: $\hat{\sigma}_{x}$, $\hat{\sigma}_{y}$. 

$$r = \frac{cov(x,y)} {\hat{\sigma}_{x} \cdot \hat{\sigma}_{y}}$$
You can rearrange the equation to calculate **r** using the mean and standard deviation as follows (note: the $n-1$ parts get canceled out):
$$r = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})\cdot(y_{i} - \bar{y}) } {\sqrt {\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}} \cdot \sqrt {\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}}}$$
```{r pearson-anno, echo=FALSE, fig.align="center", fig.cap="The Pearson Correlation Coefficient is not as bad as it loooks in algebraic form."}

knitr::include_graphics(path = "./images/pearson_anno.png")
```


You can calculate **r** using the `cor()` function and supplying `x` and `y` as arguments.  For example:

``` {r cor-example-1}
set.seed(9)
x <- 1:100
y <- x + runif(n = 100, min = -25, max = 25)

cor(x, y)
```

Also note that **r** is **not** an appropriate indicator of **non-linear correlation**. For example, in the example that follows, `y` is perfectly represented as `x^4`, but the Pearson correlation coefficient between these variables is not 1!

``` {r cor-example-non-linear}
set.seed(10)
x <- 1:100
y <- x^4 

cor(x, y)
```

If you wish to quantify correlation between two variables but fear that your data
violate the Pearson assumptions listed above, then try the ***Spearman Correlation Coefficient*** or ***Lin's Concordance Coefficient***. 

### Anscombe's Quartet
***Anscombe's Quartet*** is a fabulous example of how *correlation coefficients* may be misinterpreted or misused.  Look at the following four scatterplots below between two variables, `x` and `y`.

The data across these plots all have the same `x` mean and variance, `y` mean and variance, and the same slope, intercept, and $r^2$ when modeled under a linear OLS regression. 

``` {r anscombe, warning=FALSE, echo=FALSE, message=FALSE, fig.cap="Anscombe's Quartet. All for data sets have the same x-mean & variance, y-mean & variance, and the same slope, intercept, and correlation coefficient from an OLS linear model."}

anscombe <- read_csv("./data/anscombe.csv") %>%
  mutate(name = case_when(
    type == 1 ~ "1: Linear",
    type == 2 ~ "2: Non-linear",
    type == 3 ~ "3: Outlier",
    type == 4 ~ "4: High Leverage"
  ))

ggplot(data = anscombe,
       aes(x = x,
           y = y,
           color = name)) +
  geom_point(size = 2,
             shape = 1) +
  geom_smooth(method = "lm",
              se = FALSE,
              size = 0.75) +
  coord_fixed() +
  theme_bw(base_size = 14) +
  facet_wrap(facets = vars(name), nrow = 2) +
  theme(panel.grid = element_blank(),
        legend.position = "none",
        aspect.ratio = 1,
        strip.text = element_text(size = 10))
```

Clearly, despite similar statistics, the underlying data are from very different distributions.  This example highlights why it's so important to explore your data graphically!  The top-left plot shows an imprecise linear relationship, the top-right plot shows a non-linear relationship, the bottom-left plot shows a precise linear relationship with an outlier, and the bottom-right plot shows the effect of a *high-leverage* point (another type of outlier) on an otherwise null relationship!

## OLS Regression Coefficients {#OLS}

In [Chapter 11](#model) we discuss how to create a simple linear model (regression) between two observed variables $X$ and $Y$.  Here, we present the math behind an OLS linear fit, where the goal is to estimate model coefficients for the intercept ($\beta_{0}$) and slope ($\beta_{1}$) of the line between $Y$ and $X$ such that:

$$Y = \beta_{0} + \beta_{1}\cdot X + \epsilon$$

where the last term, $\epsilon$, represents the model error, or residual.

To calculate the slope term, $\beta_{1}$, we multiply the [Pearson correlation coefficient](#pearson) (i.e., `cor(x, y)`) by the ratio of the standard deviations for $Y$ and $X$ (i.e., `sd(y)/sd(x)`):

$$\beta_{1} = r\cdot\frac{\hat\sigma_y}{\hat\sigma_x}$$

This is mathematically equivalent to taking the ratio of the *covariance* between $X,Y$ to the *variance* of $X$:

$$\beta_{1} = \frac{cov(x,y)}{var(x)} = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})\cdot(y_{i} - \bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}$$

Once we know the slope term ($\beta_{1}$) we calculate the intercept ($\beta_{0}$) using $\beta_{1}$ and the sample means:

$$\beta_{0} =\bar{Y} - (\beta_{1}\cdot \bar{X})$$